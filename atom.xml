<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Thinking Realm</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://www.libinx.com/"/>
  <updated>2021-05-16T07:03:31.257Z</updated>
  <id>https://www.libinx.com/</id>
  
  <author>
    <name>H4ck3r L1</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>2021-05-16-谈谈反作弊风控的实践经验</title>
    <link href="https://www.libinx.com/2021/anti-cheating/"/>
    <id>https://www.libinx.com/2021/anti-cheating/</id>
    <published>2021-05-16T06:58:09.000Z</published>
    <updated>2021-05-16T07:03:31.257Z</updated>
    
    <content type="html"><![CDATA[<p>有人的地方就有江湖，互联网行业中的黑产团伙早已见怪不怪了，产品团队精心设计的营销活动奖励全部被机器人抢走、大量账号24小时挂机薅羊毛，这些行为不仅让正常用户享受不到企业的优惠福利而且还大大损害了企业自身的利益。</p><p>这种现象在笔者就职的公司也很普遍，笔者负责反作弊项目也有一段时间了，在与黑产做攻防斗争的过程中积累了一些心得体会，本文的目的是梳理自己反作弊的经验以及与大家分享交流自己反作弊的一些经验，欢迎指正！</p><h4 id="一、反作弊是啥？"><a href="#一、反作弊是啥？" class="headerlink" title="一、反作弊是啥？"></a>一、反作弊是啥？</h4><p>面对的群体：</p><ul><li>羊毛党：账号的背后是真人，这部分群体没有给平台做出贡献，但是只要有优惠活动就会一拥而上</li><li>黑产：账号的背后只有一个或者少数几个真人，这部分群体主要通过技术手段模拟真人，羊毛党的危害之于黑产要小得多</li></ul><p>羊毛党和黑产的存在会导致：耗费巨资的拉新促活方案效果不佳、平台返利都被机器人抢走、黑产和羊毛党的活跃造成了一种虚假繁荣的业务景象……反作弊的职责就是充当平台中打击黑产和羊毛党的管理员。</p><h4 id="二、反作弊策略的几个思路"><a href="#二、反作弊策略的几个思路" class="headerlink" title="二、反作弊策略的几个思路"></a>二、反作弊策略的几个思路</h4><ol><li>规则引擎<ul><li>已有策略</li><li>日常指标监控：业务方面的比如刷量监控、异常设备注册、异常版本注册、小渠道注册、虚拟机</li></ul></li><li>基于关联关系识别<ul><li>黑产也是有成本的，他们不可能在一批账号被封禁了之后就废弃设备，而是会在这些设备上重新注册新的账号，所以可以利用这个性质通过已有风险设备ID取关联出其他的同伙。但是这样做也有缺点，一个是打击面会越来越广，容易误伤，二是设备刷机了之后设备id相应地也会发生变化，导致策略失效</li><li>对于确定性很高的特征：比如IP聚集性、异常版本聚集性等等也可以扩大打击面</li></ul></li><li>基于行为或者物理特征<ul><li>行为上相似：黑产不同于羊毛党，黑产一般是通过机器控制大批量的账号，所以这些账号从行为日志数据上来看的相似度会很高，可以通过分析客户端日志事件，挖掘黑产是否有相似性，然而在实际中实际效果不太理想，容易误伤</li><li>GPS坐标、陀螺仪指标是否可疑</li><li>设备机型是否是重点被关注的对象</li></ul></li><li>基于网络或者机器学习：如标签传播（LPA）、图神经网络、GBDT等</li></ol><h4 id="三、反作弊的三阶段工作流程"><a href="#三、反作弊的三阶段工作流程" class="headerlink" title="三、反作弊的三阶段工作流程"></a>三、反作弊的三阶段工作流程</h4><h5 id="（1）-防守阶段"><a href="#（1）-防守阶段" class="headerlink" title="（1）. 防守阶段"></a>（1）. 防守阶段</h5><p>防守阶段是通过建设护城河直接把黑产挡在城外，让他们根本没有伤害我方的机会，比如通过第三方服务获取设备的风控标签、提高营销活动的参与门槛、增加二次校验防止恶意注册等手段，防止黑产进入到战场。</p><h5 id="（2）-攻击阶段"><a href="#（2）-攻击阶段" class="headerlink" title="（2）. 攻击阶段"></a>（2）. 攻击阶段</h5><ol><li>理解需要对抗的业务问题，找到一个小的切入点，找到一小部分的黑样本</li><li>从切入点或已有的黑样本突破召回一定数量的可疑账户并对其做验证分析</li><li>基于第2步，挖掘是否有新的特征并扩大召回的范围并重复第2步骤</li><li>监测反作弊的效果，搭建监测机制，量化指标异常设备量、可疑版本分布、TOP刷量等指标形成报表，非量化的则是用户反馈、是否有新的模式出现，然后工程师需要每天观察这些指标变化的情况</li><li>定期评估风控策略的效果</li></ol><h5 id="（3）-打扫战场"><a href="#（3）-打扫战场" class="headerlink" title="（3）. 打扫战场"></a>（3）. 打扫战场</h5><p>攻防阶段的过程中，黑产的进攻手段是在不断进化的，所以我们需要定期开展积累风控规则策略、总结黑产的攻击模式、积累设备黑名单列表等策略以升级现有的攻防策略。</p><h4 id="四、算法还是策略？"><a href="#四、算法还是策略？" class="headerlink" title="四、算法还是策略？"></a>四、算法还是策略？</h4><p>反作弊不同于反欺诈，反作弊的原则是“<strong>宁可放过一百，不可错杀一个</strong>”，误杀对于用户的体验影响是巨大的，在与黑产和羊毛党的攻防对抗中风险与收益是并存的。</p><p>那么，在反作弊的业务中算法和策略孰轻孰重？我觉得要分场景来考虑，打个比方，算法好比是导弹而规则策略则是一挺挺的机关枪，如果你的“战场”在一片巨大的区域，此时你面对的敌人是十万上百万数量级别，机关枪虽然好使但显然效率不高，如果你面对的仅仅是几千个好事之徒的挑衅，机关枪就足以对付，使用导弹反而容易伤及无辜。</p><p>所以，到底是算法还是策略？有几个点需要考虑，第一是业务量，如果你的业务量不是特别大，算法模型就好比大炮打蚊子，第二是可解释性，解释性较好的模型更受青睐，工程师至少知道自己的弹药打到了哪个地方，如果解释性不佳那么处境就很尴尬。笔者在算法模型上也做过一些尝试，有精力了再跟大家分享。</p><h4 id="五、做好反作弊的关键"><a href="#五、做好反作弊的关键" class="headerlink" title="五、做好反作弊的关键"></a>五、做好反作弊的关键</h4><ol><li>需要对业务问题有深刻的理解，比如拿到一个反作弊的需求，如何找到合适的切入点是非常关键的，哪怕是很小很小，不然无从下手。比如，笔者面对的群体就是黑产，那么首先需要思考黑产这个群体区别于其他正常人最大的特点是什么？经济流水正常不正常？IP地址有没有“聚集性”？</li><li>谨慎应对误伤，“宁可放过一百，不可错杀一个”，策略误杀对用户体量比较大的公司可能影响不大，对于体量较小的来说用户的不良反馈则是很影响用户体验的</li><li>考虑误伤对用户体验的影响，有的时候为了平台的热度或者引诱潜在的黑产，甚至可以适当考虑解禁小部分黑产账号</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;有人的地方就有江湖，互联网行业中的黑产团伙早已见怪不怪了，产品团队精心设计的营销活动奖励全部被机器人抢走、大量账号24小时挂机薅羊毛，这些行为不仅让正常用户享受不到企业的优惠福利而且还大大损害了企业自身的利益。&lt;/p&gt;
&lt;p&gt;这种现象在笔者就职的公司也很普遍，笔者负责反作弊
      
    
    </summary>
    
    
      <category term="Machine-Learing" scheme="https://www.libinx.com/categories/Machine-Learing/"/>
    
    
      <category term="反作弊" scheme="https://www.libinx.com/tags/%E5%8F%8D%E4%BD%9C%E5%BC%8A/"/>
    
      <category term="风控" scheme="https://www.libinx.com/tags/%E9%A3%8E%E6%8E%A7/"/>
    
  </entry>
  
  <entry>
    <title>机器学习算法系列（19）XGBoost</title>
    <link href="https://www.libinx.com/2021/machine-learning-algorithm-series-xgboost/"/>
    <id>https://www.libinx.com/2021/machine-learning-algorithm-series-xgboost/</id>
    <published>2021-05-09T13:25:33.000Z</published>
    <updated>2021-05-16T07:10:36.561Z</updated>
    
    <content type="html"><![CDATA[<p>XGBoost是GBDT的一种高效的实现，它是由华盛顿大学的陈天奇开发的一个高度可扩展的、端到端的提升系统。近几年XGBoost在各大算法竞赛中取得的成绩一时间可谓风头无两，它取得成功背后在于它在所有场景中的可扩展性，它在处理稀疏数据上的标点也是非常强大的。</p><h4 id="1-回顾提升树（Boosting-Tree）"><a href="#1-回顾提升树（Boosting-Tree）" class="headerlink" title="1. 回顾提升树（Boosting Tree）"></a>1. 回顾提升树（Boosting Tree）</h4><p>提升树（Boosting Tree）是集成学习的代表之一，在之前的文章中我们就提到过，集成学习的本质：单个单个的基学习器表达能力弱没有关系，只要把它们组合起来了就可以很强大了，可以概括为“<strong>三个臭皮匠顶个诸葛亮</strong>”。</p><p>提升树实际上是K个弱学习器的线性组合而得到的加法模型，所谓提升（Boosting）的本意是在上一个弱学习器的基础上更进一步缩小与目标值的距离，使用公式可以表示为（来自陈天奇的论文）：</p><p><img src="https://raw.githubusercontent.com/swordspoet/i/img/image-20210509215204453.png" alt="image-20210509215204453" style="zoom:50%;"></p><p>简单解释一下，数据集中的每一条数据都会落入弱学习器叶子结点的某一个区域，即对应的预测值，然后最终的预测结果等于所有树的预测值之和。如下图，小男孩（代表数据集中的一条数据）最终的预测结果为tree1和tree2预测值之和：</p><p><img src="https://raw.githubusercontent.com/swordspoet/i/img/image-20210509220124543.png" alt="image-20210509220124543" style="zoom:50%;"></p><h4 id="2-XGBoost损失函数的优化求解过程"><a href="#2-XGBoost损失函数的优化求解过程" class="headerlink" title="2. XGBoost损失函数的优化求解过程"></a>2. XGBoost损失函数的优化求解过程</h4><p>以上就是对提升树模型的简要回顾，下面正式进入XGBoost的部分，首先看XGBoost的损失函数：</p><p><img src="https://raw.githubusercontent.com/swordspoet/i/img/image-20210509220946380.png" alt="image-20210509220946380" style="zoom:50%;"></p><p>回顾GBDT的损失函数：</p><p><img src="https://raw.githubusercontent.com/swordspoet/i/img/image-20210509221457789.png" alt="image-20210509221457789" style="zoom:80%;"></p><p>可以看出，相比GBDT，XGBoost在其基础上<strong>添加了正则化项</strong>$\Omega(h<em>t) = \gamma J + \frac{\lambda}{2}\sum\limits</em>{j=1}^Jw<em>{tj}^2$，正则化项中的$w</em>{tj}$对应第$t$个弱学习器第$j$个<strong>叶子节点的最优值</strong>，与GBDT损失函数中的$c$是相同的意思（陈天奇的论文中是用$w$表示的，意思一样），$J$指的是叶节点的个数，$\gamma$和$\lambda$都是超参数。在损失函数中添加正则化项的有助于降低模型的复杂性和防止模型出现过拟合的情况。</p><p>接下来问题来了，如何最小化XGBoost的损失函数呢？对于公式（2），如果节点的分裂使用的是均方误差那很好优化，而XGBoost损失函数却无法通过一般的凸优化方法来求解，因为它实质上是一个NP hard的问题，因此XGBoost使用的贪心法来获得优化解，每次节点的分裂都<strong>期望最小化损失函数的误差</strong>，所以XGBoost的损失函数中加入了当前的树$f_t$，损失函数变成：</p><p><img src="https://raw.githubusercontent.com/swordspoet/i/img/image-20210509224544858.png" alt="image-20210509224544858" style="zoom: 67%;"></p><p>然后对该损失函数做<strong>泰勒的二阶展开</strong>得：</p><p><img src="https://raw.githubusercontent.com/swordspoet/i/img/image-20210509224650678.png" alt="image-20210509224650678" style="zoom: 67%;"></p><p>其中$g_i$和$h_i$是第i个样本在第t个弱学习器对$\hat{y}^{(t-1)}$<strong>一阶偏导和二阶偏导</strong>，由于损失函数中的$l$是常数，在最小化中可以去掉，上面的式子可以进一步写成：</p><p><img src="https://raw.githubusercontent.com/swordspoet/i/img/image-20210510200312608.png" alt="image-20210510200312608" style="zoom:67%;"></p><p>以下是公式（4）的推导改写过程，将n条数据全部代入目标函数后，可以将式子从<strong>叶节点的角度</strong>改写一下</p><p><img src="https://raw.githubusercontent.com/swordspoet/i/img/image-20210511223201530.png" alt="image-20210511223201530" style="zoom: 50%;"></p><p>对于公式（4）的$w_j$的求解比较简单，损失函数对$w_j$<strong>求导并令其为0</strong>，可得最优解$w_j^{*}$：</p><p><img src="https://raw.githubusercontent.com/swordspoet/i/img/image-20210510002428124.png" alt="image-20210510002428124" style="zoom:67%;"></p><p>然后将最优解代入公式（4），可得：</p><p><img src="https://raw.githubusercontent.com/swordspoet/i/img/image-20210510001116996.png" alt="image-20210510001116996" style="zoom:67%;"></p><p>接着，每次在节点做左右子树分裂split的时候，我们要尽量减少损失函数的损失，也就是说，假设当前节点<strong>左右子树的一阶、二阶导数</strong>和为$G_L,H_L,G_R,H_R$，我们<strong>期望最大化</strong>下式（为什么是期望最大化下面会解释）：</p><p><img src="https://raw.githubusercontent.com/swordspoet/i/img/image-20210510201321832.png" alt="image-20210510201321832" style="zoom:67%;"></p><p>整理，得：</p><p><img src="https://raw.githubusercontent.com/swordspoet/i/img/image-20210510202635578.png" alt="image-20210510202635578" style="zoom:67%;"></p><p>这里有一个疑问，上面的这个式子是怎么从公式（6）计算得来的呢？自己想了很久没有想清楚，在B站上找到了一个up主把节点分裂的过程解释得非常到位！看下图，现在假设我们有从1到8的8个样本点。</p><p><img src="https://raw.githubusercontent.com/swordspoet/i/img/image-20210510210726742.png" alt="image-20210510210726742" style="zoom:67%;"></p><p>第一次分裂，左子树两个样本点{7,8}，右子树有六个样本点{1,2,3,4,5,6}，此时的目标函数为：</p><p><img src="https://raw.githubusercontent.com/swordspoet/i/img/image-20210510210522916.png" alt="image-20210510210522916" style="zoom:67%;"></p><p>接下来，我们选择某个特征对右子树做节点分裂，假设分裂的结果为左子树为{1,3,5}，右子树为{2,4,6}，此时的目标函数为：</p><p><img src="https://raw.githubusercontent.com/swordspoet/i/img/image-20210510210543835.png" alt="image-20210510210543835" style="zoom:67%;"></p><p>在节点向下分裂的过程中，我们希望第二步目标函数变得越来越小，故实际上我们期望最大化的是<strong>第一步的目标函数减去第二步的目标函数</strong>：（这里请重点理解！）：</p><p><img src="https://raw.githubusercontent.com/swordspoet/i/img/image-20210510210629523.png" alt="image-20210510210629523" style="zoom:67%;"></p><p>到此，XGBoost损失函数的优化求解就记录得差不多了。</p><h4 id="3-XGBoost的训练过程"><a href="#3-XGBoost的训练过程" class="headerlink" title="3. XGBoost的训练过程"></a>3. XGBoost的训练过程</h4><p><img src="https://raw.githubusercontent.com/swordspoet/i/img/image-20210511224734116.png" alt="image-20210511224734116" style="zoom:67%;"></p><ol><li>计算所有m个样本对于当前轮损失函数的一阶导数$G$和二阶导数$H$</li><li>基于当前节点分裂决策树，初始化score为0，如果当前有$k$个特征：<ol><li>样本按照第k个<strong>特征排好序</strong>，放入左子树，然后计算左右子树一阶和二阶导数和</li><li>更新score</li></ol></li><li>基于score最大的特征分裂</li></ol><h4 id="4-XGBoost类库的参数"><a href="#4-XGBoost类库的参数" class="headerlink" title="4. XGBoost类库的参数"></a>4. XGBoost类库的参数</h4><p>在<a href="https://www.libinx.com/2017/machine-learning-algorithm-series-gbdt/">梯度提升决策树 </a>的文章的末尾提到了GBDT在sklearn中可以调节的参数，其实XGBoost和GBDT可调节的参数都差不多。</p><p>通用参数：</p><ol><li><code>booster</code> ：弱学习器种类，gbtree或者dart</li><li><code>objective</code>：损失函数，如果是回归问题一般使用reg:squarederror，分类问题使用<code>binary:logistic</code>或者<code>multi:softmax</code>，另外还可以通过设置损失函数决定模型的输出是概率还是原始的预测值</li><li><code>n_estimators</code>：弱学习器的个数，决定了模型的复杂程度，<strong>通常和学习率参数一块调参</strong></li></ol><p>弱学习器参数：</p><ol><li><code>eta [default=0.3, alias: learning_rate]</code>：学习率，默认等于0.3</li><li><code>gamma [default=0, alias: min_split_loss]</code>：等于目标函数中正则化项的gamma，决策树分裂时最大化下式时必须大于gamma才能继续分裂<img src="https://raw.githubusercontent.com/swordspoet/i/img/image-20210510202635578.png" alt="image-20210510202635578" style="zoom:67%;"></li><li><code>max_depth [default=6]</code>：定义树的最大深度，决定了模型的复杂程度，通常需要通过网格搜索决定最佳的树的深度</li><li><code>min_child_weight [default=1]</code>：决策树中子节点最小的权重阈值，如果子节点的权重阈值小于min_child_weight则不再继续分裂</li><li><code>subsample [default=1]</code>：训练集的采样率，介于0,1之间</li><li><code>sampling_method [default= uniform]</code>：采样方法，有uniform和gradient_based</li><li><code>colsample_bytree, colsample_bylevel, colsample_bynode [default=1]</code>：针对特征采样的手段，层级别、树级别和子节点级别，采样率介于0,1之间</li><li><code>lambda [default=1, alias: reg_lambda]</code>：L2正则化系数，对应$\Omega(h<em>t) = \gamma J + \frac{\lambda}{2}\sum\limits</em>{j=1}^Jw_{tj}^2$里面的$\lambda$</li><li><code>alpha [default=0, alias: reg_alpha]</code>：L1正则化系数，对应$\Omega(h<em>t) = \gamma J + \frac{\lambda}{2}\sum\limits</em>{j=1}^Jw_{tj}^2$里面的$\gamma$</li><li><code>tree_method</code>：默认auto，自动选择最快的方法</li></ol><p>参考：<a href="https://xgboost.readthedocs.io/en/latest/python/python_api.html" target="_blank" rel="noopener">Python API Reference — xgboost 1.5.0-SNAPSHOT documentation</a></p><h4 id="5-GBDT和XGBoost的区别与联系："><a href="#5-GBDT和XGBoost的区别与联系：" class="headerlink" title="5. GBDT和XGBoost的区别与联系："></a>5. GBDT和XGBoost的区别与联系：</h4><ol><li>GBDT是集成学习算法，而XGBoost是GBDT的一种高效工程实现</li><li>对比GBDT的损失函数，XGBoost还加入了正则化部分，防止过拟合，泛化性能更优</li><li>XGBoost对损失函数对误差部分同时做了一阶和二阶的泰勒展开，相比GBDT更加准确</li><li>GBDT的弱学习器限定了CART，而XGBoost还有很多其他的弱学习器可以选择</li><li>GBDT在每一轮迭代的时候使用了全部的数据，而XGBoost采用了跟随机森林差不多的策略，支持对数据进行采样</li><li>GBDT没有设计对缺失值的处理，XGBoost能够自动学习出缺失值的处理策略</li></ol><p>参考：</p><ol><li><a href="https://www.cnblogs.com/pinard/p/10979808.html" target="_blank" rel="noopener">XGBoost算法原理小结 - 刘建平Pinard - 博客园</a></li><li><a href="https://www.bilibili.com/video/BV1si4y1G7Jb?p=1" target="_blank" rel="noopener">XGBoost的技术剖析_哔哩哔哩 (゜-゜)つロ 干杯~-bilibili</a></li><li><a href="https://arxiv.org/pdf/1603.02754.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1603.02754.pdf</a></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;XGBoost是GBDT的一种高效的实现，它是由华盛顿大学的陈天奇开发的一个高度可扩展的、端到端的提升系统。近几年XGBoost在各大算法竞赛中取得的成绩一时间可谓风头无两，它取得成功背后在于它在所有场景中的可扩展性，它在处理稀疏数据上的标点也是非常强大的。&lt;/p&gt;
&lt;h4
      
    
    </summary>
    
    
      <category term="Machine-Learning" scheme="https://www.libinx.com/categories/Machine-Learning/"/>
    
    
      <category term="机器学习算法系列,集成学习,XGBoost,bagging" scheme="https://www.libinx.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97-%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0-XGBoost-bagging/"/>
    
  </entry>
  
  <entry>
    <title>《Spark快速大数据分析》思维导图及笔记</title>
    <link href="https://www.libinx.com/2020/spark-big-fast-data-analysis/"/>
    <id>https://www.libinx.com/2020/spark-big-fast-data-analysis/</id>
    <published>2020-09-29T14:18:53.000Z</published>
    <updated>2021-03-23T01:25:10.997Z</updated>
    
    <content type="html"><![CDATA[<p>《Spark快速大数据分析》思维导图及笔记</p><a id="more"></a><h2 id="spark定义"><a href="#spark定义" class="headerlink" title="spark定义"></a>spark定义</h2><h3 id="快速、通用的集群计算平台"><a href="#快速、通用的集群计算平台" class="headerlink" title="快速、通用的集群计算平台"></a>快速、通用的集群计算平台</h3><h3 id="2009年诞生于加州大学伯克利分校"><a href="#2009年诞生于加州大学伯克利分校" class="headerlink" title="2009年诞生于加州大学伯克利分校"></a>2009年诞生于加州大学伯克利分校</h3><h3 id="主要特点：内存运算，快！"><a href="#主要特点：内存运算，快！" class="headerlink" title="主要特点：内存运算，快！"></a>主要特点：内存运算，快！</h3><h2 id="第一章：组件"><a href="#第一章：组件" class="headerlink" title="第一章：组件"></a>第一章：组件</h2><h3 id="spark-core"><a href="#spark-core" class="headerlink" title="spark core"></a>spark core</h3><ul><li>任务调度</li><li>内存管理</li><li>错误恢复<h3 id="spark-sql"><a href="#spark-sql" class="headerlink" title="spark sql"></a>spark sql</h3><h3 id="streaming"><a href="#streaming" class="headerlink" title="streaming"></a>streaming</h3></li><li>实时数据进行流式计算：服务器日志、消息队列<h3 id="mllib"><a href="#mllib" class="headerlink" title="mllib"></a>mllib</h3></li><li>支持机器学习<h3 id="spark支持任何实现Hadoop接口的存储系统：文本文件、SequenceFile、Avro、Parquet"><a href="#spark支持任何实现Hadoop接口的存储系统：文本文件、SequenceFile、Avro、Parquet" class="headerlink" title="spark支持任何实现Hadoop接口的存储系统：文本文件、SequenceFile、Avro、Parquet"></a>spark支持任何实现Hadoop接口的存储系统：文本文件、SequenceFile、Avro、Parquet</h3><h2 id="第二章：核心概念"><a href="#第二章：核心概念" class="headerlink" title="第二章：核心概念"></a>第二章：核心概念</h2><h3 id="每个应用都由一个驱动器程序发起集群上的并行操作"><a href="#每个应用都由一个驱动器程序发起集群上的并行操作" class="headerlink" title="每个应用都由一个驱动器程序发起集群上的并行操作"></a>每个应用都由一个驱动器程序发起集群上的并行操作</h3><h3 id="驱动器程序通过sparkcontext对象访问spark"><a href="#驱动器程序通过sparkcontext对象访问spark" class="headerlink" title="驱动器程序通过sparkcontext对象访问spark"></a>驱动器程序通过sparkcontext对象访问spark</h3></li><li>val conf = new SparkConf().setMaster(“local”).setAppName(“My App”)<br>val sc = new SparkContext(conf)  # 初始化一个sparkContext<ul><li>参数<ul><li>setMaster()集群URL<ul><li>local单机执行</li><li>指定集群地址则集群执行</li></ul></li><li>setAppName()应用名</li></ul></li></ul></li><li>spark-shell启动的时候已经自动创建了一个sparkcontext对象，即sc<ul><li>调整输出日志级别<ul><li>日志设置文件的模版log4j.properties.template</li><li>log4j.rootCategory控制输出级别</li></ul></li><li>每个spark应用都必须有一个驱动程序来启动，spark-shell在启动的时候就相当于开启了驱动程序</li></ul></li><li>spark api会把一些基于函数的操作(filter)也会发送到集群上执行<h3 id="驱动器程序管理多个执行器（executor）节点"><a href="#驱动器程序管理多个执行器（executor）节点" class="headerlink" title="驱动器程序管理多个执行器（executor）节点"></a>驱动器程序管理多个执行器（executor）节点</h3></li><li>spark api会把函数发送到各个executor节点上，实现代码在多个节点上运行<h3 id="Maven"><a href="#Maven" class="headerlink" title="Maven"></a>Maven</h3></li><li>类似于Python中的pip，它是一个包管理工具，只需要将需要安装的包和版本号写在pom.xml文件中</li><li>比Python的pip更严格！<h2 id="第三章：RDD编程"><a href="#第三章：RDD编程" class="headerlink" title="第三章：RDD编程"></a>第三章：RDD编程</h2><h3 id="RDD定义"><a href="#RDD定义" class="headerlink" title="RDD定义"></a>RDD定义</h3></li><li>弹性分布式数据集</li><li>不可变的分布式对象集合<h3 id="创建RDD"><a href="#创建RDD" class="headerlink" title="创建RDD"></a>创建RDD</h3></li><li>方法一：读取外部数据集<ul><li>val lines = sc.textfile(“path/to/file”)<ul><li>数据并没有读取进来，只有在必要的时候才会读取</li></ul></li></ul></li><li>方法二：对一个集合进行并行化（一般在spark shell调试用的多）<ul><li>val lines = sc.parallelize(List(“pandas”, “i like pandas”))</li></ul></li><li>不应把RDD看做存着特定数据的数据集，而要把RDD当做一系列转化操作、计算步骤方法的列表，只有在必要的时候才执行<h3 id="特性"><a href="#特性" class="headerlink" title="特性"></a>特性</h3></li><li>惰性<ul><li>只有真正用到时才会真正计算</li></ul></li><li>弹性<ul><li>当保存RDD数据的一台机器失败时，spark可以利用弹性重新算出丢掉的分区</li><li>使用spark把rdd缓存到内存，重复使用<h3 id="RDD支持操作"><a href="#RDD支持操作" class="headerlink" title="RDD支持操作"></a>RDD支持操作</h3></li></ul></li><li>转化（transformation）：针对各个元素<ul><li>filter()<ul><li>不会修改原有的RDD，它会将满足条件的RDD放入新的RDD中返回</li></ul></li><li>map()<ul><li>函数返回的结果作为新的RDD返回</li></ul></li><li>返回的还是rdd</li></ul></li><li>行动（action）<ul><li>first()</li><li>take()</li><li>count()</li><li>collect()<ul><li>非常消耗内存，不适合大规模数据集</li><li>通常在单元测试中用</li></ul></li><li>通常有求和、求元素个数以及其他聚合操作，所以返回的是其他数据类型</li></ul></li><li>伪集合操作<ul><li>distinct<ul><li>因为它会遍历所有的数据，所以开销很大</li></ul></li><li>union</li><li>intersection</li><li>substract</li><li>sample</li><li>cartesian()笛卡尔积<ul><li>开销巨大，比如物品之间的相似度计算开销就非常大<h3 id="持久化（缓存）"><a href="#持久化（缓存）" class="headerlink" title="持久化（缓存）"></a>持久化（缓存）</h3></li></ul></li></ul></li><li>对重复使用的RDD进行持久化persist</li><li>如果RDD不会被重用，那么就没有必要对RDD持久化</li><li>cache() 与使用默认存储级别调用 persist() 是一样的<ul><li>cache将rdd缓存到内存</li><li>persist则有多种缓存策略</li></ul></li><li>StorageLevel<ul><li>MEMORY_AND_DISK:如果数据在内存中放不下,则溢写到磁盘上</li><li>MEMORY_AND_DISK_SER:如果数据在内存中放不下,则溢写到磁盘上。在内存中存放序列化后的数据<h2 id="第四章：键值对操作"><a href="#第四章：键值对操作" class="headerlink" title="第四章：键值对操作"></a>第四章：键值对操作</h2><h3 id="定义：kv"><a href="#定义：kv" class="headerlink" title="定义：kv"></a>定义：kv</h3><h3 id="操作"><a href="#操作" class="headerlink" title="操作"></a>操作</h3></li></ul></li><li>行动操作<ul><li>reduceByKey：合并有相同键的值</li><li>combineByKey<ul><li>会遍历分区的每一个元素</li><li>每个分区都是独立处理，对于同一个键会有多个累加器</li></ul></li><li>groupByKey：对有相同键的值分组</li></ul></li><li>转化操作<h3 id="数据分区"><a href="#数据分区" class="headerlink" title="数据分区"></a>数据分区</h3></li><li>通过控制rdd分区方式减少通信开销<h2 id="第五章：数据读取与保存"><a href="#第五章：数据读取与保存" class="headerlink" title="第五章：数据读取与保存"></a>第五章：数据读取与保存</h2><h3 id="spark基于Hadoop生态圈构建，能访问s3、HDFS、HBASE等文件格式与存储系统"><a href="#spark基于Hadoop生态圈构建，能访问s3、HDFS、HBASE等文件格式与存储系统" class="headerlink" title="spark基于Hadoop生态圈构建，能访问s3、HDFS、HBASE等文件格式与存储系统"></a>spark基于Hadoop生态圈构建，能访问s3、HDFS、HBASE等文件格式与存储系统</h3><h3 id="spark-shell在启动的时候自动创建了sc（spark-context）"><a href="#spark-shell在启动的时候自动创建了sc（spark-context）" class="headerlink" title="spark-shell在启动的时候自动创建了sc（spark context）"></a>spark-shell在启动的时候自动创建了sc（spark context）</h3><h3 id="读取"><a href="#读取" class="headerlink" title="读取"></a>读取</h3></li><li>文件系统<ul><li>hdfs集群</li><li>本地文件系统<ul><li>CSV、压缩文件格式</li></ul></li></ul></li><li>数据库<ul><li>JDBC</li></ul></li><li>SequenceFile<ul><li>SequenceFile 是由没有相对关系结构的键值对文件组成的常用 Hadoop 格式</li><li>val data = sc.parallelize(List((“Panda”, 3), (“Kay”, 6), (“Snail”, 2)))<br>data.saveAsSequenceFile(outputFile)</li></ul></li><li>HDFS<h2 id="第六章：Spark编程进阶"><a href="#第六章：Spark编程进阶" class="headerlink" title="第六章：Spark编程进阶"></a>第六章：Spark编程进阶</h2><h3 id="基于分区进行操作"><a href="#基于分区进行操作" class="headerlink" title="基于分区进行操作"></a>基于分区进行操作</h3></li><li>基于分区对数据进行操作可以避免为每个数据元素进行重复配置工作</li><li>foreachPartitions()</li><li>mapPartitions()<h2 id="第七章：在集群上运行Spark"><a href="#第七章：在集群上运行Spark" class="headerlink" title="第七章：在集群上运行Spark"></a>第七章：在集群上运行Spark</h2><h3 id="运行架构"><a href="#运行架构" class="headerlink" title="运行架构"></a>运行架构</h3></li><li>Spark采用的主从架构，一个Driver节点负责协调各个分布式节点Executor，Driver和Executor一同组成了Application，Spark的Application通过集群管理器（Cluster Manager）启动。<h3 id="Driver"><a href="#Driver" class="headerlink" title="Driver"></a>Driver</h3></li><li>将用户程序转为任务</li><li>为Executor调度任务<h3 id="Executor"><a href="#Executor" class="headerlink" title="Executor"></a>Executor</h3></li><li>负责运行任务</li><li>任务之间相互独立，及时某个Executor崩溃，任务也能继续执行<h2 id="第八章：Spark调优与调试"><a href="#第八章：Spark调优与调试" class="headerlink" title="第八章：Spark调优与调试"></a>第八章：Spark调优与调试</h2><h3 id="RDD并行度"><a href="#RDD并行度" class="headerlink" title="RDD并行度"></a>RDD并行度</h3></li><li>并行度太低集群资源浪费</li><li>数据shuffle的时候通过指定分区数量可以降低数据的通信成本</li><li>rdd在经过filter操作之后会产生很多空的分区或者数据很少的分区，可以通过合并降低分区数量提高性能。coalesce()<h3 id="内存管理"><a href="#内存管理" class="headerlink" title="内存管理"></a>内存管理</h3></li><li>rdd存储，默认分配60%空间</li><li>数据shuffle与聚合，默认分配20%空间</li><li>用户代码，默认分配20%空间<h2 id="第十章：streaming"><a href="#第十章：streaming" class="headerlink" title="第十章：streaming"></a>第十章：streaming</h2><h3 id="使用离散化流（discretized-stream）作为抽象表示，叫做DStream"><a href="#使用离散化流（discretized-stream）作为抽象表示，叫做DStream" class="headerlink" title="使用离散化流（discretized stream）作为抽象表示，叫做DStream"></a>使用离散化流（discretized stream）作为抽象表示，叫做DStream</h3><h3 id="输入源：Flume、Kafka、HDFS"><a href="#输入源：Flume、Kafka、HDFS" class="headerlink" title="输入源：Flume、Kafka、HDFS"></a>输入源：Flume、Kafka、HDFS</h3><h3 id="streaming-执行过程"><a href="#streaming-执行过程" class="headerlink" title="streaming 执行过程"></a>streaming 执行过程</h3></li><li>启动接收器</li><li>从输入源收集数据并保存为rdd</li><li>将收集到的数据复制到另一个执行器进程保障容错性</li><li>驱动器程序运行spark作业处理收集的数据<h3 id="操作-1"><a href="#操作-1" class="headerlink" title="操作"></a>操作</h3></li><li>转化<ul><li>无状态转化操作</li><li>有状态转化操作<ul><li>跨时区的</li></ul></li></ul></li><li>输出<ul><li>保存为文件</li><li>保存为sequencefile</li><li>使用foreachRDD()存储到外部系统<h3 id="流数据"><a href="#流数据" class="headerlink" title="流数据"></a>流数据</h3></li></ul></li><li>streamingcontext实例</li><li>zookeeper主机列表</li><li>消费组的名字</li></ul><h1 id="思维导图"><a href="#思维导图" class="headerlink" title="思维导图"></a>思维导图</h1><p><img src="https://raw.githubusercontent.com/swordspoet/i/img/Fast-Big-Data-Analysis.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;《Spark快速大数据分析》思维导图及笔记&lt;/p&gt;
    
    </summary>
    
    
      <category term="Spark" scheme="https://www.libinx.com/categories/Spark/"/>
    
    
      <category term="Spark" scheme="https://www.libinx.com/tags/Spark/"/>
    
      <category term="Scala" scheme="https://www.libinx.com/tags/Scala/"/>
    
  </entry>
  
  <entry>
    <title>Spark读取ElasticSearch数据——聚合查询</title>
    <link href="https://www.libinx.com/2020/spark-elasticsearch-aggregation-query/"/>
    <id>https://www.libinx.com/2020/spark-elasticsearch-aggregation-query/</id>
    <published>2020-09-29T14:18:53.000Z</published>
    <updated>2021-03-23T01:25:10.997Z</updated>
    
    <content type="html"><![CDATA[<p>本文主要介绍如何通过Spark读取ES中的数据，并对ES进行聚合查询。</p><a id="more"></a><p>我使用到的Scala、Spark和es的版本信息：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">properties</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">scala.version</span>&gt;</span>2.11.12<span class="tag">&lt;/<span class="name">scala.version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">spark.version</span>&gt;</span>2.4.0-cdh6.3.1<span class="tag">&lt;/<span class="name">spark.version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">properties</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.scala-lang<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>scala-library<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;scala.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.scala-lang<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>scala-actors<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;scala.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.elasticsearch<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>elasticsearch-spark-20_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>6.5.4<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br></pre></td></tr></table></figure><p>下面是Spark读取es数据的测试代码，大家可以参考一下，主要有三种方法，用到了es的高阶API（EsSparkSQL、EsSpark.esRDD）和低阶API，<strong>建议使用方法一和方法三</strong>，方法二在测试中发现es的query语句不生效，测试了很多遍也没有通过。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.net.<span class="type">InetAddress</span></span><br><span class="line"><span class="keyword">import</span> java.util.<span class="type">Properties</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">SparkSession</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions.max</span><br><span class="line"><span class="keyword">import</span> org.elasticsearch.action.search.<span class="type">SearchType</span></span><br><span class="line"><span class="keyword">import</span> org.elasticsearch.common.settings.<span class="type">Settings</span></span><br><span class="line"><span class="keyword">import</span> org.elasticsearch.common.transport.<span class="type">TransportAddress</span></span><br><span class="line"><span class="keyword">import</span> org.elasticsearch.search.aggregations.<span class="type">AggregationBuilders</span></span><br><span class="line"><span class="keyword">import</span> org.elasticsearch.search.aggregations.metrics.max.<span class="type">InternalMax</span></span><br><span class="line"><span class="keyword">import</span> org.elasticsearch.spark.rdd.<span class="type">EsSpark</span></span><br><span class="line"><span class="keyword">import</span> org.elasticsearch.spark.sql.<span class="type">EsSparkSQL</span></span><br><span class="line"><span class="keyword">import</span> org.elasticsearch.transport.client.<span class="type">PreBuiltTransportClient</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SparkEsDemo</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        run()</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> sparkSession = <span class="type">SparkSession</span></span><br><span class="line">          .builder()</span><br><span class="line">          .appName(<span class="string">"SparkEsTestDemo"</span>)</span><br><span class="line">          .config(<span class="string">"es.nodes"</span>, <span class="string">"xxx.xxx.xxx.xxx"</span>)</span><br><span class="line">          .config(<span class="string">"es.port"</span>, <span class="number">9200</span>)</span><br><span class="line">          <span class="comment">//重试5次（默认3次）</span></span><br><span class="line">          .config(<span class="string">"es.batch.write.retry.count"</span>, <span class="string">"5"</span>)</span><br><span class="line">          <span class="comment">//等待60秒（默认10s）</span></span><br><span class="line">          .config(<span class="string">"es.batch.write.retry.wait"</span>, <span class="string">"60"</span>)</span><br><span class="line">          <span class="comment">//es连接超时时间100s（默认1m）</span></span><br><span class="line">          .config(<span class="string">"es.http.timeout"</span>, <span class="string">"200s"</span>)</span><br><span class="line">          .getOrCreate()</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 方法一：通过将es的数据读取为dataframe</span></span><br><span class="line">        <span class="keyword">val</span> esResource = <span class="string">"esIndex/esType"</span>  <span class="comment">// 换成你自己的索引和索引类型</span></span><br><span class="line">        <span class="keyword">val</span> esDf = <span class="type">EsSparkSQL</span></span><br><span class="line">          .esDF(sparkSession,resource = esResource)  <span class="comment">// resource参数填写es的索引和索引类型</span></span><br><span class="line">          .agg(max(<span class="string">"fieldName"</span>))  <span class="comment">// 到了这里就相当于直接将es的数据转化为dataframe直接做计算了，很直观</span></span><br><span class="line">          .withColumnRenamed(<span class="string">"max(fieldName)"</span>,<span class="string">"maxFieldName"</span>)</span><br><span class="line">          .na.fill(<span class="number">0</span>)</span><br><span class="line">        esDf.show(<span class="literal">false</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 方法三：通过编写Query查询语句查询es数据，返回的是rdd数据</span></span><br><span class="line">        <span class="keyword">val</span> query = <span class="string">s""</span><span class="string">"&#123;</span></span><br><span class="line"><span class="string">                       |  "</span><span class="string">query": &#123;</span></span><br><span class="line"><span class="string">                       |    "</span><span class="string">bool": &#123;&#125;</span></span><br><span class="line"><span class="string">                       |  &#125;,</span></span><br><span class="line"><span class="string">                       |  "</span><span class="string">size": 0,</span></span><br><span class="line"><span class="string">                       |  "</span><span class="string">aggs": &#123;</span></span><br><span class="line"><span class="string">                       |    "</span>max_<span class="string">price": &#123;</span></span><br><span class="line"><span class="string">                       |      "</span><span class="string">max": &#123;</span></span><br><span class="line"><span class="string">                       |        "</span><span class="string">field": "</span><span class="string">price"</span></span><br><span class="line"><span class="string">                       |      &#125;</span></span><br><span class="line"><span class="string">                       |    &#125;</span></span><br><span class="line"><span class="string">                       |  &#125;</span></span><br><span class="line"><span class="string">                       |&#125;"</span><span class="string">""</span>.stripMargin</span><br><span class="line">        <span class="keyword">val</span> esRdd = <span class="type">EsSpark</span></span><br><span class="line">          .esRDD(sparkSession.sparkContext, esResource, query)</span><br><span class="line">        <span class="comment">// https://github.com/elastic/elasticsearch-hadoop/issues/276</span></span><br><span class="line">        esRdd.take(<span class="number">10</span>).foreach(println)</span><br><span class="line">        esRdd.take(<span class="number">10</span>).foreach(f =&gt; &#123;</span><br><span class="line">            <span class="keyword">val</span> map:collection.<span class="type">Map</span>[<span class="type">String</span>,<span class="type">AnyRef</span>] = f._2</span><br><span class="line">            <span class="keyword">for</span> (s &lt;- map) &#123;</span><br><span class="line">                println(s._1 + s._2)</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;)</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 方法三：通过es客户端的低阶API实现聚合查询</span></span><br><span class="line">        <span class="comment">// https://www.cnblogs.com/benwu/articles/9230819.html</span></span><br><span class="line">        <span class="keyword">val</span> prop = <span class="keyword">new</span> <span class="type">Properties</span>()</span><br><span class="line">        <span class="keyword">val</span> esClient = initClient(prop)</span><br><span class="line">        <span class="keyword">val</span> searchResponse = esClient</span><br><span class="line">          .prepareSearch(<span class="string">"recommend_smartvideo_spider_acgn"</span>)</span><br><span class="line">          .setTypes(<span class="string">"smartvideo_type_user_irrelative"</span>)</span><br><span class="line">          .setSearchType(<span class="type">SearchType</span>.<span class="type">DFS_QUERY_THEN_FETCH</span>)</span><br><span class="line">          .setExplain(<span class="literal">true</span>)</span><br><span class="line">          .setSize(<span class="number">0</span>)</span><br><span class="line">          .addAggregation(<span class="type">AggregationBuilders</span>.max(<span class="string">"max_cursor"</span>).field(<span class="string">"inx_cursor"</span>))</span><br><span class="line">          .setFetchSource(<span class="literal">false</span>)</span><br><span class="line">          .execute().actionGet()</span><br><span class="line">        <span class="keyword">val</span> maxValue:<span class="type">InternalMax</span> = searchResponse.getAggregations.get(<span class="string">"max_cursor"</span>)</span><br><span class="line">        println(maxValue.value())</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">      * 初始化es客户端</span></span><br><span class="line"><span class="comment">      * @param prop es配置类</span></span><br><span class="line"><span class="comment">      * @return</span></span><br><span class="line"><span class="comment">      */</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">initClient</span></span>(prop: <span class="type">Properties</span>): <span class="type">PreBuiltTransportClient</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> setting = <span class="type">Settings</span>.builder()</span><br><span class="line">          .put(<span class="string">"cluster.name"</span>, prop.getProperty(<span class="string">"es.cluster.name"</span>))</span><br><span class="line">          .put(<span class="string">"client.transport.sniff"</span>, <span class="literal">true</span>)</span><br><span class="line">          .build()</span><br><span class="line">        <span class="keyword">val</span> client = <span class="keyword">new</span> <span class="type">PreBuiltTransportClient</span>(setting)</span><br><span class="line">        <span class="keyword">val</span> hostAndPortArr = prop.getProperty(<span class="string">"es.host&amp;port"</span>).split(<span class="string">","</span>)</span><br><span class="line">        <span class="keyword">for</span> (hostPort &lt;- hostAndPortArr) &#123;</span><br><span class="line">            <span class="keyword">val</span> hp = hostPort.split(<span class="string">":"</span>)</span><br><span class="line">            <span class="keyword">if</span> (hp.length == <span class="number">2</span>) &#123;</span><br><span class="line">                client.addTransportAddress(</span><br><span class="line">                    <span class="keyword">new</span> <span class="type">TransportAddress</span>(<span class="type">InetAddress</span>.getByName(hp(<span class="number">0</span>)), hp(<span class="number">1</span>).toInt)</span><br><span class="line">                )</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                client.addTransportAddress(</span><br><span class="line">                    <span class="keyword">new</span> <span class="type">TransportAddress</span>(<span class="type">InetAddress</span>.getByName(hp(<span class="number">0</span>)), <span class="number">9300</span>)</span><br><span class="line">                )</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        client</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文主要介绍如何通过Spark读取ES中的数据，并对ES进行聚合查询。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Spark" scheme="https://www.libinx.com/categories/Spark/"/>
    
    
      <category term="Spark" scheme="https://www.libinx.com/tags/Spark/"/>
    
      <category term="Scala" scheme="https://www.libinx.com/tags/Scala/"/>
    
      <category term="elasticsearch" scheme="https://www.libinx.com/tags/elasticsearch/"/>
    
  </entry>
  
  <entry>
    <title>Spark编程基础（Scala版）笔记</title>
    <link href="https://www.libinx.com/2020/spark-coding-scala/"/>
    <id>https://www.libinx.com/2020/spark-coding-scala/</id>
    <published>2020-09-22T14:18:53.000Z</published>
    <updated>2021-03-23T01:25:10.997Z</updated>
    
    <content type="html"><![CDATA[<p>记录了部分章节的笔记，这本书语言平时易懂，看得出来作者是很用心地写书，不是照着官方文档翻译，我推荐林子雨老师的这本书作为入门Spark的读物。</p><a id="more"></a><h2 id="第二章：Scala语言基础"><a href="#第二章：Scala语言基础" class="headerlink" title="第二章：Scala语言基础"></a>第二章：Scala语言基础</h2><h3 id="什么是Scala"><a href="#什么是Scala" class="headerlink" title="什么是Scala ?"></a>什么是Scala ?</h3><ul><li>混合编程范式语言：函数式编程和面向对象编程</li><li>每个值都是对象，每个操作都是方法调用</li><li>兼容Java<h3 id="数据类型与变量"><a href="#数据类型与变量" class="headerlink" title="数据类型与变量"></a>数据类型与变量</h3></li><li>操作符实际也是方法：5 + 3和5.+(3)等价</li><li>尽量使用括号理清操作符的优先级别</li><li>print()和println()的区别是后者输出结束时会默认加上换行符<h3 id="控制结构"><a href="#控制结构" class="headerlink" title="控制结构"></a>控制结构</h3></li><li>三元表达式：val a  =  if (6&gt;0) 1 else -1</li><li>for循环<ul><li>for (变量 &lt;- 表达式)  {语句块}</li><li>变量 &lt;- 表达式 被称为生成器</li><li>守卫式：for (变量 &lt;- 表达式 if 条件表达式)  {语句块}</li></ul></li><li>异常处理结构<ul><li>try-catch</li><li>breakable<h3 id="数据结构"><a href="#数据结构" class="headerlink" title="数据结构"></a>数据结构</h3></li></ul></li><li>数组<ul><li>可变</li><li>可索引</li><li>元素必须是相同类型</li></ul></li><li>元组<ul><li>可以容纳不同类型</li></ul></li><li>容器<ul><li>定义序列、映射、集合等数据结构</li></ul></li><li>序列<ul><li>列表list<ul><li>采用链表结构，注意时间复杂度</li><li>一旦定义便不可变</li><li>元素必须是相同类型</li></ul></li><li>range<ul><li>带索引的不可变数字等差数列</li></ul></li></ul></li><li>集合<ul><li>无索引</li><li>元素不重复</li></ul></li><li>映射<ul><li>推荐使用get()方法<h3 id="面向对象编程"><a href="#面向对象编程" class="headerlink" title="面向对象编程"></a>面向对象编程</h3></li></ul></li><li>类（class）<ul><li>理解为“模板”，定义好类之后就能通过new关键字创建对象</li><li>语法：def 方法名(参数列表): 返回结果类型={方法体}<ul><li>所有的方法都是不可变的</li><li>允许方法重载</li></ul></li><li>可见性：privated成员只对本类型和嵌套类型可见，protected成员对本类型和其他继承类型都可见</li><li>主构造器和辅助构造器this区别：辅助构造器返回的类型为Unit、辅助构造器最终都始于对主构造器的调用</li></ul></li><li>对象（object）<ul><li>单例对象的定义与类的定义类似，只是用object关键字替换了class关键字</li><li>无法被实例化，单例</li><li>单例对象与某个类具有相同的名称时，则单例对象被称为“伴生对象”，这个类叫做“伴生类”</li><li>伴生对象和伴生类必须位于同一个文件下</li><li>调用伴生类的apply方法创建实例无需使用new关键字</li><li>apply方法也被称为工厂方法，定义在类中实现自动调用，当用户在创建实例时，无需使用new关键字就能生成对象</li></ul></li><li>继承<ul><li>如果一个类包含没有实现的成员，则必须使用abstract修饰，并定义为抽象类</li><li>子类的主构造器必须调用父类的主构造器或者辅助构造器<h3 id="函数式编程"><a href="#函数式编程" class="headerlink" title="函数式编程"></a>函数式编程</h3></li></ul></li><li>映射操作<ul><li>map</li><li>flatmap</li></ul></li><li>规约操作<ul><li>reduce</li><li>flod<h2 id="第三章：Spark的设计与运行原理"><a href="#第三章：Spark的设计与运行原理" class="headerlink" title="第三章：Spark的设计与运行原理"></a>第三章：Spark的设计与运行原理</h2><h3 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h3></li></ul></li><li>RDD<ul><li>RDD采用惰性计算，在“行动操作”之前不会发生真正的计算</li><li>RDD是一种数据抽象，它代表了一系列的转换处理，省略了大量的中间结果的存储，降低IO的开销，所以速度才会很快</li><li>容错性高</li><li>RDD分区之间会有不同的依赖关系，DAG调度器则会根据RDD之间的宽依赖或者窄依赖关系把DAG图划分为不同的阶段</li></ul></li><li>DAG（有向无环图）</li><li>Executor<ul><li>多线程</li><li>io性能高</li></ul></li><li>应用（application）<ul><li>作业（Job）<ul><li>阶段（Stage）<ul><li>任务（Task）</li></ul></li></ul></li></ul></li><li>依赖关系<ul><li>宽依赖：RDD之间是一对多的关系，所以失败之后恢复的开销比较大</li><li>窄依赖：RDD之间是一对一的关系，失败之后恢复更加高效</li><li>DAG调度器遇到宽依赖就断开，因为窄依赖不会设计shuffle可以实现流水线操作</li></ul></li><li>部署模式<ul><li>客户端模式</li><li>集群模式<h3 id="架构设计"><a href="#架构设计" class="headerlink" title="架构设计"></a>架构设计</h3></li></ul></li><li>Spark+YARN：首先创建一个SparkContext对象，然后到YARN申请资源，YARN给Excutor分配好资源之后，启动Excutor进程，SparkContext构建DAG图并分解为多个阶段的任务，并把任务分发给Excutor执行。<h2 id="第五章：RDD编程"><a href="#第五章：RDD编程" class="headerlink" title="第五章：RDD编程"></a>第五章：RDD编程</h2><h3 id="核心概念"><a href="#核心概念" class="headerlink" title="核心概念"></a>核心概念</h3></li><li>RDD是Spark的核心概念，它是一个只读的、可分区的分布式数据集，这个数据集的全部或部分可以缓存在内存中，可在多次计算间重用<h3 id="如何创建"><a href="#如何创建" class="headerlink" title="如何创建"></a>如何创建</h3></li><li>从文件系统中创建</li><li>通过并行集合创建：sc.parallelize()<h3 id="操作"><a href="#操作" class="headerlink" title="操作"></a>操作</h3></li><li>转换<ul><li>flat(),map(),flatMap(),groupByKey(),reduceByKey()</li></ul></li><li>行动<h3 id="持久化-persist"><a href="#持久化-persist" class="headerlink" title="持久化(persist)"></a>持久化(persist)</h3><h3 id="为什么要分区？"><a href="#为什么要分区？" class="headerlink" title="为什么要分区？"></a>为什么要分区？</h3></li><li>增加并行度</li><li>减少通信开销</li><li>RDD 分区的一个原则是使分区的个数尽量等于集群中的 CPU 核心（Core）数目。</li><li>可以手动设置分区的数量，主要包括两种方式：（1）创建RDD时手动指定分区个数；（2）使用reparititon方法重新设置分区个数。</li><li>自定分区的方法<ul><li>哈希分区</li><li>区域分区<h2 id="第七章：Spark-Streaming"><a href="#第七章：Spark-Streaming" class="headerlink" title="第七章：Spark Streaming"></a>第七章：Spark Streaming</h2><h3 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h3></li></ul></li><li>静态数据和流数据</li><li>批量计算和实时计算</li><li>数据分散存储在不同的机器上，因此需要实时汇总来自不同机器上的日志数据。<h3 id="数据源"><a href="#数据源" class="headerlink" title="数据源"></a>数据源</h3></li><li>Kafka<ul><li>Broker：Kafka集群的服务器</li><li>Topic：Kafka消息的类别</li><li>Partition：每个Topic包含一个或者多个Partition</li><li>Producer：生产者，负责发布消息到Broker</li><li>Consumer：消费者</li><li>Consumer Group</li></ul></li><li>Flume</li><li>HDFS<h3 id="执行流程"><a href="#执行流程" class="headerlink" title="执行流程"></a>执行流程</h3></li><li>离散化的数据流（DStream）被切分成一段一段，每段数据转换为RDD，然后可以针对RDD做相对应的操作，对DStream的操作最终都转化为对RDD的操作</li></ul><h3 id="对比Storm的区别在与Spark-Streaming无法实现毫秒级响应，但是RDD的容错率更高效"><a href="#对比Storm的区别在与Spark-Streaming无法实现毫秒级响应，但是RDD的容错率更高效" class="headerlink" title="对比Storm的区别在与Spark Streaming无法实现毫秒级响应，但是RDD的容错率更高效"></a>对比Storm的区别在与Spark Streaming无法实现毫秒级响应，但是RDD的容错率更高效</h3><h1 id="思维导图"><a href="#思维导图" class="headerlink" title="思维导图"></a>思维导图</h1><p><img src="https://raw.githubusercontent.com/swordspoet/i/img/Spark-Scala-LZY.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;记录了部分章节的笔记，这本书语言平时易懂，看得出来作者是很用心地写书，不是照着官方文档翻译，我推荐林子雨老师的这本书作为入门Spark的读物。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Spark" scheme="https://www.libinx.com/categories/Spark/"/>
    
    
      <category term="Spark" scheme="https://www.libinx.com/tags/Spark/"/>
    
      <category term="Scala" scheme="https://www.libinx.com/tags/Scala/"/>
    
      <category term="Spark编程基础" scheme="https://www.libinx.com/tags/Spark%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80/"/>
    
  </entry>
  
  <entry>
    <title>一次配置深度学习主机和安装TensorFlow2.0 GPU的经历</title>
    <link href="https://www.libinx.com/2019/experience-of-deep-learning-computer-and-tensorflow-gpu/"/>
    <id>https://www.libinx.com/2019/experience-of-deep-learning-computer-and-tensorflow-gpu/</id>
    <published>2019-11-20T15:11:45.000Z</published>
    <updated>2021-03-23T01:25:10.996Z</updated>
    
    <content type="html"><![CDATA[<p>记录一次配置深度学习主机和安装TensorFlow2.0 GPU的经历。</p><p><img src="https://camo.githubusercontent.com/0905c7d634421f8aa4ab3ddf19a582572df568e1/68747470733a2f2f7777772e74656e736f72666c6f772e6f72672f696d616765732f74665f6c6f676f5f736f6369616c2e706e67" alt=""></p><a id="more"></a><h1 id="配置深度学习主机"><a href="#配置深度学习主机" class="headerlink" title="配置深度学习主机"></a>配置深度学习主机</h1><p>工欲善其事必先利其器，既然是深度学习，如果没有GPU加持那还算是深度学习吗？不过，GPU向来价格比较昂贵，一张近一年内新上市的GPU价格动辄几千，假如你不是专门的研究者或者没有企业级别的需求，我的建议是没有必要暂时花费这笔钱，因为我自己之前就吃过这些苦头，当时一门心思买昂贵的设备用来学习，可是大多是时候都是在吃灰，自己的数据集根本不需要那么多资源。</p><p>所以，我的建议是，我们可以选择那些上市时间有两年但是算力还不错的二手显卡，如果能买到全新的那就更好了，为什么这么做？因为你想想，前两年深度学习的任务不也是使用的这些设备吗，这些GPU一样在那时也可以满足我们的需求，当然，二手的显卡需要我们仔细甄别，最好选择那些信誉、评价比较高而且真实的卖家。</p><p>当我配置主机的时候我一直保持几个原则：</p><ol><li><strong>实用</strong>。比如RGB灯光和五颜六色的装饰真的不会给你的内存条带来什么加成，内存条就是内存条，它不是装饰；另外，CPU能买散片就买散片，为什么？如果现在咱们能具备仿制出CPU的能力也不会受制于人了。</li><li><strong>货比三家</strong>。同一件商品在不同平台和不同卖家的售价往往有很大的区别，在厂家品控合格的前提下，购买低价的一般不会翻车。</li><li><strong>需求</strong>。自身的需求，我看了许多的装机视频，有些人动辄就是花费上万购置设备，装完机之后就是用来看看视频和刷刷网页，那我看就完全没有必要了。</li></ol><p>考虑用于深度学习的机器，无非有两种选择：笔记本或者台式机，前者便携省心，但价格相对昂贵，后者不易携带，然而具备价格优势，这两者似乎天生具备不可调和的矛盾。那么会不会有一种方案能同时满足这两个优势呢，既便携且价格不那么昂贵，答案是肯定的。我也是这一阵在搜集资料的时候才发现的，居然还有itx机箱这个选择，就是支持的主板为17cm*17cm尺寸大小，整机可以放入一般的双肩背包。</p><p>主机中花费最多的部分就是CPU和GPU了，这两者几乎占据了装机费用的一半，所以你的心理预期与这两个因素强烈相关，比如以我为例，刚开始我的心理预期是4000左右，现在一般的CPU散片至少得1000了，所以这就决定了我的显卡上不能投入太多，我一开始的选择是gtx750ti，算力5.0，能满足一般的需求，但是显存太小了，而且这块显卡还是二手的。</p><p>后来我把我的配置单发到V站社区，社区的给我的反馈是GPU的显存太小了，做不了什么用，至少得1060，后来我权衡了一下确实2G的显存有点小。所以，我改变了预计的方案，显卡换成了二手的微星红龙1060，这张显卡我在16年装机的时候就购入过，品控方面经得住考验，只是二手的微星红龙现在还能卖1000出头。</p><p>但是后来发现微星红龙显卡跟我的itx机箱不匹配，它太宽了，我的机箱支持的显卡宽度最大为13.2cm，而微星红龙的宽度将近14cm，所以死活也放不进机箱，无奈只能临时把微星红龙退了（红龙的造型真的很酷啊！！！），火速买了一张全新的映众1060显卡，因为我看了评测，这款显卡的散热比较强，我觉得与体积比较小的mini机箱比较匹配。</p><p>CPU一开始的选择是9400，后来发现我没有用核显的必要，所以就选择了9400f，也就是不带核显的版本，免除后续安装驱动的烦恼。</p><p>所以，整个下来我这台用于深度学习的主机的配置大致是：i5 9400f+16G内存+480G SSD+GTX 1060 6G+sanc 2k显示器，整个花费差不多5000元。</p><h1 id="在Ubuntu18-04-1上安装TensorFlow2-0-GPU"><a href="#在Ubuntu18-04-1上安装TensorFlow2-0-GPU" class="headerlink" title="在Ubuntu18.04.1上安装TensorFlow2.0-GPU"></a>在Ubuntu18.04.1上安装TensorFlow2.0-GPU</h1><p>因为我配置这台机器的目的是为了接触TensorFlow2.0，所以使用GPU版本的TensorFlow2.0是必须的，我日常的开发环境均是基于Linux桌面版，而英伟达支持的桌面发行版又仅仅限于几类。在这之前我曾经尝试在deepin上安装驱动，结果把系统给整崩溃了，所以我告诫各位不要在日常开发环境和英伟达不支持的桌面环境下安装驱动，你永远不知道前面会有多少坑要踩。</p><p>CUDA开发环境依赖于与主机开发环境（包括主机编译器和C runtime库）的紧密集成，因此仅在已获得此CUDA Toolkit版本合格的Linux发行版中受支持，换言之，如果NVIDIA官方不支持你使用的Linux发行版，你也无法使用CUDA加速！</p><p>不得不说，在Linux平台装英伟达的驱动是一个非常劳心费神的事情，这中间有无数的坑等着你去踩，如果你使用的Windows可能半天就弄明白了，如果你是Linux用户那个在这个基础上再乘以5吧，我花了差不多20多个小时才弄明白，中间系统重装了5、6次，所以这也是为什么我建议你在一台全新机器上进行这些实验，千万不要在开发环境中尝试这些操作，它有可能导致无法开机和资料丢失的情况发生。</p><p><strong>五个认知：</strong></p><ul><li>本教程仅仅针对<strong>Ubuntu18.04.1</strong>的单显卡用户，即无核显，只有独显</li><li>CUDA自带显卡驱动，所以只需要安装好了CUDA之后附带英伟达的驱动也随之安装成功，<code>nvidia-smi</code>命令可以检验驱动是否安装成功</li><li><strong>强烈建议</strong>你在一台装有全新的Ubuntu系统的设备上安装GPU驱动！！！<strong>强烈建议</strong>Ubuntu的安装的版本为<strong>18.04.1</strong>，而不是最新发布的是18.04.3，最新版本的18.04.3内核版本（5.0.0）过高，而支持CUDA10.0的内核是低于它的</li><li>CUDA版本已经升级到了10.1，<strong>然而TensorFlow2.0目前只支持CUDA10.0</strong>，所以你如果想要在TensorFlow2.0中使用GPU加速，那么你别无选择，<strong>只能安装CUDA10.0！！！</strong></li><li>英伟达官方对Linux社区的支持力度太弱了，so，Fuck NVIDIA!!!</li></ul><p><strong>三个前提：</strong></p><ol><li><strong>拥有至少一张或者多张支持CUDA加速的GPU</strong>：NVIDIA官方列出了支持CUDA加速的GPU设备列表（<a href="https://developer.nvidia.com/cuda-gpus" target="_blank" rel="noopener">CUDA GPUs | NVIDIA Developer</a>），你可以根据你的<strong>显卡型号</strong>查看是否支持CUDA加速已经当前的算力。我当前是一张GTX1060 6G的显卡，算力6.1<ul><li><code>lspci | egrep &#39;3D|VGA&#39;</code>：查看你当前的核显和独显设备</li></ul></li><li><strong>你需要在TensorFlow2.0中获得GPU加速吗？</strong>如果是，那么目前你只能安装CUDA10.0；如果不是，你仅仅是PyTorch用户，那么根据你Ubuntu的内核版本选择CUDA10.0或CUDA10.1</li><li><strong>确认Ubuntu系统版本和内核版本，并选择安装相对应的CUDA版本（这一步非常重要！！！）</strong>：比如都是Ubuntu 18.04（18.04.1），<strong>CUDA10.0就只支持4.15内核版本</strong>，如果你安装的是最新的Ubuntu18.04（18.04.3），<strong>其内核版本是5.0.0，所以你需要安装CUDA10.1</strong>。否则会报错，并提示你要安装最新版本的驱动：<code>NVIDIA-SMI has failed because it couldn&#39;t communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.</code><ul><li>CUDA10.0支持的Linux系统和内核版本：<a href="https://docs.nvidia.com/cuda/archive/10.0/cuda-installation-guide-linux/index.html" target="_blank" rel="noopener">https://docs.nvidia.com/cuda/archive/10.0/cuda-installation-guide-linux/index.html</a></li><li>CUDA10.1（最新）支持的Linux系统和内核版本：<a href="https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html" target="_blank" rel="noopener">https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html</a></li><li><code>uname -r</code>：确认你当前Ubuntu使用的内核版本，我的是<code>4.15.0-20-generic</code></li><li><code>lsb_release -a</code>：确认Ubuntu版本，我的是18.04.1</li><li>所以，我需要安装CUDA10.0版本</li></ul></li></ol><h2 id="八步支持TensorFlow2-0-GPU加速"><a href="#八步支持TensorFlow2-0-GPU加速" class="headerlink" title="八步支持TensorFlow2.0 GPU加速"></a>八步支持TensorFlow2.0 GPU加速</h2><ol><li><p>卸载已有的CUDA和英伟达驱动，如果你是全新系统，请忽略这一步</p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># Purge existign CUDA first</span><br><span class="line">sudo apt --purge remove &quot;cublas*&quot; &quot;cuda*&quot;</span><br><span class="line">sudo apt --purge remove &quot;nvidia*&quot;</span><br></pre></td></tr></table></figure></li><li><p>进入CUDA归档列表：<a href="https://developer.nvidia.com/cuda-toolkit-archive?spm=a2c4e.10696291.0.0.7b5819a4F6rq7s" target="_blank" rel="noopener">CUDA Toolkit Archive | NVIDIA Developer</a>，选择在“两个前提”确认你所要安装的CUDA版本，点击进入，并按照提供的<a href="https://developer.nvidia.com/cuda-10.0-download-archive?target_os=Linux&amp;target_arch=x86_64&amp;target_distro=Ubuntu&amp;target_version=1804&amp;target_type=deblocal" target="_blank" rel="noopener">步骤</a>逐步执行</p><p> | CUDA10.0支持的发行版和内核                                   | CUDA10.1支持的发行版和内核                                   |<br> | —————————————————————————————— | —————————————————————————————— |<br> | <img src="https://raw.githubusercontent.com/swordspoet/i/img/nvidia-support-distributions.png" alt=""> | <img src="https://raw.githubusercontent.com/swordspoet/i/img/nvidia-cuda101-support.png" alt=""> |</p></li></ol><p><img src="https://raw.githubusercontent.com/swordspoet/i/img/cuda10-toolkit.png" alt="https://developer.nvidia.com/cuda-10.0-download-archive?target_os=Linux&amp;target_arch=x86_64&amp;target_distro=Ubuntu&amp;target_version=1804&amp;target_type=deblocal"></p><ol><li>安装Anaconda：清华大学镜像源下载链接:<a href="https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/" target="_blank" rel="noopener">清华大学开源软件镜像站</a>，使用Anaconda安装cuda10.0工具包：<code>conda install cudatoolkit=10.0</code></li><li><p>添加nvcc环境变量（nvcc是英伟达的编译器）：<code>sudo gedit ~/.bashrc</code>，在文件的末尾添加以下三行，保存：</p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">export CUDA_HOME=/usr/local/cuda</span><br><span class="line">export PATH=$PATH:$CUDA_HOME/bin</span><br><span class="line">export LD_LIBRARY_PATH=/usr/local/cuda-10.0/lib64$&#123;LD_LIBRARY_PATH:+:$&#123;LD_LIBRARY_PATH&#125;&#125;</span><br></pre></td></tr></table></figure></li></ol><ol><li>执行<code>source ~/.bashrc</code>，使环境变量生效，接下来，验证CUDA编译器是否安装好，输入<code>nvcc -V</code></li><li>重启机器：<code>sudo reboot</code>，终端输入<code>nvidia-smi</code>，可以查看显卡的型号和驱动版本，如果有返回值则证明英伟达的驱动安装成功！然而，此时我们高兴得还为时过早，因为此时的NVIDIA还没有为TensorFlow2.0提供GPU支持，我们还需要安装cuDNN</li><li><p>安装cuDNN7.6.5版本：根据<a href="https://developer.nvidia.com/rdp/cudnn-download?spm=a2c4e.10696291.0.0.1df819a4HJWSTe" target="_blank" rel="noopener">页面</a>选择<strong>CUDA10.0对应cuDNN</strong>，下载：<br> <img src="https://raw.githubusercontent.com/swordspoet/i/img/nvidia-cudnn-support.png" alt=""></p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># 解压，并把文件拷贝到一个新的文件夹下：/home/yourname/software/cudnn/</span><br><span class="line">tar -zxvf cudnn-10.0-linux-x64-v7.6.5.32.tgz</span><br><span class="line"># 设置环境变量</span><br><span class="line">sudo vi ~/.bashrc</span><br><span class="line"># 末尾添加</span><br><span class="line">export LD_LIBRARY_PATH=&quot;/home/yourname/software/cudnn/lib64:$LD_LIBRARY_PATH&quot;</span><br><span class="line"># 环境变量生效</span><br><span class="line">source ~/.bashrc</span><br></pre></td></tr></table></figure></li><li><p><code>pip install tensorflow-gpu</code>，终端打开<code>iPython</code>，验证TensorFlow2.0是否已经获得GPU支持</p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line"># 如果返回true则表示TensorFlow2.0已经获得GPU支持，成功</span><br><span class="line">tf.test.is_gpu_available()</span><br></pre></td></tr></table></figure></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;记录一次配置深度学习主机和安装TensorFlow2.0 GPU的经历。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://camo.githubusercontent.com/0905c7d634421f8aa4ab3ddf19a582572df568e1/68747470733a2f2f7777772e74656e736f72666c6f772e6f72672f696d616765732f74665f6c6f676f5f736f6369616c2e706e67&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="DeepLearning" scheme="https://www.libinx.com/categories/DeepLearning/"/>
    
    
      <category term="深度学习" scheme="https://www.libinx.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="装机" scheme="https://www.libinx.com/tags/%E8%A3%85%E6%9C%BA/"/>
    
      <category term="tensorflow2.0" scheme="https://www.libinx.com/tags/tensorflow2-0/"/>
    
      <category term="tensorflow-gpu" scheme="https://www.libinx.com/tags/tensorflow-gpu/"/>
    
      <category term="深度学习主机" scheme="https://www.libinx.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%BB%E6%9C%BA/"/>
    
      <category term="cuda10.0" scheme="https://www.libinx.com/tags/cuda10-0/"/>
    
      <category term="nvidia drivers" scheme="https://www.libinx.com/tags/nvidia-drivers/"/>
    
      <category term="nvidia" scheme="https://www.libinx.com/tags/nvidia/"/>
    
      <category term="英伟达驱动安装" scheme="https://www.libinx.com/tags/%E8%8B%B1%E4%BC%9F%E8%BE%BE%E9%A9%B1%E5%8A%A8%E5%AE%89%E8%A3%85/"/>
    
  </entry>
  
  <entry>
    <title>Scala Spark取出DataFrame中列中的值</title>
    <link href="https://www.libinx.com/2019/getting-values-from-dataframe-in-spark/"/>
    <id>https://www.libinx.com/2019/getting-values-from-dataframe-in-spark/</id>
    <published>2019-11-20T14:33:57.000Z</published>
    <updated>2021-03-23T01:25:10.996Z</updated>
    
    <content type="html"><![CDATA[<p>Scala Spark取出DataFrame中列中的值</p><a id="more"></a><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> df = <span class="type">Seq</span>(</span><br><span class="line">     |     (<span class="string">"one"</span>, <span class="number">2.0</span>),</span><br><span class="line">     |     (<span class="string">"two"</span>, <span class="number">1.5</span>),</span><br><span class="line">     |     (<span class="string">"three"</span>, <span class="number">8.0</span>)</span><br><span class="line">     |   ).toDF(<span class="string">"id"</span>, <span class="string">"val"</span>)</span><br><span class="line">df: org.apache.spark.sql.<span class="type">DataFrame</span> = [id: string, <span class="keyword">val</span>: double]</span><br><span class="line"></span><br><span class="line">scala&gt; df.show()</span><br><span class="line">+-----+---+</span><br><span class="line">|   id|<span class="keyword">val</span>|</span><br><span class="line">+-----+---+</span><br><span class="line">|  one|<span class="number">2.0</span>|</span><br><span class="line">|  two|<span class="number">1.5</span>|</span><br><span class="line">|three|<span class="number">8.0</span>|</span><br><span class="line">+-----+---+</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">scala&gt; df.select(<span class="string">"id"</span>).collect().map(_(<span class="number">0</span>)).toList</span><br><span class="line">res6: <span class="type">List</span>[<span class="type">Any</span>] = <span class="type">List</span>(one, two, three)</span><br><span class="line"></span><br><span class="line">scala&gt; df.select(<span class="string">"id"</span>).rdd.map(_(<span class="number">0</span>)).collect.toList </span><br><span class="line">res7: <span class="type">List</span>[<span class="type">Any</span>] = <span class="type">List</span>(one, two, three)                                         </span><br><span class="line"></span><br><span class="line">scala&gt; df.select(<span class="string">"id"</span>).map(_.getString(<span class="number">0</span>)).collect.toList </span><br><span class="line">res8: <span class="type">List</span>[<span class="type">String</span>] = <span class="type">List</span>(one, two, three)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Scala Spark取出DataFrame中列中的值&lt;/p&gt;
    
    </summary>
    
    
      <category term="Spark" scheme="https://www.libinx.com/categories/Spark/"/>
    
    
      <category term="Spark" scheme="https://www.libinx.com/tags/Spark/"/>
    
      <category term="Scala" scheme="https://www.libinx.com/tags/Scala/"/>
    
      <category term="SparkSQL" scheme="https://www.libinx.com/tags/SparkSQL/"/>
    
  </entry>
  
  <entry>
    <title>浅析Spark Architecture：Shuffle（二）</title>
    <link href="https://www.libinx.com/2019/spark-architecture-shuffle-02/"/>
    <id>https://www.libinx.com/2019/spark-architecture-shuffle-02/</id>
    <published>2019-10-25T04:47:13.000Z</published>
    <updated>2021-03-23T01:25:10.996Z</updated>
    
    <content type="html"><![CDATA[<p>在<a href="https://www.libinx.com/2019/spark-architecture-shuffle-01/">浅析 Spark Architecture：Shuffle（一） | Thinking Realm</a>这篇文章中我主要向大家介绍了Spark Shuffle的运行原理和随着Spark升级导致Shuffle运行机制的变化。</p><p>而这篇文章主要介绍在Spark中哪些操作会触发Shuffle、Shuffle的bypassMergeThreshold运行机制和4个与Shuffle相关的参数。</p><a id="more"></a><h1 id="何时会触发Spark-Shuffle操作？"><a href="#何时会触发Spark-Shuffle操作？" class="headerlink" title="何时会触发Spark Shuffle操作？"></a>何时会触发Spark Shuffle操作？</h1><p>首先，从字面上来理解，Shuffle的意思就是“洗牌”，就是要把原来混乱的数据重新整理，而往往数据又不是分布在同一个地方的，在这个过程中必然会涉及到数据的移动，所以不难理解Shuffle是一个非常消耗资源的操作，通常可以通过数据分区来降低Shuffle带来的网络传输开销。</p><p>在Spark中，<strong>map</strong>、<strong>filter</strong>、<strong>union</strong>操作不会触发Shuffle操作，因为这些操作都是针对单个数据本身的改变，数据与数据之间并不会发生关联或者交换操作。而诸如分区操作如<strong>repartition 、coalesce</strong>或者<strong>groupByKey</strong>、<strong>sortByKey</strong>等<strong>ByKey</strong>的操作一般会触发Shuffle，groupByKey会对数据做分组处理，而sortByKey需要比较数据与数据之间的先后顺序。</p><div class="table-container"><table><thead><tr><th>类型</th><th></th></tr></thead><tbody><tr><td>repartition</td><td><code>repartition</code>or<code>coalesce</code></td></tr><tr><td>ByKey</td><td><code>groupByKey</code>or<code>sortByKey</code></td></tr><tr><td>join</td></tr></tbody></table></div><p>推荐两个链接：第一个说的是partitionBy和repartition之间的区别，第二个解释在Spark中哪些操作会引发Shuffle。</p><p><a href="http://tantusdata.com/spark-shuffle-case-1-partition-by-and-repartition/" target="_blank" rel="noopener">Spark shuffle – Case #1 – partitionBy and repartition – Tantus Data</a></p><p><a href="https://stackoverflow.com/questions/31386590/when-does-shuffling-occur-in-apache-spark" target="_blank" rel="noopener">mapreduce - When does shuffling occur in Apache Spark? - Stack Overflow</a></p><h1 id="Spark（-gt-1-2-0）Shuffle的bypassMergeThreshold运行机制"><a href="#Spark（-gt-1-2-0）Shuffle的bypassMergeThreshold运行机制" class="headerlink" title="Spark（&gt;1.2.0）Shuffle的bypassMergeThreshold运行机制"></a>Spark（&gt;1.2.0）Shuffle的bypassMergeThreshold运行机制</h1><p>从Spark 1.2.0开始，Spark Shuffle默认的算法便变为了sort，可以通过<strong><em>spark.shuffle.manager</em></strong>选择相应的Shuffle算法。在上一篇文章中有提到过，Sort Shuffle的原理与Hadoop Shuffle有着相似的实现逻辑，Map端只会输出两个文件，分别是数据文件和记录结果数据的索引文件，由此，Reduce端就很容易根据索引文件找到记录结果的数据文件位置。</p><p>值得注意的是，最新版本的Spark在Sort Shuffle机制也并不完全只是Sort Based，在SortShuffleManager下有一个<strong><em>spark.shuffle.sort.bypassMergeThreshold</em></strong>参数比较有意思，它主要用于决定当Reduce端的任务不超过Threshold值的时候采用类似Hash Based的Shuffle机制，即直接将Map端的文件先分别写入单独的文件，但是它又跟Hash Based不完全相同，它在最后一步还是会将这些文件合并成为一个单独的文件。</p><p>举个例子比较好理解，如果说你要从A城市出发去B城市，现有两种选择：打车和坐火车，打车比较灵活适合中短距离，距离太远则不经济，火车价格低廉适合距离长距离。如果A城市和B城市之间的距离大约50公里以内，那么我建议你还是打车比较合理，毕竟打车比较灵活，可以决定自己的时间。而当距离超过100公里，那现在就有必要考虑坐火车了。</p><p>Hash Based Shuffle之于Sort Based Shuffle正如打车和 坐火车的关系，Hash Based适合数据量不是特别大的计算任务，此时它会比Sort Based更快；而数据量很大的情况下，Sort Based就更胜一筹，Hash Based会把大量的Map结果写入内存，会相当耗费资源，给GC造成了巨大的压力，得不偿失。下图描述了bypassMergeThreshold运行机制下SortShuffleManager选择类似Hash Based的Shuffle原理（图片来源：<a href="https://tech.meituan.com/2016/05/12/spark-tuning-pro.html" target="_blank" rel="noopener">Spark性能优化指南——高级篇 - 美团技术团队</a>）。</p><p><img src="https://raw.githubusercontent.com/swordspoet/i/img/20191024125936.png" alt=""></p><p>Spark Shuffle的<strong><em>spark.shuffle.sort.bypassMergeThreshold</em></strong>参数正是为了兼顾Hash Shuffle在小数据集上的优异表现而设置的，<strong><em>spark.shuffle.sort.bypassMergeThreshold</em></strong>参数默认为200，当Map端的任务数量<strong>小于200时</strong>，此时的Shuffle选择的是Hash Shuffle，也就是先将<strong>大量的</strong>中间数据文件<strong>写入内存并且不排序</strong>，只是在最后每个Map task都会把中间的数据文件再汇总为一个数据文件给Reducer，这样一来大大提高运行的效率。</p><p>所以，我这里给出的建议是，如果集群的GC压力比较大，并且处理的是需要进行排序的Shuffle操作比如sortBy，可以适当地减小<strong>bypassMergeThreshold</strong>的值，选择Sort Based Shuffle。</p><h1 id="与Spark-Shuffle调优相关的参数"><a href="#与Spark-Shuffle调优相关的参数" class="headerlink" title="与Spark Shuffle调优相关的参数"></a>与Spark Shuffle调优相关的参数</h1><ol><li><code>spark.shuffle.spill</code>：溢写操作，默认打开，决定当内存不够用时将数据临时写入磁盘</li><li><code>spark.shuffle.memoryFraction</code>：决定了当Shuffle过程中使用的内存达到总内存多少比例的时候开始启动溢写（spill）操作，默认是0.2，也就是说，Executor默认只有20%的内存用来进行该操作。shuffle操作在进行聚合时，如果发现使用的内存超出了这个20%的限制，那么多余的数据就会溢写到磁盘文件中去，此时就会极大地降低性能。建议当内存不够用的时候可以适当地降低这个参数的值，可以避免出现OOM</li><li><code>spark.storage.memoryFraction</code>：用于设置RDD能在Executor内存中持久化能占的比例，默认是0.6，当代码中的持久化操作比较多时，可以适当提高该参数的值，反之，当GC频繁导致内存不够用的话，就需要降低该参数，将数据直接写入磁盘</li><li><code>spark.shuffle.spill.compress / spark.shuffle.compress</code>：决定是否对<strong>Spill的中间数据</strong>，<strong>最终的shuffle输出文件</strong>进行压缩操作，默认打开</li></ol><p>在默认情况下，Spark 会使用 60％的空间来存储 RDD，20% 存储数据混洗操作产生的数据，剩下的 20% 留给用户程序。用户可以自行调节这些选项来追求更好的性能表现。如果用户代码中分配了大量的对象，那么降低 RDD 存储和数据混洗存储所占用的空间可以有效避免程序内存不足的情况。</p><h1 id="推荐阅读"><a href="#推荐阅读" class="headerlink" title="推荐阅读"></a>推荐阅读</h1><ol><li><a href="https://spark-config.readthedocs.io/en/latest/shuffle.html#spark-shuffle-sort-bypassmergethreshold" target="_blank" rel="noopener">Shuffle 相关 — SparkConfig 0.1 documentation</a></li><li><a href="https://blog.imaginea.com/spark-shuffle-tuning/" target="_blank" rel="noopener">SPARK SHUFFLE TUNING – experience@imaginea</a></li><li><a href="http://apache-spark-developers-list.1001551.n3.nabble.com/Why-is-spark-shuffle-sort-bypassMergeThreshold-200-td20379.html" target="_blank" rel="noopener">Apache Spark Developers List - Why is spark.shuffle.sort.bypassMergeThreshold 200?</a></li><li><a href="https://spark.apache.org/docs/latest/rdd-programming-guide.html#shuffle-operations" target="_blank" rel="noopener">RDD Programming Guide - Spark 2.4.4 Documentation</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在&lt;a href=&quot;https://www.libinx.com/2019/spark-architecture-shuffle-01/&quot;&gt;浅析 Spark Architecture：Shuffle（一） | Thinking Realm&lt;/a&gt;这篇文章中我主要向大家介绍了Spark Shuffle的运行原理和随着Spark升级导致Shuffle运行机制的变化。&lt;/p&gt;
&lt;p&gt;而这篇文章主要介绍在Spark中哪些操作会触发Shuffle、Shuffle的bypassMergeThreshold运行机制和4个与Shuffle相关的参数。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Spark" scheme="https://www.libinx.com/categories/Spark/"/>
    
    
      <category term="bigdata" scheme="https://www.libinx.com/tags/bigdata/"/>
    
      <category term="spark" scheme="https://www.libinx.com/tags/spark/"/>
    
      <category term="mapreduce" scheme="https://www.libinx.com/tags/mapreduce/"/>
    
      <category term="shuffle" scheme="https://www.libinx.com/tags/shuffle/"/>
    
      <category term="spark shuffle" scheme="https://www.libinx.com/tags/spark-shuffle/"/>
    
      <category term="hadoop shuffle" scheme="https://www.libinx.com/tags/hadoop-shuffle/"/>
    
      <category term="sort shuffle" scheme="https://www.libinx.com/tags/sort-shuffle/"/>
    
      <category term="hash shuffle" scheme="https://www.libinx.com/tags/hash-shuffle/"/>
    
  </entry>
  
  <entry>
    <title>浅析Spark Architecture：Shuffle（一）</title>
    <link href="https://www.libinx.com/2019/spark-architecture-shuffle-01/"/>
    <id>https://www.libinx.com/2019/spark-architecture-shuffle-01/</id>
    <published>2019-10-20T04:15:24.000Z</published>
    <updated>2021-03-23T01:25:10.996Z</updated>
    
    <content type="html"><![CDATA[<p>作为一个接触Spark将近一年的数据挖掘工程师，Spark在处理海量数据上游刃有余的表现就强烈的吸引着我，当我在使用Spark完成数据项目、训练模型任务的过程中通常会遇到各种各样的问题，相信这些问题每个Spark的新手都会遇到过，有的时候调高driver memory或者executor memory，又或者稍微改动一下代码然后就可以了，但是却始终对其背后的原理不甚了解，这不符合一个合格工程师的应该有的职业素养。</p><a id="more"></a><p><img src="https://raw.githubusercontent.com/swordspoet/i/img/20191020114314.png" alt=""></p><h1 id="如何理解Spark-Shuffle？"><a href="#如何理解Spark-Shuffle？" class="headerlink" title="如何理解Spark Shuffle？"></a>如何理解Spark Shuffle？</h1><p>在MapReduce中，Shuffle作为连接Map和Reduce的桥梁，它是一个非常<strong>占用磁盘I/O</strong>和<strong>网络传输开销</strong>的过程，为什么这么说呢？Shuffle的过程分为Shuffle Write和Shuffle Fetch，其中Write对应的是Map端的写入，Map任务的结果会写入到文件中，而Fetch对应的是Reduce端的读取来自Map端任务的结果数据文件。从而，Shuffle的过程便有了这“一写一读”的操作，所以就不难理解为什么Shuffle为何这么占用磁盘和网络传输了。</p><p>如果对于Spark的新手可能不太理解Shuffle的过程，下面我以<strong>人口普查统计</strong>为例，人口普查统计它肯定不是由国家统计局派出一个团队，然后逐一去各个地区统计，这样统计工作就永远都完不成了。虽然大多数人跟我一样都不是人口统计学专业相关的，但是常识告诉我们，它肯定是国家统计局把各个省的统计局通知到北京来，大家一起开一个会，讨论本次统计工作要完成什么任务，需要统计哪些人口指标，最后各省将各自的统计结果递交给北京汇总。</p><p>Spark中所谓的Shuffle就发生在北京<strong>汇总（聚合）</strong>各省递交统计结果的过程中，具体一点说，比如湖南省和江西省的统计结果中有都有年龄分布和性别分布的统计任务，这两个省份把统计任务完成之后<strong>写成统计报告文件</strong>，然后国家统计局再一个一个<strong>读取省份的统计数据报告</strong>，国家统计局会把年龄分布的数据交给一个小组汇总处理，把性别分布的数据交给另外一个数据处理。从这个例子中，各个省份的数据统计工作就对应<strong>占用磁盘I/O</strong>，大量来自地级市、县级单位、乡镇的统计数据不断<strong>“写入”</strong>省统计局，而向国家统计局递交数据的过程对应<strong>网络传输开销</strong>，不同类别的数据需要送到对应的小组汇总处理。在这个例子中，不难理解省份对应Map端，国家统计局对应Reduce端。</p><h1 id="Hadoop和Spark在Shuffle上的区别"><a href="#Hadoop和Spark在Shuffle上的区别" class="headerlink" title="Hadoop和Spark在Shuffle上的区别"></a>Hadoop和Spark在Shuffle上的区别</h1><p>Hadoop的Shuffle过程是Map端的结果首先被写入缓存，当缓存满了之后就启动溢写操作（spilling process），把缓存的数据再写入磁盘，并清空缓存。而Spark 1.01之后，Spark Shuffle的Map任务就不写入缓存了，<strong>而是直接写入磁盘文件</strong>，最新版本的Spark和老版本的Spark Map端写入磁盘的方式又有所区别。</p><p>再者，两者在Map端处理数据的方式也有区别，Hadoop Shuffle的Map端是Sorted -based，会对数据进行排序和合并了之后再写入磁盘文件。Spark Sorted-Based Shuffle在Mapper 端是排序的，包括partition的排序和每个partition内部元素的排序！<strong>但在 Reducer 端是没有进行排序</strong>，<strong>所以Job的结果默认不是排序的</strong>。Sorted-Based Shuffle采用了 Tim-Sort排序算法，好处是可以极为高效的使用Mapper端的排序成果全局排序。</p><h1 id="新老版本Spark-Shuffle运行的区别"><a href="#新老版本Spark-Shuffle运行的区别" class="headerlink" title="新老版本Spark Shuffle运行的区别"></a>新老版本Spark Shuffle运行的区别</h1><p>在Spark 1.2.0之前的Spark Shuffle默认是Hash based，老版本Spark Shuffle的每一个Map任务会根据Reduce任务的数量创建相应的桶（Bucket），Map的结果会写入到这些桶里面，因此桶的数量是m×r，所以Shuffle的过程会产生大量的小文件，<strong>导致大量内存消耗和GC 的巨大负担</strong>。所以，此时Spark最为紧要的问题是解决Map端生成大量文件的弊端，减轻GC负担。</p><p>而新版本（从Spark 1.2.0开始）的Spark Shuffle逐渐抛弃Hash based，拥抱Sort based（默认），它与Hadoop Shuffle思想一致。<strong>在Spark2.X版本中只有SortShuffleManager，已经没有了Hash-Based Shuffle Manager 了</strong>。它的设计思想则是每个Map任务只创建一个桶，意味着Map任务的结果只写入两个文件，数据文件和索引文件，一个数据文件用于记录Map任务的结果，另外一个索引文件用于记录数据文件中的分区信息。</p><p>所以，对比新老版本的Spark Shuffle，老版本的Shuffle所产生的文件数量是新版本的r/2倍$(\frac{m<em>r}{m</em>2})$，新版本的Spark Shuffle为文件系统减少了不少压力。</p><h1 id="推荐文章"><a href="#推荐文章" class="headerlink" title="推荐文章"></a>推荐文章</h1><ol><li><a href="https://www.cnblogs.com/jcchoiling/p/6440102.html" target="_blank" rel="noopener">[Spark性能调优] 第三章 : Spark 2.1.0 中 Sort-Based Shuffle 产生的内幕 - 無情 - 博客园</a></li><li><a href="https://tech.meituan.com/2016/04/29/spark-tuning-basic.html" target="_blank" rel="noopener">Spark性能优化指南——基础篇 - 美团技术团队</a></li><li><a href="http://people.apache.org/~pwendell/spark-nightly/spark-master-docs/latest/rdd-programming-guide.html#shuffle-operations" target="_blank" rel="noopener">RDD Programming Guide - Spark 2.3.0 Documentation</a></li><li><a href="https://www.cnblogs.com/hseagle/p/3979744.html" target="_blank" rel="noopener">Apache Spark源码走读之24 -- Sort-based Shuffle的设计与实现 - 徽沪一郎 - 博客园</a></li><li><a href="https://github.com/JerryLead/SparkInternals/blob/master/markdown/4-shuffleDetails.md" target="_blank" rel="noopener">SparkInternals/4-shuffleDetails.md at master · JerryLead/SparkInternals · GitHub</a></li><li><a href="https://0x0fff.com/spark-architecture-shuffle/" target="_blank" rel="noopener">Spark Architecture: Shuffle | Distributed Systems Architecture</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;作为一个接触Spark将近一年的数据挖掘工程师，Spark在处理海量数据上游刃有余的表现就强烈的吸引着我，当我在使用Spark完成数据项目、训练模型任务的过程中通常会遇到各种各样的问题，相信这些问题每个Spark的新手都会遇到过，有的时候调高driver memory或者executor memory，又或者稍微改动一下代码然后就可以了，但是却始终对其背后的原理不甚了解，这不符合一个合格工程师的应该有的职业素养。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Spark" scheme="https://www.libinx.com/categories/Spark/"/>
    
    
      <category term="bigdata" scheme="https://www.libinx.com/tags/bigdata/"/>
    
      <category term="spark" scheme="https://www.libinx.com/tags/spark/"/>
    
      <category term="mapreduce" scheme="https://www.libinx.com/tags/mapreduce/"/>
    
      <category term="shuffle" scheme="https://www.libinx.com/tags/shuffle/"/>
    
      <category term="spark shuffle" scheme="https://www.libinx.com/tags/spark-shuffle/"/>
    
      <category term="hadoop shuffle" scheme="https://www.libinx.com/tags/hadoop-shuffle/"/>
    
      <category term="sort shuffle" scheme="https://www.libinx.com/tags/sort-shuffle/"/>
    
      <category term="hash shuffle" scheme="https://www.libinx.com/tags/hash-shuffle/"/>
    
  </entry>
  
  <entry>
    <title>Linux创建IDEA快捷方式</title>
    <link href="https://www.libinx.com/2019/idea-quick-start/"/>
    <id>https://www.libinx.com/2019/idea-quick-start/</id>
    <published>2019-10-11T05:20:01.000Z</published>
    <updated>2021-03-23T01:25:10.996Z</updated>
    
    <content type="html"><![CDATA[<p>先前每次启动idea都要到bin目录下执行<code>./idea.sh</code>脚本，比较麻烦，故直接在桌面创建快捷方式，点击图标便可以直接启动idea。</p><a id="more"></a><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"># 创建快捷方式</span><br><span class="line">touch idea.desktop</span><br><span class="line"></span><br><span class="line"># 编辑此文件</span><br><span class="line">vi idea.desktop</span><br><span class="line"></span><br><span class="line"># 添加以下内容</span><br><span class="line">[Desktop Entry]</span><br><span class="line">Name=IntelliJ Ultimate IDEA  # 桌面上显示应用的名字</span><br><span class="line">Comment=IntelliJ Ultimate IDEA  # 鼠标悬浮在图标上时显示的名字</span><br><span class="line">Exec=/home/lb/software/idea-IU-192.6817.14/bin/idea.sh</span><br><span class="line">Icon=/home/lb/software/idea-IU-192.6817.14/bin/idea.png</span><br><span class="line">Terminal=false</span><br><span class="line">Type=Application</span><br><span class="line">Categories=Developer;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;先前每次启动idea都要到bin目录下执行&lt;code&gt;./idea.sh&lt;/code&gt;脚本，比较麻烦，故直接在桌面创建快捷方式，点击图标便可以直接启动idea。&lt;/p&gt;
    
    </summary>
    
    
      <category term="教程" scheme="https://www.libinx.com/categories/%E6%95%99%E7%A8%8B/"/>
    
    
      <category term="Linux" scheme="https://www.libinx.com/tags/Linux/"/>
    
      <category term="idea" scheme="https://www.libinx.com/tags/idea/"/>
    
      <category term="快捷启动" scheme="https://www.libinx.com/tags/%E5%BF%AB%E6%8D%B7%E5%90%AF%E5%8A%A8/"/>
    
  </entry>
  
  <entry>
    <title>启动Scala REPL报错：java.lang.NoClassDefFoundError:javax/script/Compilable</title>
    <link href="https://www.libinx.com/2019/Scala-repl-error/"/>
    <id>https://www.libinx.com/2019/Scala-repl-error/</id>
    <published>2019-09-16T13:54:44.000Z</published>
    <updated>2021-03-23T01:25:10.996Z</updated>
    
    <content type="html"><![CDATA[<p>启动Scala REPL报错: <code>Exception in thread &quot;main&quot; java.lang.NoClassDefFoundError: javax/script/Compilable</code></p><a id="more"></a><p>原因是Scala不兼容JDK 10，解决方案是安装JDK 8并将其设置为默认使用，即：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ sudo apt-get install openjdk-8-jdk</span><br><span class="line">$ sudo apt-get install openjdk-8-jre</span><br><span class="line">$ sudo update-alternatives --config java</span><br><span class="line">$ sudo update-alternatives --config javac</span><br></pre></td></tr></table></figure><p>选择2所示的JDK 8即可</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">lb@hw:~$ sudo update-alternatives --config java</span><br><span class="line">有 2 个候选项可用于替换 java (提供 /usr/bin/java)。</span><br><span class="line"></span><br><span class="line">  选择       路径                                          优先级  状态</span><br><span class="line">------------------------------------------------------------</span><br><span class="line">* 0            /usr/lib/jvm/java-10-openjdk-amd64/bin/java      1101      自动模式</span><br><span class="line">  1            /usr/lib/jvm/java-10-openjdk-amd64/bin/java      1101      手动模式</span><br><span class="line">  2            /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java   1081      手动模式</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;启动Scala REPL报错: &lt;code&gt;Exception in thread &amp;quot;main&amp;quot; java.lang.NoClassDefFoundError: javax/script/Compilable&lt;/code&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="教程" scheme="https://www.libinx.com/categories/%E6%95%99%E7%A8%8B/"/>
    
    
      <category term="scala" scheme="https://www.libinx.com/tags/scala/"/>
    
      <category term="java" scheme="https://www.libinx.com/tags/java/"/>
    
      <category term="REPL" scheme="https://www.libinx.com/tags/REPL/"/>
    
  </entry>
  
  <entry>
    <title>余弦相似度与皮尔逊相关系数之间的比较</title>
    <link href="https://www.libinx.com/2019/difference-between-cosine-similarity-and-pearson-similarity/"/>
    <id>https://www.libinx.com/2019/difference-between-cosine-similarity-and-pearson-similarity/</id>
    <published>2019-09-09T11:18:15.000Z</published>
    <updated>2021-03-23T01:25:10.995Z</updated>
    
    <content type="html"><![CDATA[<p>余弦相似度（也叫作余弦距离）和皮尔逊相关系数是数据挖掘中很常见的两种相似度计算方式，除此之外还有欧式距离、Jaccard距离、曼哈顿距离等计算方法。</p><p>本文主要讨论余弦相似度和皮尔逊相关系数之间的区别，首先，这两者返回的结果（相似度、相关系数）都介于0到1之间，并且都是值越大代表越相似，直觉上看起来两种算法之间并没有什么区别，然而实际并非如此，<strong>在用户的评分维度差距比较大的场景下，余弦相似度得到的结论可能不太合理</strong>，这篇文章的目的就是为了把余弦相似度和皮尔逊相似系数之间的区别解释清楚。</p><a id="more"></a><h1 id="余弦相似度"><a href="#余弦相似度" class="headerlink" title="余弦相似度"></a>余弦相似度</h1><p>余弦相似度通过计算两个向量的夹角的余弦值来度量它们之间的相似性，在计算文档相似度中应用非常广泛，计算公式如下：</p><script type="math/tex; mode=display">cos(a,b)=\frac{a·b}{||a||·||b||}</script><p>其中，上式中的分子是向量a与向量b的内积，分母是向量a与向量b模的乘积，可以肯定的是分母一定是大于零的，所以余弦相似度的正负性完全由分子决定。</p><p>举个例子，比如向量a为<code>(1,2,4)</code>，向量b为<code>(-1,0,2)</code>，那a和b之间的余弦相似度是这样计算的：</p><p>首先，计算内积：$1*(-1)+2×0+4×2=7$</p><p>然后，计算向量模的乘积：$\sqrt{(1+4+16)}*\sqrt{(1+0+4)}$</p><p>最后相除得向量a与b的余弦相似度为0.683，说明两个向量比较相似。</p><h1 id="调整余弦相似度（修正余弦相似度）"><a href="#调整余弦相似度（修正余弦相似度）" class="headerlink" title="调整余弦相似度（修正余弦相似度）"></a>调整余弦相似度（修正余弦相似度）</h1><p>然而，余弦相似度也有它的不足，比如在推荐业务中通常基于用户对物品的评分计算用户之间的相似度，它无法把不同用户的打分维度也纳入到相似度计算中来。</p><p>比如用户A、B和C对三个物品的评分分别为（1,4,0）、(3,5,1)和(8,9,2)，用余弦相似度计算得到A和B用户之间的相似度为0.874</p><script type="math/tex; mode=display">(1*8+4*9+0*2)/(\sqrt{1^2+4^2+0^2}*\sqrt{8^2+9^2+2^2})</script><p>然而，从数据上来看A用户对这两个物品的喜好程度都不如B，直觉告诉我这两个用户不太可能有这么相似，因为他们的口味相差实在有点大，正如男生和女生对篮球和运动物品的喜好程度是不一样的，所以用上述的计算方法不太合理。</p><p>一般针对评分内容的相似度计算会用到改进后的余弦相似度算法——调整余弦相似度（Adjusted Cosine Similarity），计算公式为：</p><script type="math/tex; mode=display">sim(a,b)=\frac{\sum_{i\in{I}}(R_{i,a}-\overline{R}_i)(R_{i,b}-\overline{R}_i)}{\sqrt{\sum_{i\in{I}}(R_{i,a}-\overline{R}_i)^2}\sqrt{\sum_{i\in{I}}(R_{i,b}-\overline{R}_i)^2}}</script><p>其中$\overline{R}_i$为物品$i$得分的平均值，由上文得物品1的平均得分为4，物品2的平均得分为6，代入上述计算公式：</p><script type="math/tex; mode=display">\frac{(1-4)*(8-4)+(4-6)*(9-6)+(0-1)*(2-1)}{\sqrt{(1-4)^2+(4-6)^2+(0-1)^2}*\sqrt{(8-4)^2+(9-6)^2+(2-1)^2}}</script><p>得调整余弦相似度为-0.996，得出用户A和用户C之间根本就不相似，这与余弦相似度得出的结论完全相反，这是因为A和C对3个商品的偏好差距太大了，余弦相似度没能捕捉到这种差异，调整余弦相似度正好弥补了它的缺陷。</p><p>当然，如果你再计算一遍A和B的余弦相似度和调整余弦相似度，你会发现A和B的差异其实不大，从打分的差异上也可以看出来这一点。</p><p>从计算过程来看，调整余弦相似度和余弦相似度的计算方法非常相似，只是用户对物品的打分都减去了该物品所有得分的平均值，这一步等价于对数据做了标准化的处理。</p><p>另外，把上式中的符号稍微修改一下就能应用于计算物品之间的相似度（把人和物品对调一下），公式如下：</p><script type="math/tex; mode=display">sim(i,j)=\frac{\sum_{u\in{U}}(R_{u,i}-\overline{R}_u)(R_{u,j}-\overline{R}_u)}{\sqrt{\sum_{u\in{U}}(R_{u,i}-\overline{R}_u)^2}\sqrt{\sum_{u\in{U}}(R_{u,j}-\overline{R}_u)^2}}</script><p>其中$\overline{R}_u$为用户$u$打分的平均值。</p><h1 id="皮尔逊相关系数"><a href="#皮尔逊相关系数" class="headerlink" title="皮尔逊相关系数"></a>皮尔逊相关系数</h1><script type="math/tex; mode=display">sim(a,b)=\frac{\sum_{i\in{I}}(R_{i,a}-\overline{R}_a)(R_{i,b}-\overline{R}_b)}{\sqrt{\sum_{i\in{I}}(R_{i,a}-\overline{R}_a)^2}\sqrt{\sum_{i\in{I}}(R_{i,b}-\overline{R}_b)^2}}</script><p>其中，$\overline{R}_a$和$\overline{R}_b$代表各自打分的平均值。</p><p>接着上文，我们试着计算A与C之间的皮尔逊相似系数，$\overline{R}_a$和$\overline{R}_c$分别为1.67和6.33，代入上个式子中</p><script type="math/tex; mode=display">\frac{(1-1.67)*(8-6.33)+(4-1.67)*(9-6.33)+(0-1.67)*(2-6.33)}{\sqrt{(1-1.67)^2+(4-1.67)^2+(0-1.67)^2}*\sqrt{(8-6.33)^2+(9-6.33)^2+(2-6.33)^2}}</script><p>计算得到的结果是0.782，说明a，b两个用户之间的皮尔逊相似度为0.782，相比余弦相似度计算得到用户高得离谱的相似度稍显正常一点。</p><h1 id="对比"><a href="#对比" class="headerlink" title="对比"></a>对比</h1><p>皮尔逊相关系数实际上可以看作是一种特殊的余弦相似度计算方式，如果从公式的外形上给三个算法论资排辈的话，皮尔逊相关系数应该是调整余弦相似度的堂兄弟，是余弦相似度的亲儿子。</p><p>从上文中举的例子，我们也提到过余弦相似度在面对评分维度差距较大的情形下计算会不太准确，所以建议在用户评分维度差异比较大的场景下优先使用调整余弦相似度和皮尔逊相似度，这一步相当于也是先对数据做了标准化处理。</p><ol><li><a href="https://www.zhihu.com/question/19734616/answer/349132554" target="_blank" rel="noopener">如何理解皮尔逊相关系数（Pearson Correlation Coefficient）？ - 微调的回答 - 知乎</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;余弦相似度（也叫作余弦距离）和皮尔逊相关系数是数据挖掘中很常见的两种相似度计算方式，除此之外还有欧式距离、Jaccard距离、曼哈顿距离等计算方法。&lt;/p&gt;
&lt;p&gt;本文主要讨论余弦相似度和皮尔逊相关系数之间的区别，首先，这两者返回的结果（相似度、相关系数）都介于0到1之间，并且都是值越大代表越相似，直觉上看起来两种算法之间并没有什么区别，然而实际并非如此，&lt;strong&gt;在用户的评分维度差距比较大的场景下，余弦相似度得到的结论可能不太合理&lt;/strong&gt;，这篇文章的目的就是为了把余弦相似度和皮尔逊相似系数之间的区别解释清楚。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Machine-Learning" scheme="https://www.libinx.com/categories/Machine-Learning/"/>
    
    
      <category term="统计学" scheme="https://www.libinx.com/tags/%E7%BB%9F%E8%AE%A1%E5%AD%A6/"/>
    
      <category term="相似度" scheme="https://www.libinx.com/tags/%E7%9B%B8%E4%BC%BC%E5%BA%A6/"/>
    
      <category term="余弦相似度" scheme="https://www.libinx.com/tags/%E4%BD%99%E5%BC%A6%E7%9B%B8%E4%BC%BC%E5%BA%A6/"/>
    
      <category term="皮尔逊相似度" scheme="https://www.libinx.com/tags/%E7%9A%AE%E5%B0%94%E9%80%8A%E7%9B%B8%E4%BC%BC%E5%BA%A6/"/>
    
      <category term="推荐业务" scheme="https://www.libinx.com/tags/%E6%8E%A8%E8%8D%90%E4%B8%9A%E5%8A%A1/"/>
    
      <category term="皮尔逊相似系数" scheme="https://www.libinx.com/tags/%E7%9A%AE%E5%B0%94%E9%80%8A%E7%9B%B8%E4%BC%BC%E7%B3%BB%E6%95%B0/"/>
    
  </entry>
  
  <entry>
    <title>2019阅读记录</title>
    <link href="https://www.libinx.com/2019/reading-2019/"/>
    <id>https://www.libinx.com/2019/reading-2019/</id>
    <published>2019-09-07T02:56:15.000Z</published>
    <updated>2021-03-23T01:25:10.995Z</updated>
    
    <content type="html"><![CDATA[<p>阅读在短期内可能不会给你的生活带来任何改变，但是只要你长期坚持下去，你会逐渐爱上这项脑力运动，它给你的回报也会越来越大。</p><p>从去年抛弃朋友圈到今年年初放弃微博，我可以说差不多已经脱离社交网络了，脱离社交网络给我的生活带来最大的改变在于我可以腾出更多的时间在阅读软件上而不是沉迷于微信微博，上半年列出的这些书大多是在通勤的地铁、午间休息和晚上下班回家之后看完的。</p><p>我爱阅读，我爱思考。</p><a id="more"></a><h2 id="穷查理宝典-豆瓣"><a href="#穷查理宝典-豆瓣" class="headerlink" title="穷查理宝典 (豆瓣)"></a><a href="https://book.douban.com/subject/26831789/" target="_blank" rel="noopener">穷查理宝典 (豆瓣)</a></h2><p>芒格非常强调跨学科知识的重要性，如果要预测一个指标，仅仅用一个自变量是不够的，它需要综合物理、生物、数学等多个学科的知识，也就是我们需要一种把问题“函数化”的思维，找准你的问题，定义你的自变量，然后找到一个最佳的模型去拟合它。他在书中特别提到了心理学在投资决策中的作用，优秀的投资人必须得懂点心理学，但也不要被心理学误导。芒格在书的下半部分用了大量篇幅黑了好几把经济学，说经济学仅仅是为了自己的结论好看而枉顾其在现实中的表现。他在书中推荐的书籍我也比较感兴趣。</p><h2 id="叫魂-豆瓣"><a href="#叫魂-豆瓣" class="headerlink" title="叫魂 (豆瓣)"></a><a href="https://book.douban.com/subject/1269182/" target="_blank" rel="noopener">叫魂 (豆瓣)</a></h2><p>叫魂（偷取别人的灵魂）是发生在1768年的一桩可有可无的民事纠纷案件，但是由于当事人涉嫌在官方的控制之外擅自与神灵发生某种联系，从而牵动了统治者的神经，统治者一方面不愿意看到它的存在，另一方面又不方便放下身段与其公开争夺对神的解释权，否则便是承认了它的存在。由于根本就没有人能够偷取别人的灵魂，所以从一开始这场纠纷就注定没有结局，是乾隆一手将它变成了一场彻头彻尾的闹剧，他发现自己根本没有办法掌控偌大的官僚集团，只有不断地呵斥底下的官员去抓取一个子虚乌有的人，自然是乾隆皇帝和官员们一次又一次被打脸，当时官僚体系的弊端通通暴露了出来。</p><p>可以说叫魂是乾隆时代一次对官僚体系的大考吧，考试检验的结果自然是无法直视，对比同时代的西方国家，早就已经在民主的路上走得越来越远，偌大的中国仍然还在纠结于那些威胁统治合法性的力量，差距太大了，其实差距倒不是问题，问题是自己还不知道，要是早一点把这个帝国从睡梦中敲醒就好了！</p><p>乾隆真的好难，下边的官员不对他说真话，他得花费大量的精力辨别官员报告的真假，有什么破解之法？如果说缺乏监督，那会也有都察院，可为什么不能起作用，都流于形式了呢？</p><h2 id="增长黑客-豆瓣"><a href="#增长黑客-豆瓣" class="headerlink" title="增长黑客 (豆瓣)"></a><a href="https://book.douban.com/subject/27593848/" target="_blank" rel="noopener">增长黑客 (豆瓣)</a></h2><p>对于不知道如何做用户增长的小白是一本不错的入门书，肖恩介绍一些浅显的增长概念、帮助增长的手段很有用，看完后值得偶尔去翻一翻。这本书是我年初的时候在地铁上看完的，可以说对于一个想要把自己的水平提升一个档次的公司，这本书是一本不错的参考书，当然，最主要的还是取决于公司的控制人是否有数据驱动、不断尝试的意识。以后，除了技术方面的书籍，产品设计、用户增长的书籍也要多多接触。</p><h2 id="显微镜下的大明-豆瓣"><a href="#显微镜下的大明-豆瓣" class="headerlink" title="显微镜下的大明 (豆瓣)"></a><a href="https://book.douban.com/subject/30414743/" target="_blank" rel="noopener">显微镜下的大明 (豆瓣)</a></h2><p>每个小人物都是明帝国的一个红细胞，源源不断地为他输送血液，只是由官僚机构组成的各个器官之间为争夺血液打得不可开交，久而久之，大脑便供血不足，等到大脑极度缺血那一天也是帝国倒下之时。从这本书开始逐渐对明朝的历史有了一点兴趣。</p><h2 id="13-67-豆瓣"><a href="#13-67-豆瓣" class="headerlink" title="13 67 (豆瓣)"></a><a href="https://book.douban.com/subject/25897884/" target="_blank" rel="noopener">13 67 (豆瓣)</a></h2><p>在往返的火车上看完了13 67，这是我看过的第一本推理小说，我可以给五分吧！陈浩基以六个案件构成了这本小说，初读可能觉得是相互独立的，然而在时间上实际是串联在一起的，描绘了香港回归前的香港员警和市民之间的景象。我觉得很适合拍成经典的港版警匪片，特别是刚刚看完的最后一章阿七（关振铎）和我（第一节出现的阿棠）合力阻止的一场针对香港警务处长的爆炸事件非常有画面感，还有看到第六章的两位主人公在第一章中时隔多年再次“交手”，不禁暗自赞叹微笑。总之强烈推荐，可能是最开始和最后，我对第一和第六的章节印象最深，火车上网络不好，想到什么再补充。</p><h2 id="十六世纪明代中国之财政与税收-豆瓣"><a href="#十六世纪明代中国之财政与税收-豆瓣" class="headerlink" title="十六世纪明代中国之财政与税收 (豆瓣)"></a><a href="https://book.douban.com/subject/1006419/" target="_blank" rel="noopener">十六世纪明代中国之财政与税收 (豆瓣)</a></h2><p>无财政制度可言：十六世纪明代可以说没有财政制度可言，仅仅只有效率低下的管理而已，有明一代甚至没有出现过一个改革派的人物，无论是皇帝还是大臣始终无比忠诚地遵守着明太祖在开国之初定下的那一套强制对全国的财源进行统一的管理制度，这种过分简化的危险毫无疑问无法适应资源、地貌、经济发展水平存在巨大差异的帝国。过度的简化实际上带来的是无穷的混乱，但是对于各种问题，明朝政府总是采取应付的态度，无意根本解决问题，这就会使这些问题蔓延开来。</p><p>错误的经济思想：明太祖出身于底层，可能是早年的生活让他对贫困有着深刻的认识，从他的低税赋的经济思想中不难看出他的出发点是抱着对百姓的体恤的，然而实际上很多政策却适得其反，百姓并未因此而减轻负担，反而国家税收的减少导致增加的政府管理成本最终全部转移到了底层身上，事实上明代的税赋并不轻松，可谓越减负担越重，这些现实帝国的拥有者可能也意识到了，但没有发现他们有变革的动力。</p><p>其他财政制度、税收政策的细节就不再总结了，毕竟我不是财政专业的研究者，从书中结尾作者罗列的相关资料，黄仁宇在这本书上应该花费了相当多的时间，并且他的大历史观也特别适合混乱的明代财政与税收的话题。</p><h2 id="人类简史-豆瓣"><a href="#人类简史-豆瓣" class="headerlink" title="人类简史 (豆瓣)"></a><a href="https://book.douban.com/subject/25985021/" target="_blank" rel="noopener">人类简史 (豆瓣)</a></h2><p>20万年前，智人在东非演化，走出非洲后上演了一幅波澜壮阔的崛起之路，如果说所到之处寸草不生可能有点夸张，但是他们对于自然的改造能力远远超过了自然自身演变的速度。看看下面的这张图谱，智人占领世界的脚步从未停止：<br>4.5万年　智人抵达澳大利亚。澳大利亚巨型动物绝种。<br>3万年　尼安德特人绝种。<br>1.6万年　智人抵达美洲。美洲巨型动物绝种。<br>1.3万年　弗洛里斯人绝种。智人成为唯一存活的人类物种。<br>智人相对其他物种有一个致命的优势，智人具备能够想象即抽象的能力，它使得智人能够在客观之外抽象出主观的概念，后来便有了“我们”和“他们”，相比局限于实体的尼安德特人，智人的想象力对于他们来说简直是降维打击，毫无还手之力，智人在认知革命中取得了胜利。我觉得这是本书最精彩的部分，后期的论证则有些乏力，摘出几个有意思的话题：<br>农业革命：是人类驯化了植物还是植物驯化了人类？<br>科学革命&amp;科学与帝国的联姻：同是航海探险，无论是规模还是技术能力远胜欧洲的明帝国为什么没有给之后的世界发展进程造成同等规模的影响？是因为天朝本来便可自给自足还是囿于道德规范而不方便放下架子去掠夺？又或者是还是沉浸在自己是世界中心的幻想中？都是比较有意思的问题。<br>在看这个部分的时候一度让我有点像在看高中语文阅读题的错觉。后期发力不足，论述似乎有些乏力，蜻蜓点水般的单薄案例无法支撑如此宏大的论题，强行政治正确的论证也很难有说服力，属于看过就会忘的那种书。听说人类简史是赫拉利简史三部曲之一，作者的另外两本简史，我估计也不会看了，三分不能再多了。</p><h2 id="南腔北调-豆瓣"><a href="#南腔北调-豆瓣" class="headerlink" title="南腔北调 (豆瓣)"></a><a href="https://book.douban.com/subject/30443280/" target="_blank" rel="noopener">南腔北调 (豆瓣)</a></h2><p>把南腔北调看完了，自己对语言的理解有些改变了，语言不仅是沟通的工具，而且还是文化传承、地域差异的一种表现，从语言中发现历史是一个比较有趣的方法。这本书给3分吧，这类随笔类型的作品我更推荐辉格，比如他写的沐猿而冠更有启发性，也可能跟我不是语言学专业有关吧。</p><h2 id="棋王·树王·孩子王-豆瓣"><a href="#棋王·树王·孩子王-豆瓣" class="headerlink" title="棋王·树王·孩子王 (豆瓣)"></a><a href="https://book.douban.com/subject/26734559/" target="_blank" rel="noopener">棋王·树王·孩子王 (豆瓣)</a></h2><p>除了主人公，最佩服的是“脚卵”，这人有股浩然之气，气度不凡。</p><h2 id="中国国家治理的制度逻辑-豆瓣"><a href="#中国国家治理的制度逻辑-豆瓣" class="headerlink" title="中国国家治理的制度逻辑 (豆瓣)"></a><a href="https://book.douban.com/subject/26901114/" target="_blank" rel="noopener">中国国家治理的制度逻辑 (豆瓣)</a></h2><p>这是本年度耗费阅读时长最长的一本书。</p><p>作者在一开始的时候指出，对于中国这样一个人口、地域超级大的规模国家，治理模式是相当复杂的，我并不认为大规模是一种所谓的借口。中国国家目前的治理逻辑相比之前帝国模式并没有太大的区别，依旧是中央集权、资源向上集中的模式。作者根据自己的田野调查，多次讨论了这种模式的弊端和优势，并指出如果不处理弊端的话，卡里斯马权威将再次会登上历史舞台。这本书给我带来最有收获的观点是有关于国家政府权力的合法性解释；以及运动型治理模式解释为什么经常会有各种各样的活动，比如先前的打老虎、近来的扫黑除恶运动；还有作者关于为什么地方政府偏爱资源密集型项目的解释也很精彩。</p><blockquote><p>摘一：<em>有关中国国家规模及其政治意义诸问题在公共舆论界已多有讨论，但在学术界和政策研究界尚未得到应有的重视。在比较研究的众多讨论中，学者经常引用评价不同国家的治理模式，如新加坡模式、日本模式等等。这些讨论大多忽略了一个重要的维度，即国家治理的规模。例如，新加坡整个国家的领土和人口规模约相当于中国的一个中等城市，韩国的人口仅为江苏省人口的三分之二左右，而日本的岛国特点和民族同质性与中国历史形成的辽阔国土、多元文化中心格局也相去甚远。中国国土面积接近整个欧洲，而人口为其两倍强。</em></p><p>摘二： <em>从国家的视角来审视当代中国的政治过程，我们的问题是：如此负荷累累的一统体制是如何维系的？从国家治理的种种实践来看，以下两个机制尤为突出：第一，一个严密有序的官僚组织制度贯彻自上而下的行政命令和政策意图，从而确保不同属地与中央政府的步调一致；第二，以认同中央权力为核心的价值观念制度，在政府内部官员和社会文化中建立和强化对中央政权的向心力（金观涛、刘青峰 2011 ，陈旭麓 1991 ）。简言之，维系一统体制的两个核心组织机制，一是官僚制度，二是观念制度。</em></p><p>摘三： <em>在韦伯的讨论中，“支配是指这样一种情形：支配者所明示的意志（‘命令’）旨在影响他人（被支配者）的行为，且确实对其行为产生了一定社会意义上的影响，即被支配者犹如把命令的内容当作自己行为的准则。从这一关系的被支配者一端来看，这一情形可称为‘顺从’”</em></p><p><em>在家长（产）制支配形式中，传统权威通过人们对家长或首领权力遵从的传统习俗为其合法性提供了基础。因此，弘扬传统的礼节仪式等制度设施不断强化人们对传统权威的认同，延续了传统权威的合法性基础。在官僚制中，法理权威与正式程序之间有着密切关系，故特别强调维护程序公正、法理面前人人平等的基本原则，以此作为其行使权力的合法性基础。</em></p><p><em>卡理斯玛权威建立在领袖的超凡禀赋之上，因此，如何不断创造“奇迹”以显示其超凡禀赋和如何保持“追随者”对这一禀赋的认可与服从，成为其合法性基础的关键所在，也诱发了相应的制度安排。</em></p></blockquote><p>金家政权对应卡理斯玛权威，中国目前对应的是第一种支配形式，以上是韦伯对于三种支配形式的定义</p><blockquote><p>摘四： <em>官僚体制的常规机制越发达，其组织失败越是凸显，从而更有可能诱发运动型治理机制；而后者的启动有可能造成失控局面，打断社会正常节奏，迫使国家治理回归常规轨道之上。我们也看到，运动型治理机制正面临着危机，越来越不适应现代社会的演变，受到社会多元化趋势的挑战。然而，无论国家治理的意识形态如何选择，无论国家领导人如何更替，面临的组织困难都是一样的。欲从根本上解决运动式治理带来的问题和危机，必须找出新的替代机制来应对、治理官僚体制和常规机制的组织失败。若基本治理逻辑未变，替代机制缺失，则运动型治理机制不废。</em></p><p>摘五：<em>官僚体制的常规机制越发达，其组织失败越是凸显，从而更有可能诱发运动型治理机制；而后者的启动有可能造成失控局面，打断社会正常节奏，迫使国家治理回归常规轨道之上。我们也看到，运动型治理机制正面临着危机，越来越不适应现代社会的演变，受到社会多元化趋势的挑战。然而，无论国家治理的意识形态如何选择，无论国家领导人如何更替，面临的组织困难都是一样的。欲从根本上解决运动式治理带来的问题和危机，必须找出新的替代机制来应对、治理官僚体制和常规机制的组织失败。若基本治理逻辑未变，替代机制缺失，则运动型治理机制不废。</em></p><p>摘六： <em>2007年我们的研究人员参与和跟踪了中国中部某省D县计划生育部门年终考核，观察了省、市、县三级政府进行的三次年终考核过程。第二个案例故事发生于环境保护政策领域，2008年我们的研究人员在中国北方某省Y市环境保护局参与和跟踪了该地区接受中央政府和省级政府考核检查的过程。每次考核检查持续三至十五天不等，研究人员全程参与考核检查过程，包括考核检查的制度安排、组织实施过程，尤其是这个过程中上下级政府讨价还价、应对考核后果等诸多策略性行为。在调查过程中，田野研究人员参与政府日常工作如帮助工作人员完成统计报表、撰写报告等，有时还参与了部分非正式活动（如私下聚餐等），也参与了当地政府部门应对来自上级政府的考核检查的过程。</em></p></blockquote><p>组织决策的一统性加剧了执行过程的灵活性：当决策权力以及资源向政府上层机关集中时，自上而下的决策和随之而来的资源分配就更需要依赖漫长的行政链条和基层政府的“灵活执行”实施之，共谋行为的组织基础和制度环境则应运而生。简言之，基层政府共谋行为是中央集权决策过程所付出的代价。</p><p>之前我以为权力越向上集中，执行的灵活性则越小，后来发现不是，越严格则越存在更多trade off的空间</p><p>刚看完第九章，主要写的是两个村在“村村通”项目中如何举债修路、背负债务和“违规挪用资金”偿还债务的故事。想起来十几年前我们村当时修路的情形，很相似，县政府不会给每个村拨款修路，想修路自己想办法去。当时我们村的书记跟书中的康书记很相似，路子广、有魄力，他把村里两条主干道旁边种植了二十几年的杉树全部卖掉，然后按照每户分担自家门前路段的修路成本（家里有劳动力的出劳动力没有劳动力的出钱）、修路人的伙食和茶水，然后还有一些弄不清楚的资金来源，最后奇迹般地比其他大村把路先修好了。还有另外一个事情是有一年，我们村着遇到洪水，然后他不知道是动什么什么社会关系，居然来了几卡车部队来帮助我们抗洪。在基层当村委书记没点路子是做不成的。不过这个村支书的做法有的人赞同有的人也不赞同。很可惜的是这个书记得癌症死了。</p><h2 id="统计学-豆瓣"><a href="#统计学-豆瓣" class="headerlink" title="统计学 (豆瓣)"></a><a href="https://book.douban.com/subject/26375999/" target="_blank" rel="noopener">统计学 (豆瓣)</a></h2><p><a href="https://www.libinx.com/2019/statistics-review-notes/">贾俊平版本统计学复习笔记 | Thinking Realm</a></p><h2 id="长安十二时辰-上-豆瓣"><a href="#长安十二时辰-上-豆瓣" class="headerlink" title="长安十二时辰 上 (豆瓣)"></a><a href="https://book.douban.com/subject/26899537/" target="_blank" rel="noopener">长安十二时辰 上 (豆瓣)</a></h2><p>马伯庸真的是一位认真的作家，看完《显微镜下的大明》之后还没有这么认为，不过从长安十二时辰让我确信了这一点。先不说小说的架构，单单从马伯庸在书中对于一千多年前的唐朝政治、风俗、军事、建筑娓娓道来这一点就让人佩服。马伯庸承认自己是从美剧反恐24小时中得到了灵感，将本书分为24个章节，一个章节对应一个小时，所以作者必须对时间有很好的掌控能力。张小敬原本是历史中华阴尉姚汝能笔下一个平平常常的人物，然而却在马伯庸的笔下无比放大，成为了一个24小时内拯救长安的英雄，可谓不辱使命。不过毕竟是小说，张小敬在连续24小时精神高度紧张下还能继续格斗、做出判断有点超乎寻常了。小说中的另外一个人物龙波也就是萧规留给我的印象不亚于张小敬，甚至在后半段有些“抢戏”，类似《蝙蝠侠：黑暗骑士》的小丑，锋芒甚至盖过了主角，不过他没有张小敬的那般犹豫，至始至终都不为自己的选择后悔，“每个人都要为自己的选择负责”这句在书中正派人物口中反复吐出的话我觉得放在他身上再确切不过了，张小敬、李泌难道就没有后悔自己的决定吗？再说小说的结构，可能是先前的悬念、打斗细节兜得太大，以至于后程作者似乎也无法掌控，灯楼爆炸的细节、背后大 boss从李相-李亨-安禄山三者之间来回切换，最后却是贺知章身边的义子贺东，然而他在全书的出现不过寥寥数次，读者无论如何也不会想到会是他，读到这里有些诧异又有些遗憾。总的来说，这本书还是值得一读的，至于最近火爆的网剧我看了几集，剧中人物名字全部被改，演员的功力无法驾驭如此丰富的人物，剧情也不是小说那般进行，给人印象深刻的就是服装道具布景，十分用心。</p><h2 id="腾讯传-豆瓣"><a href="#腾讯传-豆瓣" class="headerlink" title="腾讯传 (豆瓣)"></a><a href="https://book.douban.com/subject/26929955/" target="_blank" rel="noopener">腾讯传 (豆瓣)</a></h2><p>第一次如此全面地对腾讯崛起的历史有了全面的理解，依旧记得第一次听到QQ提示音的惊喜，第一次打开微信的那种新鲜感。吴晓波的叙事能力一流，上部分腾讯初创时期创始人的各种有趣的故事十分吸引人，但估计大部分不太可考吧。中部分则叙述了腾讯如何在web、移动互联网时代快速崛起的历程，当然我觉得“一个人的命运当然要靠自我奋斗，但是也要考虑历史的进程”，腾讯与周鸿祎、微博、盛大厮杀的过程似乎在一开始就决定了胜负，腾讯拥有巨大的用户基数，翻手为云覆手为雨，它更像是一个军和一个师之间的战争，赢了不值得炫耀，输了才丢人。如果想跟腾讯竞争，最好用其他兵种打击，如今短视频头条系的异军突起就是最好的证明。毕竟是官方委托写的传记，自然敏感的地方无法抒发作者自己的见解。比如3Q大战，难道周鸿祎就有那么不堪吗？（我不是360的用户，360太恶心了，现在也与360彻底无缘），周鸿祎好不容易从移动互联网杀出一条免费杀毒软件的血路，到手的肉眼看就被别人抢走了，求和不成下他才发动与腾讯的战争估计也是无奈之举，“王侯将相，宁有种乎？”，我认为360如海明威笔下的老人虽败犹荣。</p><h2 id="邓小平时代-豆瓣"><a href="#邓小平时代-豆瓣" class="headerlink" title="邓小平时代 (豆瓣)"></a><a href="https://book.douban.com/subject/20424526/" target="_blank" rel="noopener">邓小平时代 (豆瓣)</a></h2><p>三起三落、广场事件、改革开放是邓小平的政治生涯最为重要的三段经历，不好好研究就无法读懂邓小平。先前我以为自己在墙外看了一些纪录片和评论就对事件有了比较完整的认识，读了书之后后来发现完全不够，所以以后会更谨慎地去评论。不过正如邓公在事件发生后5天对军区干部的谈话所说的“这也许是一件好事”、“迟早是要来的”，有人说时间会向我们证明一切，如今正好30年过去了，但是我觉得还不够，目前还没有发展到能自信地评价那些历史事件。不过可以肯定的是如果当年没有果断地处理，一场血雨腥风是无法避免的。在微信上架的邓小平时代是删节版，关键处有网友补上了原文，注意。</p><h2 id="枪炮、病菌与钢铁-豆瓣"><a href="#枪炮、病菌与钢铁-豆瓣" class="headerlink" title="枪炮、病菌与钢铁 (豆瓣)"></a><a href="https://book.douban.com/subject/26743265/" target="_blank" rel="noopener">枪炮、病菌与钢铁 (豆瓣)</a></h2><p>今年以来看过最佳的一本社科书籍，无论是作者的论证、写作技巧、知识视野都属于一流，强烈推荐。</p><p>西班牙人胜利的秘密：</p><ol><li>西班牙的新式武器如枪炮、马匹、钢刀对印加帝国的士兵来说具有极强的心里震撼作用</li><li>印加帝国缺乏文字这一准确的沟通工具，故无法获得真实的情报</li><li>西班牙人带来的流行感冒、天花摧毁了印加帝国绝大多数没有抗体的人</li><li>俘虏了阿塔瓦尔帕基本上控制了8万士兵，不得不说这样的中心化的组织设计太脆弱了</li></ol><p>从一开始摆出人类起源于非洲的事实，然后叙述人类如何走出非洲，按道理起源越早应该具备更大的先发优势，读者这时便会有疑问，为什么现代的非洲会落后于其他大陆呢？在第二章，作者接着举例毛利人到达新西兰后快速发展的故事，说明非洲大陆领先500万年的的优势在100年面前微不足道，读到这里读者又会有疑问，那同时期的人类为什么又会有差异呢？作者又说起了毛利人和莫里奥里人的故事，这两个人群同时从波利尼西亚群岛走出来，结果却发展成为了实力悬殊的两个群体，前者把后者按在地上摩擦。作者觉得这样还不够信服，又举了162个西班牙人征服印加帝国的故事，听完这个故事你总该被说服了吧？不过读者可能还会有疑问，为什么总是欧洲人具有优势，新大陆的人却总是挨揍呢？作者每论证完一个论点又抛出一个新的问题，让人停不下来，戴蒙德的写作技巧真是赞啊！</p><p>我想看到这里我应该被作者的论证说服了，作者在本章解释了为什么新几内亚、澳大利亚和加利福尼亚原本存在有利于粮食生产的地区却没有出现，这不能怪生活在那里的新几内亚人、澳大利亚人没有作为，他们只是没有新月沃地得天独厚的丰富作物资源、适合植物生长的气候条件、可供驯化的大型哺乳动物。事实证明，粮食作物的驯化也在不久后在这些地区出现，但是太晚了，还没有等到驯化完，来自欧洲的殖民者就已经到来了。</p><p>所以，一开始读这本书的时候我很怀疑作者能不能讲好为“种族歧视”辩护这样一个政治正确的话题，看到这里我放心了。</p><h2 id="心流-豆瓣"><a href="#心流-豆瓣" class="headerlink" title="心流 (豆瓣)"></a><a href="https://book.douban.com/subject/27186106/" target="_blank" rel="noopener">心流 (豆瓣)</a></h2><p>这是一本值得经常翻看的书。</p><p>作者在书中提到的心流的概念，心流指的是我们内心的一种和谐的状态。在大多数情况下，我们的意识并不是完全能受我们大脑的控制，而心流便是一种能够控制你自己的美妙体验。</p><p>作者在书中是这么解释的：</p><p>第一，注意力。他说：体验过心流的人都知道，那份深沉的快乐是严格的自律、集中注意力换来的。第二，有一个他愿意为之付出的目标。那目标是什么不要紧，只要那目标将他的注意力集中于此。第三，有即时的回馈。第四，因全神贯注于此，日常恼人的琐事被忘却和屏蔽。第五，达到了忘我的状态。</p><p>你可以理解为沉浸式的体验，比如你沉迷于阅读一本书，并从里面学到了许多有意思的东西，从而继续想读下一本书，然后继续在这一种体验中获得快乐，再往大的说可以认为是天人合一的状态。跟中国文化里的不以物喜，不以己悲，庄子所言的：臣以神遇而不以目视，官知止而神欲行，有异曲同工之妙。</p><p>心流有好坏之分，并不是你完全投入了，然后沉浸在那种体验里面就是一个好的心流。好的心流应该是复杂化和独特化的统一，不然你的目标总是在原地踏步，并且不是基于你的一个自我去设计的，那么它就不是一个好的心流。只有具备了这两个特征，心流才有助于帮你提升生活品质。</p><p>心流有高低之分，复杂的自我是高级心流所具备的特征，独特性自我则是高级心流的必然产物。人们可以通过不断实现自己的目标以复杂化自我。</p><p>如何去获取心流呢？</p><p>首先，你得要有一个明确的目标，也就是说你要预期自己要达到什么样的状态，然后在状态完成的过程中需要不断地获得及时的反馈，这些反馈能够帮助你坚持到任务和目标顺利完成的那一刻。</p><p>在完成目标的过程中，你需要非常专注，也就是努力使自己达到一种忘我的状态，当然这种状态是非常难以拥有的。</p><p>在电影功夫熊猫2中有一个场景，阿宝去问他的师傅在做什么，他师傅说我在寻找内心的平静，即inner peace，那么什么是内心的平静呢？Master Shifu是这么回答的：</p><blockquote><p><em>It was the most painful, mind destroying, horrible moment I have ever experienced. But once I realized the problem was not you, but within me. I found Inner Peace, and was able to harness the flow of the universe.</em></p></blockquote><p>这曾是最痛苦，令人心碎，可怕的时刻，我之前从来没有体验过的。但自从我发现问题并不在于你，而是在于我，我领悟到了心如止水，从而能够利用宇宙之量。</p><p>这个回答或许有点禅的意味。</p><blockquote><p><em>Your mind is like this water, my friend. When it is agitated, it becomes difficult to see. But if you allow it to settle, the answer becomes clear.</em></p></blockquote><p>只有你不断地专注于某个设计的目标，然后自己制定计划去逐步的实现。它在实现计划的过程中能够有及时的反馈。知道自己要什么，并朝这个方向努力的人，感觉、思想、行动都能配合无间，内心的和谐自然涌现。</p><p>这个星期打算再把功夫熊猫2看一遍，其实这部电影的inner peace在我迷茫的时刻曾经让我思考了许久，到底什么才是心如止水。现在我大概明白那时一种合一的状态，你在空间中是一种流畅运行的态势，你能流畅地控制自我以及外物，正如master shifu能让一滴水在其手上自如流动最后无声重回水的怀抱。</p><p><strong>豆瓣书评</strong>：<a href="https://book.douban.com/review/10515845/" target="_blank" rel="noopener">心流=inner peace（心流）书评</a></p><h2 id="理性乐观派-豆瓣"><a href="#理性乐观派-豆瓣" class="headerlink" title="理性乐观派 (豆瓣)"></a><a href="https://book.douban.com/subject/6913343/" target="_blank" rel="noopener">理性乐观派 (豆瓣)</a></h2><p><strong>持有信心并非是一种浅薄</strong></p><p>总结下来，里得利认为交换和专业化的分工是人类经济不断进步的主要原因。</p><p>他在这本书里不厌其烦地搜集了大量的材料予以佐证自己为何对未来拥有理性的客观态度，从20万年前到现在，从经济、历史、化学再到地理，学科涉及面之广令人叹服，所以里得利的视野值得肯定。</p><p>书中提到</p><blockquote><p>“费马大定理的知识一点也不假，类星体也的确是遥远的星系，可它们恐怕永远也不会增加国内生产总值，不过，钻研这些知识，兴许可以提高某个人的生活质量。”</p></blockquote><p>让我想起为什么古代中国没有兴起像工业革命这般的技术狂潮，而是对于新技术的研究和出现总是保持戒备心理。工业革命的爆发是建立在无数人极小的改进上的，如果没有对技术持有热衷或者仅仅抱有功利心，那么永远都无法占据技术的领先地位。联想到目前的中国男篮，我的看法也是如此。</p><p>最后驳斥没有头脑的激进环保主义者，对于大多数人根深蒂固的悲观环境态度是一次沉重的打击，我们的环境并没有想象的那么糟糕，甚至比以前更好了。当然，作者所引用的材料大多没有给出来源，我对可信度持保留态度。</p><p><strong>豆瓣书评</strong>：<a href="https://book.douban.com/review/10483552/" target="_blank" rel="noopener">持有信心并非是一种浅薄（理性乐观派）书评</a></p><h2 id="事实-豆瓣"><a href="#事实-豆瓣" class="headerlink" title="事实 (豆瓣)"></a><a href="https://book.douban.com/subject/33385402/" target="_blank" rel="noopener">事实 (豆瓣)</a></h2><h2 id="沉默的大多数-豆瓣"><a href="#沉默的大多数-豆瓣" class="headerlink" title="沉默的大多数 (豆瓣)"></a><a href="https://book.douban.com/subject/27013716/" target="_blank" rel="noopener">沉默的大多数 (豆瓣)</a></h2><h2 id="Spark编程基础（Scala版）-豆瓣"><a href="#Spark编程基础（Scala版）-豆瓣" class="headerlink" title="Spark编程基础（Scala版） (豆瓣)"></a><a href="https://book.douban.com/subject/30450066/" target="_blank" rel="noopener">Spark编程基础（Scala版） (豆瓣)</a></h2><h2 id="无敌舰队-豆瓣"><a href="#无敌舰队-豆瓣" class="headerlink" title="无敌舰队 (豆瓣)"></a><a href="https://book.douban.com/subject/27172829/" target="_blank" rel="noopener">无敌舰队 (豆瓣)</a></h2>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;阅读在短期内可能不会给你的生活带来任何改变，但是只要你长期坚持下去，你会逐渐爱上这项脑力运动，它给你的回报也会越来越大。&lt;/p&gt;
&lt;p&gt;从去年抛弃朋友圈到今年年初放弃微博，我可以说差不多已经脱离社交网络了，脱离社交网络给我的生活带来最大的改变在于我可以腾出更多的时间在阅读软件上而不是沉迷于微信微博，上半年列出的这些书大多是在通勤的地铁、午间休息和晚上下班回家之后看完的。&lt;/p&gt;
&lt;p&gt;我爱阅读，我爱思考。&lt;/p&gt;
    
    </summary>
    
    
      <category term="思想王国" scheme="https://www.libinx.com/categories/%E6%80%9D%E6%83%B3%E7%8E%8B%E5%9B%BD/"/>
    
    
      <category term="阅读" scheme="https://www.libinx.com/tags/%E9%98%85%E8%AF%BB/"/>
    
      <category term="豆瓣" scheme="https://www.libinx.com/tags/%E8%B1%86%E7%93%A3/"/>
    
      <category term="微信读书" scheme="https://www.libinx.com/tags/%E5%BE%AE%E4%BF%A1%E8%AF%BB%E4%B9%A6/"/>
    
      <category term="地铁读书" scheme="https://www.libinx.com/tags/%E5%9C%B0%E9%93%81%E8%AF%BB%E4%B9%A6/"/>
    
  </entry>
  
  <entry>
    <title>使用Tesseract提取图像中的文字</title>
    <link href="https://www.libinx.com/2019/Tesseract-ocr/"/>
    <id>https://www.libinx.com/2019/Tesseract-ocr/</id>
    <published>2019-09-02T01:37:29.000Z</published>
    <updated>2021-03-23T01:25:10.995Z</updated>
    
    <content type="html"><![CDATA[<p>针对图像中的标准的印刷体文字，Tesseract或许是一个简单、高效的图片OCR方案选择。</p><a id="more"></a><h1 id="前期准备"><a href="#前期准备" class="headerlink" title="前期准备"></a>前期准备</h1><p>以Deepin为例，首先安装Tesseract 4.00，python依赖pytesseract和Pillow，并定义好环境变量。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"># 1. 安装Tesseract 4.00版本</span><br><span class="line">sudo apt-get tesseract-ocr</span><br><span class="line"># 2. 安装Tesseract 4.00 中文简体识别模型</span><br><span class="line">sudo apt install tesseract-ocr-chi-sim</span><br><span class="line"># 查看支持的语言：chi_sim指中文简体、eng指英文字体</span><br><span class="line">tesseract --list-langs</span><br><span class="line"># 查看目前系统已经安装的字体</span><br><span class="line">text2image --fonts_dir /usr/share/fonts --list_available_fonts</span><br><span class="line"># 3. 定义环境变量，不然会报错：“Failed loading language &apos;eng&apos; Tesseract couldn&apos;t load any languages!”</span><br><span class="line">export TESSDATA_PREFIX=/usr/share/tesseract-ocr/4.00/tessdata/</span><br><span class="line"># 4. 安装python库</span><br><span class="line">sudo pip install pytesseract</span><br><span class="line"># 5. 安装python图片处理库</span><br><span class="line">sudo pip install Pillow</span><br><span class="line"># 6. 把langdata_lstm和tesseract两个项目clone到本地，这两个项目是为了训练字体模型而准备的，如果仅仅使用默认的模型去识别字体那么不用clone这两个项目也行（项目体积有点大，clone时间会比较长）</span><br><span class="line">git clone https://github.com/tesseract-ocr/langdata_lstm</span><br><span class="line">git clone https://github.com/tesseract-ocr/tesseract</span><br></pre></td></tr></table></figure><h1 id="生成待训练字体数据集"><a href="#生成待训练字体数据集" class="headerlink" title="生成待训练字体数据集"></a>生成待训练字体数据集</h1><p>由于中文存在不同的字体形式，如果使用场景中有别的字体，比如隶书，那默认的字体识别模型效果就不会很好，所以有的时候需要根据场景中的字体训练相对应的字体识别模型。以下以生成<strong>华文行楷</strong>字体的训练数据集为例（前提是机器已经安装好了该字体），然后执行生成训练数据步骤：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">~/image_projects/tesseract/src/training/tesstrain.sh \</span><br><span class="line">  --fonts_dir /usr/share/fonts \</span><br><span class="line">  --lang chi_sim --linedata_only  \</span><br><span class="line">  --noextract_font_properties \</span><br><span class="line">  --langdata_dir ~/image_projects/langdata/langdata_lstm   \</span><br><span class="line">  --tessdata_dir ~/image_projects/tesseract/tessdata   \</span><br><span class="line">  --save_box_tiff --maxpages 150  \</span><br><span class="line">  --fontlist &quot;STXingkai&quot; \</span><br><span class="line">  --output_dir ~/image_projects/model</span><br></pre></td></tr></table></figure><p>参数解释：</p><ul><li><code>--fonts_dir</code> 字体安装目录</li><li><code>--langdata_dir</code> 上一节中clone下来的langdata_lstm项目</li><li><code>--tessdata_dir</code> 上一节中clone下来的tesseract项目</li><li><code>--fontlist</code> 你需要训练字体的名称</li></ul><h1 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">OMP_THREAD_LIMIT=8 lstmtraining --model_output ~/image_projects/model/STXingkai \</span><br><span class="line">  --continue_from ~/image_projects/tesseract/tessdata/chi_sim.lstm \</span><br><span class="line">  --traineddata ~/image_projects/tesseract/tessdata/chi_sim.traineddata \</span><br><span class="line">  --train_listfile ~/image_projects/model/chi_sim.training_files.txt \</span><br><span class="line">  --max_iterations 2000</span><br></pre></td></tr></table></figure><h1 id="评估"><a href="#评估" class="headerlink" title="评估"></a>评估</h1><p>此处的评估结果并不会给出准确率或者相关的评估指标，只会给出原文和识别结果的对比。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">lstmeval --model ~/image_projects/tesseract/tessdata/chi_sim.lstm \</span><br><span class="line">--traineddata ~/image_projects/tesseract/tessdata/chi_sim.traineddata \</span><br><span class="line">--eval_listfile ~/image_projects/model/chi_sim.training_files.txt</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lstmeval --model STXingkai_checkpoint --traineddata ~/image_projects/tesseract/tessdata/chi_sim.traineddata  --eval_listfile chi_sim.training_files.txt</span><br></pre></td></tr></table></figure><h1 id="与现有的模型合并"><a href="#与现有的模型合并" class="headerlink" title="与现有的模型合并"></a>与现有的模型合并</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">lstmtraining --stop_training \</span><br><span class="line">  --continue_from ~/image_projects/model/STXingkai_checkpoint \</span><br><span class="line">  --traineddata ~/image_projects/tesseract/tessdata/chi_sim.traineddata \</span><br><span class="line">  --model_output ~/image_projects/model/chi_sim_xingkai.traineddata</span><br></pre></td></tr></table></figure><p>现在，查看当前的系统支持的字体模型，已经支持华文行楷<code>chi_sim_xk</code>的识别了。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 查看支持的语言：chi_sim指中文简体、eng指英文字体</span><br><span class="line">tesseract --list-langs</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">List of available languages (5):</span><br><span class="line">osd</span><br><span class="line">chi_sim_xk</span><br><span class="line">chi_sim_vert</span><br><span class="line">eng</span><br><span class="line">chi_sim</span><br></pre></td></tr></table></figure><h1 id="相关链接"><a href="#相关链接" class="headerlink" title="相关链接"></a>相关链接</h1><p><a href="https://github.com/tesseract-ocr/langdata_lstm" target="_blank" rel="noopener">https://github.com/tesseract-ocr/langdata_lstm</a><br><a href="https://0o0.me/legendary/tesseract-fine-tune.html" target="_blank" rel="noopener">https://0o0.me/legendary/tesseract-fine-tune.html</a><br><a href="https://qianjiye.de/2015/08/tesseract-ocr#outline_0" target="_blank" rel="noopener">https://qianjiye.de/2015/08/tesseract-ocr#outline_0</a><br><a href="https://iami.xyz/Review-S05-Tesseract-lstm-traning-datafile/" target="_blank" rel="noopener">https://iami.xyz/Review-S05-Tesseract-lstm-traning-datafile/</a><br><a href="https://0o0.me/legendary/tesseract-fine-tune.html" target="_blank" rel="noopener">https://0o0.me/legendary/tesseract-fine-tune.html</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;针对图像中的标准的印刷体文字，Tesseract或许是一个简单、高效的图片OCR方案选择。&lt;/p&gt;
    
    </summary>
    
    
      <category term="教程" scheme="https://www.libinx.com/categories/%E6%95%99%E7%A8%8B/"/>
    
    
      <category term="Tesseract" scheme="https://www.libinx.com/tags/Tesseract/"/>
    
      <category term="ocr" scheme="https://www.libinx.com/tags/ocr/"/>
    
      <category term="图像文字" scheme="https://www.libinx.com/tags/%E5%9B%BE%E5%83%8F%E6%96%87%E5%AD%97/"/>
    
      <category term="pytesseract" scheme="https://www.libinx.com/tags/pytesseract/"/>
    
  </entry>
  
  <entry>
    <title>贾俊平版本统计学复习笔记</title>
    <link href="https://www.libinx.com/2019/statistics-review-notes/"/>
    <id>https://www.libinx.com/2019/statistics-review-notes/</id>
    <published>2019-07-14T01:58:53.000Z</published>
    <updated>2021-03-23T01:25:10.995Z</updated>
    
    <content type="html"><![CDATA[<p>前一段时间在工作中发现自己对于统计知识的理解有点不够系统，比如我知道了解数据的形态需要从集中趋势、离散趋势以及形态方面着手，但是对于其中的细节却总是模糊的，比如我无法回答为什么通常用标准差而不是方差来反映数据的离散程度，我也无法回答泊松分布到底是个什么鬼，所以出于非应试的目的，利用差不多一个多月的业余时间，我把贾俊平编写的统计学的前半部分大致的过了一遍，从中选出几点我先前不太明白的地方记录下来，先记录这么多，以后有新的再补上。</p><a id="more"></a><h2 id="参数与统计量"><a href="#参数与统计量" class="headerlink" title="参数与统计量"></a>参数与统计量</h2><p>参数与统计量的区别：参数是对总体特征的概括性数字度量，因为总体数据通常是不知道的，所以参数一般未知。统计量是描述样本数据的概括性数字度量，因为样本是抽样出来的，所以统计量是已知的，统计量用于估计总体的参数。</p><h2 id="数据描述"><a href="#数据描述" class="headerlink" title="数据描述"></a>数据描述</h2><p>我们一般可以将数据大致分为：<strong>数值型、分类型和顺序型数据</strong>，顺序型数据就是将数值型数据按照大小排序后的形态，一般从集中趋势、离散趋势和形态度量这三个方面去获得对数据的整体感知。在复习统计学的时候我发现我先前对于数据离散趋势的度量并不是那么准确，比如，对于分类数据来说，有<strong>异众比率</strong>，顺序数据来说，是<strong>四分位差</strong>，数值型数据主要是<strong>方差和标准差</strong>。</p><p>其中有一条经验法则挺值得注意的，也就是说在数据对称分布的时候，大多数的数据是在±k个标准差的范围之内，如果某个数据超过了这个范围，那可以把它被认定是离群点。对于不对称分布的数据，则可以根据切比雪夫不等式来知道大多数数据的分布范围。</p><p>当我们想要比较两组数据的离散程度，如果均值不同的话是不能直接拿标准差去比较的，因为他们不在同一个量纲下，所以为了消除量纲的差距，我们需要用相对离散程度，即<strong>离散系数，标准差与平均数之比</strong></p><p><img src="https://raw.githubusercontent.com/swordspoet/i/img/0714-%E7%BB%8F%E9%AA%8C%E6%95%B0%E6%8D%AE%E5%88%86%E5%B8%83.jpg" alt="经验数据分布范围"></p><h2 id="均值与数学期望的区别"><a href="#均值与数学期望的区别" class="headerlink" title="均值与数学期望的区别"></a>均值与数学期望的区别</h2><p>均值是实验后对统计结果的平均数而数学期望是实验前根据概率分布对实验结果的预测的平均值。因为我们在实验中没办法穷尽所有的可能，所以只能根据概率，分布去预测样本的平均值，即数学期望。</p><h2 id="统计量及其抽样分布"><a href="#统计量及其抽样分布" class="headerlink" title="统计量及其抽样分布"></a>统计量及其抽样分布</h2><p>抽样分布、参数估计和假设检验是统计推断的三个中心内容，在正态分布的总体条件下，统计学下有三大分布：卡方分布、t分布和F分布。这三大分布对后面的参数估计和假设检验至关重要。</p><p>还需要记得一条伟大的中心极限定理。中心极限定理的大致意思就是如果我从一个服从正态分布的总体中抽出n个样本，当样本量充分大的时候（经验上一般大于30即可）样本的均值也是服从正态分布。</p><p><img src="https://raw.githubusercontent.com/swordspoet/i/img/0714-p80923011-1.jpg" alt=""></p><p><img src="https://raw.githubusercontent.com/swordspoet/i/img/0714-p80923011-2.jpg" alt=""></p><p>之前一直不理解泊松分布和指数分布在实际中的应用是如何的，直到看到了阮一峰写得这篇简短明了的<a href="http://www.ruanyifeng.com/blog/2015/06/poisson-distribution.html" target="_blank" rel="noopener">文章</a>才恍然大悟。</p><blockquote><p>泊松分布是单位时间内独立事件发生次数的概率分布，指数分布是独立事件的时间间隔的概率分布。</p></blockquote><h2 id="参数检验"><a href="#参数检验" class="headerlink" title="参数检验"></a>参数检验</h2><p>比较两个群体之间的参数是否有显著差别，比如两个不同行业职工的平均收入水平是否存在差异，一般的逻辑是直接比较均值或者更不合理的是拿一个群体的均值作为代表跟另外一个群体相比较，而从统计学的意义上来说，需要用<strong>两个总体参数的检验</strong>寻找答案，按照下图属于两个总体参数之差的检验，那么根据样本大小或者方差是否位置分别采用z统计量和t统计量来做检验。</p><p><img src="https://raw.githubusercontent.com/swordspoet/i/img/0714-p80897996-1.jpg" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;前一段时间在工作中发现自己对于统计知识的理解有点不够系统，比如我知道了解数据的形态需要从集中趋势、离散趋势以及形态方面着手，但是对于其中的细节却总是模糊的，比如我无法回答为什么通常用标准差而不是方差来反映数据的离散程度，我也无法回答泊松分布到底是个什么鬼，所以出于非应试的目的，利用差不多一个多月的业余时间，我把贾俊平编写的统计学的前半部分大致的过了一遍，从中选出几点我先前不太明白的地方记录下来，先记录这么多，以后有新的再补上。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Machine-Learning" scheme="https://www.libinx.com/categories/Machine-Learning/"/>
    
    
      <category term="统计学" scheme="https://www.libinx.com/tags/%E7%BB%9F%E8%AE%A1%E5%AD%A6/"/>
    
  </entry>
  
  <entry>
    <title>基于Spark逻辑回归的流失预测实践</title>
    <link href="https://www.libinx.com/2019/user-loss-prediction-in-spark-logistic-regression/"/>
    <id>https://www.libinx.com/2019/user-loss-prediction-in-spark-logistic-regression/</id>
    <published>2019-07-07T07:25:34.000Z</published>
    <updated>2021-03-23T01:25:10.994Z</updated>
    
    <content type="html"><![CDATA[<p>先前的一段时间接手到一个流失用户预测的活，也就是根据某个群体用户的行为数据（动态特征）、自身特征（静态特征），建立一套流失预警的分类模型，预测用户的流失概率。类似于这种机器学习任务，毫无疑问，逻辑回归算法是首选之选。因为在很多场景下的需求问题都可以很容易地转化为一个分类或者预测问题，而逻辑回归的输出值可以无缝地适应这两类问题，如果你想要做分类，那么你控制好门槛值（threshold），如果你想做预测，它输出的概率值很好解释。所以，逻辑回归这个算法在机器学习中有点AK-47的味道：简单易懂、适应场景广泛、容易优化，如果效果不好，尝试增大数据集、增加几个特征、调节参数这些常见优化方法，一般来说，应对离线场景下的需求绰绰有余。这篇文章暂时就离线问题作为讨论的话题，因为我还没有特别熟练的实时机器学习模型部署经验。</p><p>对比Python引用Scikit-Learn构建机器学习模型和使用Scala在Spark下搭建机器学习模型，两者也有值得注意的地方，比如Spark在训练的时候更加直观，你只需要把所有的特征全部汇集到<code>features</code>一列下就行，然后再指定一列作为<code>label</code>，到此为止就算是成功了一半。</p><h2 id="样本选择"><a href="#样本选择" class="headerlink" title="样本选择"></a>样本选择</h2><p>样本的选择是模型搭建中很关键的一个环节，因为它直接会关系到你的模型的预测对象是否符合需求，比如这篇文章介绍就比较详细：<a href="https://cloud.tencent.com/developer/article/1352076" target="_blank" rel="noopener">用户增长分析——用户流失预警 - 云+社区 - 腾讯云</a>。举个例子，如果你要预测平台忠实用户的流失概率，那么你首先要考虑的问题便是<strong>如何定义忠实的活跃用户</strong>，只有先从用户池划定了一个区域，然后所有的分析、特征选取、训练预测都围绕圈定的用户群体来做，否则你的模型做得再好也是徒劳的。我是从以下几个方面来定义活跃用户的：</p><ul><li>过去N天登录次数大于M次</li><li>连续活跃的天数大于N次</li><li>用户的主要功能事件行为</li><li>过滤黑名单用户、新增用户、作弊用户</li><li>……</li></ul><p>当然，过滤的条件越多，你的SQL也会越来越复杂，你可能需要把SQL层层嵌套，这个环节需要非常细心。</p><h2 id="特征提取"><a href="#特征提取" class="headerlink" title="特征提取"></a>特征提取</h2><p>做完了上述的工作之后，下一步我们需要做的就是如何选取特征了，这也是相当关键的一个步骤（貌似没有不关键的==），garbage in garbage out，选取特征的好坏直接决定了模型的质量。你需要根据你所在的业务场景设计自己的指标体系，我所在项目是娱乐社交平台，关系用户留存的因素我总结为三大类：社交行为、业务行为和消费行为，下图展示的是我在模型中选取的部分特征，其中第一个部分的业务、社交活跃特征最多，总共有40个，这些特征的计算工作依赖前期数据项目人天表的积累，如果之前的这些数据项目没有做好，临时再去计算这些特征需要花费大量的精力；第二和第三个部分的业务行为特征我主要涉及到用户的消费行为，很容易想到一个在平台经常消费送礼的用户应该是不大可能会流失的，而那些消费行为记录为零的用户则很有可能再也不玩APP了；截图中还有一部分特征没有显示出来，这部分的特征我主要选的是用户的一些反馈行为，比如说举报其他用户或使用APP过程中的反馈行为，这些行为数据跟用户的流失存在着很大的关系。总的来说，特征的选取需要贴合实际的业务场景，先理解了业务场景剩下的工作便是“开脑洞”从各个业务表中寻找特征数据。</p><p><img src="https://raw.githubusercontent.com/swordspoet/i/img/spark-logisitic-regression-1.jpg" alt=""></p><p>在正式进入模型的搭建之前，我们最好在心底确定下一个目标，确保喂给训练器的dataframe是一个用户id对应一行特征数据，并且数据还需要符合没有null值和空值、连续型的特征字段取值是double的条件。</p><h2 id="模型搭建与训练"><a href="#模型搭建与训练" class="headerlink" title="模型搭建与训练"></a>模型搭建与训练</h2><p>为了方便阅读代码，我把模型训练部分的代码全部放在文章的末尾了。</p><p>另外还遇到的一个问题是性能，当时我总是遇到内存溢出，后来我发现自己对于业务场景的理解有错误，也就是数据的准备不需要及时化，我的业务场景是离线的，所以我不需要像实时预测的需求那样拿到数据之后立马做出判断，所以这两步连接在一块的话就会造成OOM的错误，所以在离线的场景下，我们可以将离线的预测拆分为数据准备、训练、预测三个阶段，或者是根据你的需要分为更多的步骤，这样能减少内存占用的压力。</p><p><img src="https://raw.githubusercontent.com/swordspoet/i/img/spark-logisitic-regression-2.jpg" alt=""></p><p>SparkML还有一个比较方便的就是它的pipeline，pipeline，顾名思义，即流水线。想象你的原始数据需要进行各种转换操作，代码会变得非常冗余难以阅读，在pipeline下，你至于要把需要的转换操作放在pipeline里面即可，<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> pipeline = <span class="keyword">new</span> <span class="type">Pipeline</span>()</span><br><span class="line">    .setStages(<span class="type">Array</span>(assembler,stdScaler,featureSelector,labelIndexer,lr))</span><br></pre></td></tr></table></figure></p><p><img src="https://raw.githubusercontent.com/swordspoet/i/img/spark-logisitic-regression-3.jpg" alt=""><br>比如<code>assembler</code>是把所有的特征数值和向量全部汇总到一列下，<code>stdScaler</code>是对特征数据做标准化处理，<code>featureSelector</code>是特征选择器，<code>labelIndexer</code>则是目标变量转换操作，这样就保证了数据和数据操作的代码不会混合到一块，可以说pipeline真的让数据模型的代码变得很整洁！</p><h2 id="网格搜索"><a href="#网格搜索" class="headerlink" title="网格搜索"></a>网格搜索</h2><p>为了让模型能够训练出一组更好的参数，网格搜索是必不可少的，在Spark中我们可以通过<code>ParamGridBuilder</code>来构建一个参数网格搜索器。在本文中我选择了特征数量、正则化系数、<code>elasticNetParam</code>三个参数作为优化的对象：</p><ul><li>特征数量：虽然在模型中引入了很多的特征，但不是每一个特征都起作用，那些作用不大的特征反而会拖慢运行的速度，所以你可以根据实际的情况设置一组特征数量，比如（10,20,30）就是分别从所有特征中挑出10个、20个、30个最有代表性的</li><li>正则化系数：默认是0.0（一般业务场景下数据都比较稀疏，所以正则化系数要尽量取小一点，如0.01，0.001）</li><li><code>elasticNetParam</code>：是混合了L1和L2正则化系数的参数，公式如下：$\alpha \left( \lambda |w|_1 \right) + (1-\alpha) \left( \frac{\lambda}{2}|w|_2^2 \right) , \alpha \in [0, 1], \lambda \geq 0$，elasticNetParam指的就是公式中的$\alpha$，它代表了L1和L2正则化之间的权衡，当elasticNetParam越接近于1时，L1正则化发挥的作用就越大，这时就会导致特征的权重很多为零，当elasticNetParam越接近于0，L2发挥的作用就越大，会导致特征的权重变得很小。<ul><li><blockquote><p><strong>Spark官方文档解释</strong>：Set the ElasticNet mixing parameter. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty. For alpha in (0,1), the penalty is a combination of L1 and L2. Default is 0.0 which is an L2 penalty.</p></blockquote></li></ul></li></ul><h2 id="交叉验证"><a href="#交叉验证" class="headerlink" title="交叉验证"></a>交叉验证</h2><p>除了正则化手段，机器学习中还可以通过交叉验证来防止模型的过拟合，Spark提供了<code>CrossValidator</code>接口用于将训练和测试集数据切分为K折，比如5折交叉验证，<code>CrossValidator</code>会生成5组（训练集，测试集）数据，4/5用于训练，1/5用于验证，然后<code>CrossValidator</code>会根据5个模型计算得到的一个平均评估指标（average evaluation metric）。值得注意的是，交叉验证是非常耗费时间的，比如网格搜索中有两个参数，一共4×2种组合，再加上K折交叉验证，那么就是4×2×k种模型的组合了。</p><h2 id="测试模型"><a href="#测试模型" class="headerlink" title="测试模型"></a>测试模型</h2><p>模型训练完了之后紧接着我们就需要在测试数据集上测试模型，Spark测试模型的步骤如下，用已有的model去transform测试数据集，然后数据集中会新生成<code>rawPrediction</code>、<code>probablity</code>、<code>prediction</code>三列，分别代表原始预测、概率值、预测结果。比如1.79是原始预测值，经过sigmoid转换之后变成了[0,1]之间的概率值，再根据你自己定义的门槛值来确定预测结果是正或反。通常，测试逻辑回归模型的指标有auc，在Spark中可以用<code>BinaryClasssificationEvaluator</code>获得auc的值。</p><p><img src="https://raw.githubusercontent.com/swordspoet/i/img/spark-logisitic-regression-4.jpg" alt=""></p><p>分类任务除了auc指标，还需要考察查准率和召回率，关于这些指标的解释可以参考我的先前的文章：<a href="https://www.libinx.com/2017/evaluate-machine-learning-models/">评估机器学习模型：指标解释 | Thinking Realm</a>。而查准率和召回率则需要根据具体业务场景，比如在本文模型的目标是找到那些流失的用户，所以我更关注的是流失用户的查全率，即模型能把实际数据中的流失用户找出来，而总体的准确率能够达到合格的水平就够了。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">UserLossPredictionDriver</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">          * 1. 加载数据：1 留存 0 流失</span></span><br><span class="line"><span class="comment">          * 2. 数据转换</span></span><br><span class="line"><span class="comment">          * 3. 训练模型</span></span><br><span class="line"><span class="comment">          */</span></span><br><span class="line">        <span class="keyword">val</span> properties = <span class="keyword">new</span> <span class="type">Properties</span>()</span><br><span class="line">            properties.load(<span class="keyword">new</span> <span class="type">FileInputStream</span>(<span class="string">"./properties/user_loss_prediction.properties"</span>))</span><br><span class="line">        <span class="keyword">val</span> taskName = properties.getProperty(<span class="string">"task.name"</span>)</span><br><span class="line">        <span class="comment">// 模型保存路径</span></span><br><span class="line">        <span class="keyword">val</span> modelSavePath = properties.getProperty(<span class="string">"model.save.path"</span>)</span><br><span class="line">        <span class="comment">// 外部特征表字段名和表名</span></span><br><span class="line">        <span class="keyword">val</span> featureTabComField = properties.getProperty(<span class="string">"feature.cols"</span>)</span><br><span class="line">        <span class="keyword">val</span> featureTab = properties.getProperty(<span class="string">"feature.tab"</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> activeUserInfoTab = properties.getProperty(<span class="string">"active.user.info.tab"</span>)</span><br><span class="line">        <span class="keyword">val</span> numTopFeatures = properties.getProperty(<span class="string">"num.top.features"</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> conf = <span class="type">SparkHiveUtil</span>.<span class="type">InitSparkConf</span>(taskName).set(<span class="string">"spark.kryo.registrationRequired"</span>,<span class="string">"false"</span>)</span><br><span class="line">        <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line">        <span class="keyword">val</span> sqlContext = <span class="type">SparkHiveUtil</span>.<span class="type">InitHiveContext</span>(sc)</span><br><span class="line">        <span class="keyword">val</span> predictDt = properties.getProperty(<span class="string">"date.time"</span>)</span><br><span class="line">        <span class="keyword">val</span> dateFormat = properties.getProperty(<span class="string">"date.format"</span>)</span><br><span class="line">        <span class="keyword">var</span> dtArr = <span class="type">Array</span>[<span class="type">String</span>]()</span><br><span class="line">        dtArr = &#123;</span><br><span class="line">            <span class="keyword">if</span> (<span class="literal">null</span> == predictDt || <span class="string">"null"</span>.equals(predictDt) || <span class="string">""</span>.equalsIgnoreCase(predictDt.trim))</span><br><span class="line">                <span class="type">Array</span>(<span class="type">TimeUtil</span>.getYesterday(dateFormat))</span><br><span class="line">            <span class="keyword">else</span></span><br><span class="line">                predictDt.split(<span class="string">","</span>)</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 方案一：读取特征数据</span></span><br><span class="line"><span class="comment">//        var featureDataFrame: DataFrame = null</span></span><br><span class="line"><span class="comment">//        for (dt &lt;- dtArr) &#123;</span></span><br><span class="line"><span class="comment">//            val activeUserDataFrame = DataInit.activeUserDataFrame(sqlContext,properties,dt)</span></span><br><span class="line"><span class="comment">//            val userDeviceDataFrame = DataInit.userDeviceFeature(sqlContext,dt,properties)</span></span><br><span class="line"><span class="comment">//            val userSongFeedBackDataFrame = DataInit.userSongFeedBackDataFrame(sqlContext,dt,properties)</span></span><br><span class="line"><span class="comment">//            val userRechargeDataFrame = DataInit.userRechargeBehavior(sqlContext,dt)</span></span><br><span class="line"><span class="comment">//            val userSendGiftDataFrame = DataInit.userSendGiftBehavior(sqlContext,dt)</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="comment">//            val midDataFrame = activeUserDataFrame.repartition(activeUserDataFrame("user_id"))</span></span><br><span class="line"><span class="comment">//              .join(userDeviceDataFrame.repartition(userDeviceDataFrame("user_id")),Seq("user_id"),"left")</span></span><br><span class="line"><span class="comment">//              .join(userSongFeedBackDataFrame.repartition(userSongFeedBackDataFrame("user_id")),Seq("user_id"),"left")</span></span><br><span class="line"><span class="comment">//              .join(userRechargeDataFrame.repartition(userRechargeDataFrame("user_id")),Seq("user_id"),"left")</span></span><br><span class="line"><span class="comment">//              .join(userSendGiftDataFrame.repartition(userSendGiftDataFrame("user_id")),Seq("user_id"),"left")</span></span><br><span class="line"><span class="comment">//              .na.fill(0)</span></span><br><span class="line"><span class="comment">//            if ( null == featureDataFrame ) &#123;</span></span><br><span class="line"><span class="comment">//                featureDataFrame = midDataFrame</span></span><br><span class="line"><span class="comment">//            &#125; else &#123;</span></span><br><span class="line"><span class="comment">//                featureDataFrame = featureDataFrame.unionAll(midDataFrame.repartition(midDataFrame("user_id")))</span></span><br><span class="line"><span class="comment">//            &#125;</span></span><br><span class="line"><span class="comment">//        &#125;</span></span><br><span class="line">        <span class="comment">// 方案一：读取特征数据</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 方案二：先将特征数据集写入hive表，再读取hive表中的特征字段数据</span></span><br><span class="line">        <span class="keyword">val</span> featureCols = properties.getProperty(<span class="string">"feature.cols"</span>).split(<span class="string">","</span>)</span><br><span class="line">        <span class="keyword">val</span> featureDataFrameFromHive = sqlContext</span><br><span class="line">          .sql(<span class="string">""</span><span class="string">"SELECT * FROM dws_base.user_loss_model_feature WHERE dt IN ("</span><span class="string">""</span> + dtArr.mkString(<span class="string">","</span>) + <span class="string">""</span><span class="string">") AND user_id &lt;&gt; 0"</span><span class="string">""</span>)</span><br><span class="line">          .drop(<span class="string">"dt"</span>)  <span class="comment">// 去掉dt字段</span></span><br><span class="line">          .na.fill(<span class="number">0.0</span>)</span><br><span class="line">          .persist(<span class="type">StorageLevel</span>.<span class="type">MEMORY_AND_DISK_SER</span>)</span><br><span class="line">        <span class="comment">// 方案二：先将特征数据集写入hive表</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 预处理数据集</span></span><br><span class="line">        <span class="keyword">val</span> trainAndTestData = <span class="type">PreprocessData</span>.preprocessData(featureDataFrameFromHive,properties)</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 将特征列汇总至raw_features一列下</span></span><br><span class="line">        <span class="keyword">val</span> assembler = <span class="keyword">new</span> <span class="type">VectorAssembler</span>()</span><br><span class="line">          .setInputCols(featureCols)</span><br><span class="line">          .setOutputCol(<span class="string">"raw_features"</span>)</span><br><span class="line">        <span class="comment">// 数据标准化</span></span><br><span class="line">        <span class="keyword">val</span> stdScaler = <span class="keyword">new</span> <span class="type">StandardScaler</span>()</span><br><span class="line">          .setInputCol(<span class="string">"raw_features"</span>)</span><br><span class="line">          .setOutputCol(<span class="string">"std_features"</span>)</span><br><span class="line">          .setWithStd(<span class="literal">true</span>)    <span class="comment">// 将数据集标准化</span></span><br><span class="line">          .setWithMean(<span class="literal">false</span>)  <span class="comment">// 默认是false，原数据集比较稀疏的时候慎用</span></span><br><span class="line">        <span class="comment">// 特征数量选择器</span></span><br><span class="line">        <span class="keyword">val</span> featureSelector = <span class="keyword">new</span> <span class="type">ChiSqSelector</span>()</span><br><span class="line">          .setFeaturesCol(<span class="string">"std_features"</span>)</span><br><span class="line">          .setLabelCol(<span class="string">"class"</span>)</span><br><span class="line">          .setOutputCol(<span class="string">"features"</span>)</span><br><span class="line">        <span class="comment">// 目标变量转换</span></span><br><span class="line">        <span class="comment">// 这里需要注意label到class的转换时，频次出现较高的label会被标记为0</span></span><br><span class="line">        <span class="comment">// 所以会出现特征dataframe中label和class不一致的情况</span></span><br><span class="line">        <span class="keyword">val</span> labelIndexer = <span class="keyword">new</span> <span class="type">StringIndexer</span>()</span><br><span class="line">          .setInputCol(<span class="string">"class"</span>)</span><br><span class="line">          .setOutputCol(<span class="string">"label"</span>)</span><br><span class="line">        <span class="comment">// 初始化LR模型</span></span><br><span class="line">        <span class="keyword">val</span> maxIter = properties.getProperty(<span class="string">"max.iter"</span>)</span><br><span class="line">        <span class="keyword">val</span> regParam = properties.getProperty(<span class="string">"reg.param"</span>).split(<span class="string">","</span>).map(f =&gt; f.toDouble)</span><br><span class="line">        <span class="keyword">val</span> elasticNetParam = properties.getProperty(<span class="string">"elastic.net.param"</span>).split(<span class="string">","</span>).map(f =&gt; f.toDouble)</span><br><span class="line">        <span class="keyword">val</span> numFeatures = properties.getProperty(<span class="string">"num.features"</span>).split(<span class="string">","</span>).map(f =&gt; f.toInt)</span><br><span class="line">        <span class="keyword">val</span> lr = <span class="keyword">new</span> <span class="type">LogisticRegression</span>()</span><br><span class="line">          .setMaxIter(maxIter.toInt)</span><br><span class="line">          .setWeightCol(<span class="string">"classWeightCol"</span>)</span><br><span class="line">          .setLabelCol(<span class="string">"label"</span>)</span><br><span class="line">          .setFeaturesCol(<span class="string">"features"</span>)</span><br><span class="line">        <span class="comment">// pipeline</span></span><br><span class="line">        <span class="keyword">val</span> pipeline = <span class="keyword">new</span> <span class="type">Pipeline</span>()</span><br><span class="line">          .setStages(<span class="type">Array</span>(assembler,stdScaler,featureSelector,labelIndexer,lr))</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 训练集和测试集数据</span></span><br><span class="line">        <span class="keyword">val</span> trainData = trainAndTestData(<span class="number">0</span>).cache()</span><br><span class="line">        <span class="keyword">val</span> testData = trainAndTestData(<span class="number">1</span>).cache()</span><br><span class="line">        featureDataFrameFromHive.unpersist()</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 网格搜索：特征数量、正则化系数、elasticNetParam</span></span><br><span class="line">        <span class="keyword">val</span> paramGrid = <span class="keyword">new</span> <span class="type">ParamGridBuilder</span>()</span><br><span class="line">          .addGrid(featureSelector.numTopFeatures, numFeatures)</span><br><span class="line">          .addGrid(lr.regParam, regParam)</span><br><span class="line">          .addGrid(lr.elasticNetParam, elasticNetParam)</span><br><span class="line">          .build()</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 交叉验证</span></span><br><span class="line">        <span class="keyword">val</span> cv = <span class="keyword">new</span> <span class="type">CrossValidator</span>()</span><br><span class="line">          .setEstimator(pipeline)</span><br><span class="line">          .setEvaluator(<span class="keyword">new</span> <span class="type">BinaryClassificationEvaluator</span>())</span><br><span class="line">          .setEstimatorParamMaps(paramGrid)</span><br><span class="line">          .setNumFolds(properties.getProperty(<span class="string">"k.folds"</span>).toInt)  <span class="comment">// K折交叉验证</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 提取最优模型的参数</span></span><br><span class="line">        <span class="keyword">implicit</span> <span class="class"><span class="keyword">class</span> <span class="title">BestParamMapCrossValidatorModel</span>(<span class="params">cvModel: <span class="type">CrossValidatorModel</span></span>) </span>&#123;</span><br><span class="line">            <span class="function"><span class="keyword">def</span> <span class="title">bestEstimatorParamMap</span></span>: <span class="type">ParamMap</span> = &#123;</span><br><span class="line">                cvModel.getEstimatorParamMaps</span><br><span class="line">                  .zip(cvModel.avgMetrics)</span><br><span class="line">                  .maxBy(_._2)</span><br><span class="line">                  ._1</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 训练模型</span></span><br><span class="line">        <span class="keyword">val</span> cvModel = cv.fit(trainData)</span><br><span class="line">        <span class="keyword">val</span> pipelineModel = cvModel.bestModel.asInstanceOf[<span class="type">PipelineModel</span>]</span><br><span class="line">        <span class="keyword">val</span> lrModel = pipelineModel.stages(<span class="number">4</span>).asInstanceOf[<span class="type">LogisticRegressionModel</span>]  <span class="comment">// 数字得根据pipeline stage array中模型的位置来决定</span></span><br><span class="line">        <span class="keyword">val</span> trainingSummary = lrModel.summary</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Obtain the objective per iteration.</span></span><br><span class="line">        <span class="keyword">val</span> objectiveHistory = trainingSummary.objectiveHistory</span><br><span class="line">        println(<span class="string">"############################模型在每个迭代的损失值"</span>)</span><br><span class="line">        objectiveHistory.foreach(loss =&gt; println(loss))</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Obtain the metrics useful to judge performance on test data.</span></span><br><span class="line">        <span class="comment">// We cast the summary to a BinaryLogisticRegressionSummary since the problem is a</span></span><br><span class="line">        <span class="comment">// binary classification problem.</span></span><br><span class="line">        <span class="keyword">val</span> binarySummary = trainingSummary.asInstanceOf[<span class="type">BinaryLogisticRegressionSummary</span>]</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Obtain the receiver-operating characteristic as a dataframe and areaUnderROC.</span></span><br><span class="line">        <span class="keyword">val</span> roc = binarySummary.roc</span><br><span class="line">        println(<span class="string">"#############################ROC曲线"</span>)</span><br><span class="line">        roc.show()</span><br><span class="line">        println(<span class="string">"ROC面积"</span> + binarySummary.areaUnderROC)</span><br><span class="line"><span class="comment">//        println(binarySummary.predictions.show(20))</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// Set the model threshold to maximize F-Measure</span></span><br><span class="line"><span class="comment">//        val fMeasure = binarySummary.fMeasureByThreshold  // (threshold, F-Measure) curve</span></span><br><span class="line"><span class="comment">//        val maxFMeasure = fMeasure.agg(functions.max("F-Measure")).head().getDouble(0)</span></span><br><span class="line"><span class="comment">//        val bestThreshold = fMeasure</span></span><br><span class="line"><span class="comment">//          .where(fMeasure("F-Measure") === maxFMeasure)</span></span><br><span class="line"><span class="comment">//          .select("threshold").head().getDouble(0)</span></span><br><span class="line"><span class="comment">//        lrModel.setThreshold(bestThreshold)</span></span><br><span class="line">        <span class="comment">// 保存模型</span></span><br><span class="line">        pipelineModel.write.overwrite.save(modelSavePath)</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 测试模型</span></span><br><span class="line">        println(<span class="string">"###############################预测结果"</span>)</span><br><span class="line">        <span class="keyword">val</span> predictions = pipelineModel.transform(testData)</span><br><span class="line">        predictions.select(<span class="string">"user_id"</span>,<span class="string">"label"</span>,<span class="string">"rawPrediction"</span>,<span class="string">"probability"</span>,<span class="string">"prediction"</span>).show(<span class="number">50</span>)</span><br><span class="line">        <span class="keyword">val</span> evaluator = <span class="keyword">new</span> <span class="type">BinaryClassificationEvaluator</span>().setLabelCol(<span class="string">"label"</span>)</span><br><span class="line">        <span class="keyword">val</span> areaUnderROC = evaluator.setMetricName(<span class="string">"areaUnderROC"</span>).evaluate(predictions)</span><br><span class="line">        <span class="keyword">val</span> areaUnderPR = evaluator.setMetricName(<span class="string">"areaUnderPR"</span>).evaluate(predictions)</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 检查模型在测试集上的表现</span></span><br><span class="line">        <span class="keyword">val</span> lp = predictions.select( <span class="string">"label"</span>, <span class="string">"prediction"</span>)</span><br><span class="line">        <span class="keyword">val</span> countTotal = predictions.count()</span><br><span class="line">        <span class="keyword">val</span> correct = lp.filter(lp(<span class="string">"label"</span>) === lp(<span class="string">"prediction"</span>)).count()  <span class="comment">// 预测正确的样本数量</span></span><br><span class="line">        lp.show(<span class="number">200</span>)</span><br><span class="line">        <span class="keyword">val</span> ratioCorrect = correct.toDouble / countTotal.toDouble</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 1 流失 0 留存</span></span><br><span class="line">        <span class="keyword">val</span> truePositive = lp.filter(lp(<span class="string">"prediction"</span>) === <span class="number">1.0</span>).filter(lp(<span class="string">"label"</span>) === lp(<span class="string">"prediction"</span>)).count()  <span class="comment">// 真流失用户</span></span><br><span class="line">        <span class="keyword">val</span> falsePositive = lp.filter(lp(<span class="string">"prediction"</span>) === <span class="number">1.0</span>).filter(lp(<span class="string">"label"</span>) !== lp(<span class="string">"prediction"</span>)).count()  <span class="comment">// 假流失用户</span></span><br><span class="line">        <span class="keyword">val</span> trueNegative = lp.filter(lp(<span class="string">"prediction"</span>) === <span class="number">0.0</span>).filter(lp(<span class="string">"label"</span>) === lp(<span class="string">"prediction"</span>)).count()  <span class="comment">// 真留存用户</span></span><br><span class="line">        <span class="keyword">val</span> falseNegative = lp.filter(lp(<span class="string">"prediction"</span>) === <span class="number">0.0</span>).filter(lp(<span class="string">"label"</span>) !== lp(<span class="string">"prediction"</span>)).count()  <span class="comment">// 假留存用户</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 真正例率、假正例率</span></span><br><span class="line">        <span class="keyword">val</span> tpr = truePositive.toDouble / (truePositive + falseNegative)</span><br><span class="line">        <span class="keyword">val</span> fpr = falsePositive.toDouble / (falsePositive + trueNegative)</span><br><span class="line">        <span class="comment">// 流失用户查准率</span></span><br><span class="line">        <span class="keyword">val</span> positivePrecision = truePositive.toDouble / (truePositive + falsePositive)</span><br><span class="line">        <span class="comment">// 流失用户召回率</span></span><br><span class="line">        <span class="keyword">val</span> positiveRecall = truePositive.toDouble / (truePositive + falseNegative)</span><br><span class="line">        <span class="comment">// 留存用户查准率</span></span><br><span class="line">        <span class="keyword">val</span> negativeRecall = trueNegative.toDouble / (trueNegative + falsePositive)</span><br><span class="line">        <span class="comment">// 流失用户召回率</span></span><br><span class="line">        <span class="keyword">val</span> negativePrecision = trueNegative.toDouble / (trueNegative + falseNegative)</span><br><span class="line">        println(<span class="string">s"预测样本总数: <span class="subst">$countTotal</span>"</span>)</span><br><span class="line">        println(<span class="string">s"正确预测样本数量: <span class="subst">$correct</span>"</span>)</span><br><span class="line">        println(<span class="string">s"模型准确率: <span class="subst">$ratioCorrect</span>"</span>)</span><br><span class="line">        println(<span class="string">s"模型ROC值：<span class="subst">$areaUnderROC</span>"</span>)</span><br><span class="line">        println(<span class="string">s"模型PR值：<span class="subst">$areaUnderPR</span>"</span>)</span><br><span class="line">        println(<span class="string">s"预测结果中真流失用户个数：<span class="subst">$truePositive</span>"</span>)</span><br><span class="line">        println(<span class="string">s"预测结果中假流失用户个数：<span class="subst">$falsePositive</span>"</span>)</span><br><span class="line">        println(<span class="string">s"预测结果中真流失用户比例: <span class="subst">$tpr</span>"</span>)</span><br><span class="line">        println(<span class="string">s"预测结果中假流失用户比例: <span class="subst">$fpr</span>"</span>)</span><br><span class="line">        println(<span class="string">s"流失用户查准率：<span class="subst">$positivePrecision</span>"</span>)</span><br><span class="line">        println(<span class="string">s"流失用户召回率：<span class="subst">$positiveRecall</span>"</span>)</span><br><span class="line">        println(<span class="string">s"留存用户查准率：<span class="subst">$negativePrecision</span>"</span>)</span><br><span class="line">        println(<span class="string">s"留存用户召回率：<span class="subst">$negativeRecall</span>"</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 查看最优模型的参数</span></span><br><span class="line">        println(<span class="string">"打印模型参数：1"</span>)</span><br><span class="line">        println(cvModel.bestEstimatorParamMap)</span><br><span class="line">        println(<span class="string">"打印模型参数：2"</span>)</span><br><span class="line">        println(<span class="string">s"Coefficients: <span class="subst">$&#123;lrModel.coefficients&#125;</span> Intercept: <span class="subst">$&#123;lrModel.intercept&#125;</span>"</span>)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="在构建模型中遇到的问题"><a href="#在构建模型中遇到的问题" class="headerlink" title="在构建模型中遇到的问题"></a>在构建模型中遇到的问题</h2><h3 id="正负样本分配权重"><a href="#正负样本分配权重" class="headerlink" title="正负样本分配权重"></a>正负样本分配权重</h3><p>样本分布不均匀是再正常不过的了，计算正负样本占比，给不平衡的样本分配更多的权重。</p><p><a href="https://stackoverflow.com/questions/33372838/dealing-with-unbalanced-datasets-in-spark-mllib" target="_blank" rel="noopener">machine learning - Dealing with unbalanced datasets in Spark MLlib - Stack Overflow</a></p><h3 id="模型所有系数都为零"><a href="#模型所有系数都为零" class="headerlink" title="模型所有系数都为零"></a>模型所有系数都为零</h3><p><a href="https://stackoverflow.com/questions/46131807/empty-coefficients-in-logistic-regression-in-spark" target="_blank" rel="noopener">Empty Coefficients in Logistic regression in spark - Stack Overflow</a></p><p><a href="https://stackoverflow.com/questions/51397827/all-coefficients-turn-zero-in-logistic-regression-using-scikit-learn" target="_blank" rel="noopener">python - all coefficients turn zero in Logistic regression using scikit learn - Stack Overflow</a></p><h3 id="如何打印模型特征的权重？"><a href="#如何打印模型特征的权重？" class="headerlink" title="如何打印模型特征的权重？"></a>如何打印模型特征的权重？</h3><p><a href="https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/5211178207246023/3527163800997788/7788830288800223/latest.html" target="_blank" rel="noopener">Matching LR Coefficients With Feature Names - Databricks</a></p><h3 id="如何提取交叉验证模型最佳参数？"><a href="#如何提取交叉验证模型最佳参数？" class="headerlink" title="如何提取交叉验证模型最佳参数？"></a>如何提取交叉验证模型最佳参数？</h3><p><a href="https://stackoverflow.com/questions/31749593/how-to-extract-best-parameters-from-a-crossvalidatormodel" target="_blank" rel="noopener">scala - How to extract best parameters from a CrossValidatorModel - Stack Overflow</a></p><h3 id="逻辑回归多分类问题"><a href="#逻辑回归多分类问题" class="headerlink" title="逻辑回归多分类问题"></a>逻辑回归多分类问题</h3><p><a href="https://www.codercto.com/a/52163.html" target="_blank" rel="noopener">多分类实现方式介绍和在 Spark 上实现多分类逻辑回归 | 码农网</a></p><h3 id="如何在SQL中将所有特征放置在一行？"><a href="#如何在SQL中将所有特征放置在一行？" class="headerlink" title="如何在SQL中将所有特征放置在一行？"></a>如何在SQL中将所有特征放置在一行？</h3><p><a href="https://www.analyticshut.com/big-data/hive/pivot-rows-to-columns-in-hive/" target="_blank" rel="noopener">Pivot rows to columns in Hive - Analyticshut</a></p><h2 id="参考资源"><a href="#参考资源" class="headerlink" title="参考资源"></a>参考资源</h2><ol><li><a href="https://mapr.com/blog/predicting-breast-cancer-using-apache-spark-machine-learning-logistic-regression/" target="_blank" rel="noopener">Predicting Breast Cancer Using Apache Spark Machine Learning Logistic Regression | MapR</a></li><li><a href="https://mapr.com/blog/predicting-loan-credit-risk-using-apache-spark-machine-learning-random-forests/" target="_blank" rel="noopener">Predicting Loan Credit Risk using Apache Spark Machine Learning Random Forests | MapR</a></li><li><a href="https://cloud.tencent.com/developer/article/1352076" target="_blank" rel="noopener">用户增长分析——用户流失预警 - 云+社区 - 腾讯云</a></li><li><a href="https://www.libinx.com/2016/2016-11-29-machine-learning-algorithm-series-logistic-regression/">机器学习算法系列（3）Logistic Regression | Thinking Realm</a></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;先前的一段时间接手到一个流失用户预测的活，也就是根据某个群体用户的行为数据（动态特征）、自身特征（静态特征），建立一套流失预警的分类模型，预测用户的流失概率。类似于这种机器学习任务，毫无疑问，逻辑回归算法是首选之选。因为在很多场景下的需求问题都可以很容易地转化为一个分类或者
      
    
    </summary>
    
    
      <category term="Machine-Learning" scheme="https://www.libinx.com/categories/Machine-Learning/"/>
    
    
      <category term="logistic regression" scheme="https://www.libinx.com/tags/logistic-regression/"/>
    
      <category term="spark" scheme="https://www.libinx.com/tags/spark/"/>
    
      <category term="machine learning" scheme="https://www.libinx.com/tags/machine-learning/"/>
    
      <category term="scala" scheme="https://www.libinx.com/tags/scala/"/>
    
      <category term="逻辑回归" scheme="https://www.libinx.com/tags/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/"/>
    
      <category term="流失预测" scheme="https://www.libinx.com/tags/%E6%B5%81%E5%A4%B1%E9%A2%84%E6%B5%8B/"/>
    
  </entry>
  
  <entry>
    <title>白话解释统计学中的P值</title>
    <link href="https://www.libinx.com/2019/p-value-by-plain-explanation/"/>
    <id>https://www.libinx.com/2019/p-value-by-plain-explanation/</id>
    <published>2019-07-06T02:03:03.000Z</published>
    <updated>2021-03-23T01:25:10.994Z</updated>
    
    <content type="html"><![CDATA[<p>关于P值，统计学教材中的解释都比较拗口，难以理解，我举一个例子大家就明白了。</p><a id="more"></a><blockquote><p><em>但什么样的概率才算小呢？著名的英国统计学家费希尔把小概率的标准定为 0.05 ，虽然费希尔并没有对为什么选择 0.05 给出充分的解释，但人们还是沿用了这个标准，把 0.05 或比 0.05 更小的概率看成小概率。</em></p><p><em><a href="https://book.douban.com/subject/26375999/" target="_blank" rel="noopener">统计学 (豆瓣)</a></em></p></blockquote><p>当我们做假设检验的时候，通常是可以通过z统计量或者P值比较。但是，教程中的解释都比较拗口，我举一个例子大家就明白了。</p><p>关于P值的定义，就是我们假定原假设为真的前提下，<strong>由实际观察到的数据与原假设不一致的概率</strong>。</p><p>举个例子，我们假定硬币是均匀的，掷一枚普通硬币5次，如果硬币是均匀的（假定原假设为真），连抛5次得到都是正面的概率就是0.5的5次方，即0.03125（实际观察到的数据），这就是我们所说的P值，即发生这种事件（5次得到都是正面）的概率为0.03125。</p><p>使用P值决策的时候，我们会去拿一个观察到的事件发生概率（P值）与0.05做比较，如果这如果这个值比0.05还要小，那么说明，几乎不可能发生的事情，现在居然就发生了，所以我们就有理由拒绝原假设，不相信它是真的。</p><blockquote><p><em>P值的长处是它反映了观察到的实际数据与原假设之间不一致的概率值，与传统的拒绝域范围相比，P是一个具体的值，这样就提供了更多的信息。如果事先确定了显著性水平，如α＝0.05，则在双侧检验中，P＞0.025（α/2＝0.025）不能拒绝原假设；反之，P＜0.025则拒绝原假设。在单侧检验中，P＞0.05不能拒绝原假设，P＜0.05则拒绝原假设。当然，也可以直接使用P值进行决策，这时P值本身就代表了显著性水平。我们也可以使用P值，按照我们所需要的显著性水平进行判断和决策，具体做法就是用P值和需要的显著性水平进行比较。</em></p><p><em><a href="https://book.douban.com/subject/26375999/" target="_blank" rel="noopener">统计学 (豆瓣)</a></em></p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;关于P值，统计学教材中的解释都比较拗口，难以理解，我举一个例子大家就明白了。&lt;/p&gt;
    
    </summary>
    
    
      <category term="教程" scheme="https://www.libinx.com/categories/%E6%95%99%E7%A8%8B/"/>
    
    
      <category term="p值" scheme="https://www.libinx.com/tags/p%E5%80%BC/"/>
    
      <category term="假设检验" scheme="https://www.libinx.com/tags/%E5%81%87%E8%AE%BE%E6%A3%80%E9%AA%8C/"/>
    
      <category term="p-value" scheme="https://www.libinx.com/tags/p-value/"/>
    
  </entry>
  
  <entry>
    <title>再见，2018</title>
    <link href="https://www.libinx.com/2019/goodbye-2018/"/>
    <id>https://www.libinx.com/2019/goodbye-2018/</id>
    <published>2019-04-07T06:51:41.000Z</published>
    <updated>2021-03-23T01:25:10.994Z</updated>
    
    <content type="html"><![CDATA[<p>其实，年前的时候就计划着写一篇2018年的年终总结和另外一篇技术文章，奈何过年前的一段时间工作太忙，有的事情当时觉得不太方便放在年终总结里，再加上懒癌发作，后来就不了了之。正好清明节有点时间，腾出一个下午来补上之前的年终总结。</p><p>到目前为止，2019年到现在也仅仅发布了一则关于SQL优化的博文，输出量有点低，一来上班后就没有太多时间整理，总是想着把事情全部干完，事实上呢事情哪有做得完的，所以老是比较忙；二来总是觉得自己功底不够，零零散散的材料写得不够好，免得误导了别人，倒不如多累积一段时间，发布一些质量比较高的文章。</p><p>2018年对于我来说，相当于是人生一个阶段的终点，当年苦下决心二战也要考上研究生，从当时在哈尔滨的失意，到现在到帝都搬砖，一晃就四年了。当时完全出于对计算机感兴趣，并不知道这几年AI、机器学习会如此火爆，到现在来看，那是的坚持应该是正确的，现在仍然喜欢着自己所从事的工作。庆幸有那么段空闲的时间，否则我没有机会思考自己的出路。路还是要靠人走出来的。毕业那会正谋划着写一篇自己一路如何走过来的经历，因为只有自己走过之后才能有所体会，想着也许能给别人带来一些有益的启示，毕竟现在的人都很浮躁。前几天一个学弟用qq来咨询我专业相关的事情，开口就问毕业之后能挣多少钱，工作后要不要加班，我很难回答，有好几个学弟就咨询过我，后来都不了了之。</p><p>在我身边，这一年发生了不少事情，有好的也有坏的。梳理今年的几个节点：年初到北京找工作（初次体验北京的冬天，冷）、修改毕业论文（连续熬夜一个星期）、一次小型车祸（还好，人没事）、顺利通过毕业答辩、步入正式工作环境、前所未有的工作压力、自如空气质量房间（运气不好，造成短期身体不适）、转机。不过，起起落落，浮浮沉沉，不过都是过眼云烟，不存在对不起谁，问心无愧就好。</p><p>如果要给我的2018年选定一个关键词的话，我大概会选择：<strong>怀疑</strong>，因为有的事情颠覆了我之前的认知。</p><p><img src="https://raw.githubusercontent.com/swordspoet/i/img/778d5ca9ly1g1u91rf1d2j211j0p1q68.jpg" alt=""></p><h1 id="出师不利"><a href="#出师不利" class="headerlink" title="出师不利"></a>出师不利</h1><p>年初的那会到北京找工作，当时未花太多的时间就拿到了几个offer，从中间选择了一个我当时状态下最理想的一个。鉴于我的出身，以及那时的能力水平，当时觉得已经差不多了，做往上可能空间也不是特别大，后来就没有再继续下工夫去找了，开始在学校一心一意修改论文，这段时间也差不多把之前学习的东西都丢了，给后来的事件也造成了一定的影响。这份offer给当时的我带来了蜜汁自信（现在一想真是可笑），自我感觉良好，我想我这个水平放在我们学院同级的毕业生里面应该算很牛的吧，另外又想到我是从文科转向的cs，更加觉得自己了不得，似乎未来尽在我的掌控之中（哈哈），可是，天堂与地狱，谁能知道呢？</p><p>先前，我对自己的心态调整能力还是比较满意的，根据往常的经验，一般来说，再难的槛几天之后就会自然消散，后来才发现那是没有遇到真正的压力，还是太嫩了。我写下这段文字并不是为了去辩诉什么亦或者倒苦水给大家喝，更不会去诋毁他人，因为本来就不是统一路人，就权当做一个记录吧。</p><p>还记得从长沙买了一张站票，站了一个晚上才到北京，在第一个早上，陪伴了我三年的iphone6就在火车上宣布报废，一半面积的屏幕损坏，然后便拖着箱子去找房子。我曾经尝试着几次手动让它重新回到原来的状态，但都好景不长，这次彻底的报废正好让我死了折腾的这条心。</p><p>作为一个南方人，体验过让你无处躲藏的南方湿热后，心底自然而然地会像萌生像东北人炫耀他们那的天寒地冻的“优越感”，“你这20几度算什么热啊，去南方试试？”。然而，现实是，北京的夏天真的有点热，要不然清政府的统治者也不会花个几十年建造一座行宫用于避暑。这个城市如此傲娇，你爱来不来，爱走不走，她就在那里，我不管你这个社区住着几十万人，每天早上大约有一万人通勤，抱歉，入口只有这么大，等候区的酷热受不了我也没有办法（其实可以弄一个落地扇）。</p><p>一直以来，我非常痛恨那些抱怨生活的人，在我的世界观里，没有什么是忍受不了的，如果忍受不了，那就，那就再忍忍吧。偶然一次深夜下班打车回家，我骤然发现自己变成曾经讨厌的那个人了，我每天会向快车司机咒骂目前的处境、抱怨决策者的强权、宣泄工作上的压力，很庆幸再北京遇到的快车司机大多都是比较好的倾听者，他们总安慰我刚走进社会的还需要适应适应。</p><p>到现在我还记得某个休息日打车来公司加班的下午，当时北京的温度已经没有七月那般变态，快车司机跟我说他没有什么文化，说话比较朴素，当附近有附近高等学府的学子打车时他明显会感受到隔阂，我说应该不是你想的那样，他说可能是吧。然后，我照例抱怨了一下当前的处境，然后接下来的情景似好莱坞剧情般的演绎，他说起了他的故事，又谈到现在的处境，之后再分析我的处境，最后得出都会好的结论，当时的一番“演讲”让我浑身充满了能量。订单结束后，我给了他五星好评，还写了一段好评。</p><p><strong>一切如他所料，之后确实是变好了，不过来得有点迟。</strong></p><h2 id="未曾预料到的痛苦"><a href="#未曾预料到的痛苦" class="headerlink" title="未曾预料到的痛苦"></a>未曾预料到的痛苦</h2><p>正式的工作环境很nice，牛人很多，我所在的那个片区应该只有我一个双非学校的吧，也见识了那种写代码超级厉害的神人，跟我预想中的差不多。</p><p>我可以肯定地说，我肯定没有一般心理学上说的“自我归因偏差”，因为一般来说无论我碰到了什么质疑，首先我都会从自身开始寻找原因。这给当时的我带来了很大的痛苦，让我一度怀疑人生，怀疑自己。我不太确定当时的精神状态是否跟房子的空气质量有关，亦或是与工作压力共同作用的结果，睡眠质量差、发烧、脱发、耳鸣困扰了我很长时间，那段时间我甚至发现自己的听力下降了。一开始我一度怀疑自己不是干这行的料，几度濒临崩溃，越做越差。无论我怎么做似乎在别人眼里都是bullshit，都会受到嘲讽，我把原因都归结到了自己身上，我板上钉钉地包揽了所有的过错。后来，我不过才明白那早就是一艘即将沉没的船，这艘还未经历风暴考验的船处女航便被要求远渡重洋，沉没只是迟早的事，船员想要自保是最正当不过的了。也算是年轻时交的学费吧，It’s not my fault!!!从那以后，我便心里时时刻刻告诫自己以后绝对不会成为他们那样的人，出来混，迟早要还的。</p><p>不过后来还好，正如快车司机所憧憬的：“一切都会好的”，对，一切都会好的，如果还没好，那就再耐心等等吧。</p><h2 id="因祸得福"><a href="#因祸得福" class="headerlink" title="因祸得福"></a>因祸得福</h2><p>现在想想，庆幸当时没继续在那个地方，否则我会死的。我现在的工作的地方虽然不大，但是至少有成长的空间，领导和同事们都很包容。现在偶尔会写一些Python，目前主要的开发语言是Scala，接触大数据。接触Scala后我发现了一片新天地，原来Python束手无策的海量数据在Scala面前可以随意所欲地处理，Apache下的开源组件种类如此丰富，几乎覆盖了数据业务的方方面面。刚开始接触Scala觉得这门语言很奇怪，特别是箭头和占位符的用法，另外，Scala给人的感觉太灵活了，因为你会发现要实现一个功能，可以有很多种写法，这也可以，那也可以。写了几个SparkSQL任务后，发现上手也挺快的，再到后来就开始结合Spark做一些机器学习相关的项目了（小声说一句：Spark的官方文档真的很糟糕）。</p><p>以前我不相信运气一说，后来我相信了，努力不过是入场券，运气才决定了你能否留下来。很多时候你缺少的是那个小小的机会，举个例子，如果没有接触到实际的业务，一般来说练习sql的机会比较少，所以刚开始我觉得SQL很抽象，竟然不区分大小写，对格式也不敏感，所以觉得巨难掌握。由于缺少锻炼的机会，导致SQL都不怎么会写，其实吧，只要连续写上四五天的SQL就能达到熟练的水平了。<em>Don’t ever let somebody tell you… You can’t do something</em>，这话听着可能有些鸡汤，然而事实就是如此，不要因为别人随便一说就轻易地否定自己，说不定换个思路就好了，换个环境就好了，保持大脑极度开放，极度透明。</p><p>我在这里写了这么多东西并不是想控诉谁或者发牢骚，而是单纯地想记录下我的一点感受，人与人之间观念的差异是巨大的，你不可能让所有人都如你的意，那些走不到一条路上的人就各自安好吧。</p><h1 id="变化"><a href="#变化" class="headerlink" title="变化"></a>变化</h1><h2 id="可有可无的网络社交圈子"><a href="#可有可无的网络社交圈子" class="headerlink" title="可有可无的网络社交圈子"></a>可有可无的网络社交圈子</h2><blockquote><p><em>1 不要从朋友圈去猜测别人生活，也不要去羡慕或者获得优越感，因为那都是假的，你看到的是别人愿意让你看到的</em><br><em>2 自己也不要去朋友圈发生活照以获得认同，来自朋友圈的认同没有任何意义，冷暖自知</em><br><em>3 这是什么狗屎时代</em></p><p>—— <strong><em>知乎网友</em></strong></p></blockquote><p>境界是什么，境界就是能看到平时看不到的东西，听到平时听不到的声音。而要达到这种境界，首先需要提高对周围事物的敏感性，用心去感受，用眼睛去观察，用皮肤去体会触感。很可惜，有了社交软件，现在很少有机会能达到这种沉浸式的境界了，就是那种顺其自然的心流。音乐能快速地让人达到这种状态，我想那些花了几十万去买音响的人大概也是出于这个目的吧，他们想听听一些平时听不到的声音。大多数人没有那个条件，我想比较简单的办法是远离焦躁的社交圈子，抛弃一切扰乱你心智的东西，多跟自己对话，多听听别人的声音。</p><p>如果要说过去一年多一个比较大的变化的话，脱离无聊的社交圈子算是一个吧，大概是从去年的三月底开始我关闭了微信朋友圈，除了12月份在朋友圈发了一张图片，我大概有一年多时间没有看过朋友圈或者发布过其他消息了。朋友圈已经变成了爱慕虚荣者的竞技场，滋生嫉妒感的温床，满足窥视欲的工具，真没意思。原来自己也没有想象中的那么重要，除了亲人和几个好友，其他人的生活变化对我也没有太大的影响。关闭朋友圈带来的改变可能就是没有先前那么焦虑了吧，毕竟看到别人天南地北地耍而自己在苦苦挣扎，又或者做不到发自心底为别人的现状而高兴，倒不如什么也不看，落个清净，爱咋咋地。</p><p>我很高兴地宣布，自从限制了在这两个社交软件上投入的时间，即使工作再忙，我每个星期也能腾出八九个小时额外时间来看些杂七杂八的书，历史、投资、小说。我想，再过一点时间可能我连微信都会放弃，电报就是一个比较好的软件，活跃、友善、注重隐私（至少我加入的几个群组都是）。说到这里，还必须提到前一阵被销号的微博帐号，由于一时手贱，发布了一张调戏审查机制的图片，最终导致陪伴我近8年的帐号彻底死亡。令我愤怒的是之前写过的一些文章也全部被删除了，再也找不到了！后来一想，算了吧，反正也没有会去看。其实我是很高兴的，因为我之前数次戒掉微博失败，我再也不用在这个平台上浪费时间了，我不用为了博得别人关注而斟字酌句，又或者花时间写那些根本不会有人阅读的文章，又或者在这个平台上缺少友好、有帮助的交流气氛。如果说新浪还有那么点作用的话，图床算不算？我将把更多的时间花在Kindle和微信读书上。</p><h2 id="远离焦虑"><a href="#远离焦虑" class="headerlink" title="远离焦虑"></a>远离焦虑</h2><p>早在年初的时候，就有一篇贩卖焦虑的爆款文章《摩拜创始人套现15亿背后，你的同龄人，正在抛弃你》掀起一番波澜，我也写过关于焦虑的文章，不过由于帐号被封就看不到了。其实，多数人焦虑的原因是希望期待得到获取本阶段能力无法匹配的回报，也不是没有办法获得那些回报，自然得要拿出某些东西去换取，比如尊严、欺骗带来的信用破产、超常的付出、常人难以忍受的压力、运气等等，我称之为“献祭”，别无他法。</p><p>通常带着丛林法则来看待这个群体的有两类人，一类是与生俱来地拥有着普通人难以企及的资源，比如很多从事 IT 的朋友或许在大学之前都没有正儿八经摸过计算机，或者教育资源等等，这类人群把现阶段的优于平均水平的现状归结于自身的努力而非出生时的运气，他们认为只要努力也能轻而易举地达到他一样的水平，而忽略了运气的成分，他们不知道努力仅仅是步入更高阶段的入场券，开局一条狗和爆神装不是同一个概念。另一类人群则本身没什么资源，通过各种“献祭”手段达到超出平均水平一点点的位置，他们清楚什么才是真正的“努力”，清楚吃得苦中苦，方为人上人到底意味着什么，因为他们真的走过这条路，知道这条路很难走。说到底还是与自己的斗争，突破了，就赢了，没有超过临界值，就老老实实在原来的圈子待着。</p><p>考虑到两类人不同的背景，从他们看待问题的角度，或嘲讽、或蔑视都是可以理解的，毕竟立场不同。你所要做的，只有耐心等待，让自己每天进步一点点，哪怕就是一点点，认真地对待手头的每一件事情，该来的，总会来。</p><blockquote><p><em>人的一生会长大三次。第一次是在发现自己不是世界中心的时候。第二次是在发现即使再怎么努力，终究还是有些事令人无能为力的时候。第三次是在明知道有些事可能会无能为力，但还是会尽力争取的时候。</em></p><p>—— <strong><em>出处未知</em></strong></p></blockquote><h1 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h1><p>此时，2019年已经过去了三分之一了，想要写的文章很多，然而总是觉得无从下笔，想要看得书依然很多，我尽量抽时间在看，想要学的东西更多，一点一点来吧。我写这些东西呢，一方面是记录自己，另外呢就是分析给读者一些经验，可能有点说教的感觉。但是我觉得吧，当时这些道理都没人跟我讲过，我很清楚这条路是怎么走过来的，所以非常能够理解有些人的处境，如果能给他们带去一些有用的东西，那就再好不过了。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;其实，年前的时候就计划着写一篇2018年的年终总结和另外一篇技术文章，奈何过年前的一段时间工作太忙，有的事情当时觉得不太方便放在年终总结里，再加上懒癌发作，后来就不了了之。正好清明节有点时间，腾出一个下午来补上之前的年终总结。&lt;/p&gt;
&lt;p&gt;到目前为止，2019年到现在也仅
      
    
    </summary>
    
    
      <category term="思想王国" scheme="https://www.libinx.com/categories/%E6%80%9D%E6%83%B3%E7%8E%8B%E5%9B%BD/"/>
    
    
      <category term="总结" scheme="https://www.libinx.com/tags/%E6%80%BB%E7%BB%93/"/>
    
      <category term="年终总结" scheme="https://www.libinx.com/tags/%E5%B9%B4%E7%BB%88%E6%80%BB%E7%BB%93/"/>
    
  </entry>
  
  <entry>
    <title>SQL性能优化实践</title>
    <link href="https://www.libinx.com/2019/sql-optimization-practice/"/>
    <id>https://www.libinx.com/2019/sql-optimization-practice/</id>
    <published>2019-03-05T13:32:34.000Z</published>
    <updated>2021-03-23T01:25:10.994Z</updated>
    
    <content type="html"><![CDATA[<p>记录一次优化SQL查询的经历。</p><p><img src="https://raw.githubusercontent.com/swordspoet/i/img/778d5ca9ly1g0s916ail2j215o0rsn62.jpg" alt=""></p><a id="more"></a><p><strong>需求</strong>：统计历史某个时间段内每个新增设备在未来30天中的活跃情况</p><p><strong>问题</strong>：原SQL执行特别慢，原因在于通过<code>device_id</code>去关联<code>dws_base.device_info_day</code>的方式效率不高，它会为每个device_id去<code>dws_base.device_info_day</code>执行一次查询，每次的查询时间区间为自注册日<code>first_day</code>后的30天，所以导致查询时间特别长</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 原SQL</span></span><br><span class="line"><span class="keyword">SET</span> mapreduce.job.queuename=root.hive;</span><br><span class="line"><span class="keyword">select</span> a.dt,<span class="keyword">count</span>(<span class="keyword">distinct</span> a.device_id)new_device_nums,<span class="keyword">count</span>( <span class="keyword">case</span> <span class="keyword">when</span> b.device_id <span class="keyword">is</span> <span class="literal">null</span> <span class="keyword">then</span> a.device_id <span class="keyword">end</span>)liushi_device_nums</span><br><span class="line"><span class="keyword">from</span> </span><br><span class="line">(<span class="keyword">select</span> device_id,from_unixtime(<span class="keyword">unix_timestamp</span>(first_day),<span class="string">'yyyyMMdd'</span>)dt </span><br><span class="line"><span class="keyword">from</span> dws_base.device_info_all </span><br><span class="line"><span class="keyword">where</span> first_day&gt;=<span class="string">"2018-12-17 00:00:00"</span></span><br><span class="line"><span class="keyword">and</span> first_day&lt;=<span class="string">"2018-12-23 00:00:00"</span>)a  </span><br><span class="line"><span class="keyword">left</span> <span class="keyword">join</span></span><br><span class="line">(<span class="keyword">select</span> <span class="keyword">distinct</span> b.device_id,from_unixtime(<span class="keyword">unix_timestamp</span>(first_day),<span class="string">'yyyyMMdd'</span>)dt  </span><br><span class="line"><span class="keyword">from</span> dws_base.device_info_all a </span><br><span class="line"><span class="keyword">left</span> <span class="keyword">join</span> dws_base.device_info_day b </span><br><span class="line"><span class="keyword">on</span> a.device_id = b.device_id </span><br><span class="line"><span class="keyword">where</span> first_day&gt;=<span class="string">"2018-12-17 00:00:00"</span></span><br><span class="line"><span class="keyword">and</span> first_day&lt;=<span class="string">"2018-12-23 00:00:00"</span></span><br><span class="line"><span class="keyword">and</span> b.dt &gt;= from_unixtime(<span class="keyword">unix_timestamp</span>(a.first_day)+<span class="number">1</span>*<span class="number">24</span>*<span class="number">60</span>*<span class="number">60</span>,<span class="string">'yyyyMMdd'</span>)</span><br><span class="line"><span class="keyword">and</span> b.dt &lt;= from_unixtime(<span class="keyword">unix_timestamp</span>(a.first_day)+<span class="number">30</span>*<span class="number">24</span>*<span class="number">60</span>*<span class="number">60</span>,<span class="string">'yyyyMMdd'</span>)</span><br><span class="line"> )b</span><br><span class="line"><span class="keyword">on</span> a.device_id=b.device_id</span><br><span class="line"><span class="keyword">and</span> a.dt=b.dt</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> a.dt</span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span> a.dt;</span><br></pre></td></tr></table></figure><p><strong>思路</strong>：<code>device_info_all</code>（记为a）表中有设备id和设备首次登陆时间，<code>dws_base.device_info_day</code>（记为b）表中有设备id和只记录其当天的登陆的一条记录。 要统计a表中新增设备的登陆情况，则需要通过关联b表才能出来，但这里有一个问题，a表中设备的登陆记录可能不止一条，所以设备登陆日距离首次登陆日的日期差值date_diff会有多个，<code>date_diff</code>如何计算呢？如果按照之前的SQL，逻辑上是正确的，但是执行起来会消耗大量的资源，因为它会为每个设备id去b表中执行一次子查询，最终会导致内存溢出而执行不成功。所以我想了一个比较笨的办法，举个例子，如果我们要考虑<code>&quot;2018-12-17 00:00:00&quot;</code>至<code>&quot;2018-12-23 00:00:00&quot;</code>之间新注册登录的设备在在未来30天的活跃情况，<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(<span class="keyword">SELECT</span> <span class="keyword">distinct</span> device_id, from_unixtime(<span class="keyword">unix_timestamp</span>(first_day),<span class="string">'yyyyMMdd'</span>) dt <span class="keyword">FROM</span> dws_base.device_info_all </span><br><span class="line"><span class="keyword">where</span> first_day&gt;=<span class="string">"2018-12-17 00:00:00"</span> <span class="keyword">and</span> first_day&lt;=<span class="string">"2018-12-23 00:00:00"</span> <span class="keyword">and</span> device_id <span class="keyword">IS</span> <span class="keyword">NOT</span> <span class="literal">NULL</span> <span class="keyword">AND</span> device_id &lt;&gt; <span class="string">""</span>) <span class="keyword">as</span> a</span><br></pre></td></tr></table></figure></p><p>那么我们<strong>仅仅需要在b表中把设备登陆日志的时间窗口限制在<code>&quot;2018-12-18 00:00:00&quot;</code>至<code>&quot;2018-01-22 00:00:00&quot;</code>之间</strong>即可，<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">(<span class="keyword">select</span> * <span class="keyword">FROM</span> dws_base.device_info_day </span><br><span class="line"><span class="keyword">where</span> dt &gt;= from_unixtime(<span class="keyword">unix_timestamp</span>(<span class="string">"2018-12-17 00:00:00"</span>)+<span class="number">1</span>*<span class="number">24</span>*<span class="number">60</span>*<span class="number">60</span>,<span class="string">'yyyyMMdd'</span>) </span><br><span class="line">  <span class="keyword">and</span> dt &lt;= from_unixtime(<span class="keyword">unix_timestamp</span>(<span class="string">"2018-12-23 00:00:00"</span>)+<span class="number">30</span>*<span class="number">24</span>*<span class="number">60</span>*<span class="number">60</span>,<span class="string">'yyyyMMdd'</span>)) b</span><br></pre></td></tr></table></figure></p><p>然后计算a表中设备id首次登陆日期与b表中该id在未来30多天登陆记录日期的日期差<code>date_diff</code>。这样，便保证了<code>&quot;2018-12-17 00:00:00&quot;</code>至<code>&quot;2018-12-23 00:00:00&quot;</code>之间的设备完整地被限制在30天的窗口内，虽然除了23号，其他天数的考察窗口都超过了30天，但我们可以通过<code>date_diff</code>参数来区分，留存和流失用户。</p><p>最后我们拿到的数据有4列，分别是设备id（<code>device_id</code>）、设备首次登陆日期（<code>first_day</code>）、设备日常登陆日期（<code>login_day</code>）、设备登陆时距离首次登陆的时间（<code>date_diff</code>）到底当设备的<code>date_diff</code>符合什么样的条件才能被判定为活跃呢？总结了一下，大体分为4种情形：</p><ol><li>设备自首次登陆之后就再也没有登陆过，那它的<code>date_diff</code>就都为0，所以该设备被判定为<strong>流失</strong></li><li>设备自首次登陆之后的30天内没有登陆记录，但是30天后有登陆记录，此时的<code>date_diff{31,35,38...}</code>都大于30，但该设备判定为<strong>流失</strong></li><li>设备自首次登陆之后的30天内有登陆记录，并且30天后也有登陆记录，此时的<code>date_diff{0,5,16,31...}</code>有大于30的有小于30的，该设备判定为<strong>留存</strong></li><li>设备自首次登陆之后的30天内有登陆记录，并且30天后也有登陆记录，此时的<code>date_diff{5,16,31...}</code>有大于30的有小于30的，该设备判定为<strong>留存</strong></li></ol><p>现在的任务就是如何写出一个条件准确地筛选出流失用户和留存用户，一开始我的设置的条件是<code>min(date_diff)&lt;=30 and min(date_diff)&gt;0</code>，也就是只要当date_diff的最小值小于等于30且大于0就判定为留存用户，但是SQL执行后的结果却很奇怪，只有时间窗口的第一天（2018-12-17）数据是正常的，其他的日期则留存设备数量明显偏少，很不符合实际情况。后来才发现，如果是<code>2018-12-18</code>，因为设备登陆日志的窗口在<code>&quot;2018-12-18 00:00:00&quot;</code>至<code>&quot;2018-01-22 00:00:00&quot;</code>，那么此时的date_diff会有等于0的情形，而我们的<code>min(date_diff)&gt;0</code>把这部分数据都过滤掉了（对应情形3），所以导致数据不正常。然后我把过滤条件修改为<code>min(date_diff)&lt;=30 and avg(date_diff)&gt;0</code>，这样就完美地把4种情形区分开来了。<br>​<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 优化后的sql</span></span><br><span class="line"><span class="keyword">SET</span> mapreduce.job.queuename=root.hive;</span><br><span class="line"><span class="keyword">select</span> d.first_day dt,<span class="keyword">count</span>(<span class="keyword">distinct</span> d.device_id)new_device_nums,<span class="keyword">sum</span>(label)liucun_device_nums</span><br><span class="line"><span class="keyword">FROM</span> </span><br><span class="line">(<span class="keyword">select</span> c.device_id,c.first_day, </span><br><span class="line"><span class="keyword">case</span> <span class="keyword">when</span> (<span class="keyword">min</span>(date_diff)&lt;=<span class="number">30</span> <span class="keyword">and</span> <span class="keyword">avg</span>(date_diff)&gt;<span class="number">0</span>) <span class="keyword">then</span> <span class="number">1</span> <span class="comment">-- 防止过滤掉date_diff为0的数据</span></span><br><span class="line"><span class="comment">-- else date_diff=0 and min(date_diff)&gt;30 then 0 </span></span><br><span class="line"><span class="keyword">else</span> <span class="number">0</span> <span class="keyword">end</span> <span class="keyword">as</span> label</span><br><span class="line"><span class="keyword">FROM</span> </span><br><span class="line">(<span class="keyword">select</span> a.device_id,a.dt first_day,b.dt login_day,</span><br><span class="line"><span class="keyword">datediff</span>(from_unixtime(<span class="keyword">unix_timestamp</span>(b.dt,<span class="string">'yyyyMMdd'</span>),<span class="string">'yyyy-MM-dd'</span>),from_unixtime(<span class="keyword">unix_timestamp</span>(a.dt,<span class="string">'yyyyMMdd'</span>),<span class="string">'yyyy-MM-dd'</span>)) <span class="keyword">as</span> date_diff</span><br><span class="line"><span class="keyword">FROM</span> </span><br><span class="line">(<span class="keyword">SELECT</span> <span class="keyword">distinct</span> device_id, from_unixtime(<span class="keyword">unix_timestamp</span>(first_day),<span class="string">'yyyyMMdd'</span>) dt <span class="keyword">FROM</span> dws_base.device_info_all </span><br><span class="line"><span class="keyword">where</span> first_day&gt;=<span class="string">"2018-12-17 00:00:00"</span> <span class="keyword">and</span> first_day&lt;=<span class="string">"2018-12-23 00:00:00"</span> <span class="keyword">and</span> device_id <span class="keyword">IS</span> <span class="keyword">NOT</span> <span class="literal">NULL</span> <span class="keyword">AND</span> device_id &lt;&gt; <span class="string">""</span>) <span class="keyword">as</span> a</span><br><span class="line"><span class="keyword">left</span> <span class="keyword">join</span></span><br><span class="line">(<span class="keyword">select</span> * <span class="keyword">FROM</span> dws_base.device_info_day </span><br><span class="line"><span class="keyword">where</span> dt &gt;= from_unixtime(<span class="keyword">unix_timestamp</span>(<span class="string">"2018-12-17 00:00:00"</span>)+<span class="number">1</span>*<span class="number">24</span>*<span class="number">60</span>*<span class="number">60</span>,<span class="string">'yyyyMMdd'</span>) </span><br><span class="line">  <span class="keyword">and</span> dt &lt;= from_unixtime(<span class="keyword">unix_timestamp</span>(<span class="string">"2018-12-23 00:00:00"</span>)+<span class="number">30</span>*<span class="number">24</span>*<span class="number">60</span>*<span class="number">60</span>,<span class="string">'yyyyMMdd'</span>)) b </span><br><span class="line"><span class="keyword">on</span> a.device_id=b.device_id) </span><br><span class="line"><span class="keyword">as</span> c <span class="keyword">group</span> <span class="keyword">by</span> c.device_id,c.first_day) </span><br><span class="line"><span class="keyword">as</span> d</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> d.first_day</span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span> d.first_day;</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;记录一次优化SQL查询的经历。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/swordspoet/i/img/778d5ca9ly1g0s916ail2j215o0rsn62.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="Database" scheme="https://www.libinx.com/categories/Database/"/>
    
    
      <category term="sql" scheme="https://www.libinx.com/tags/sql/"/>
    
      <category term="hive" scheme="https://www.libinx.com/tags/hive/"/>
    
      <category term="大数据" scheme="https://www.libinx.com/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="bigdata" scheme="https://www.libinx.com/tags/bigdata/"/>
    
      <category term="性能优化" scheme="https://www.libinx.com/tags/%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96/"/>
    
  </entry>
  
  <entry>
    <title>查找算法（一）：查找</title>
    <link href="https://www.libinx.com/2018/linear-search/"/>
    <id>https://www.libinx.com/2018/linear-search/</id>
    <published>2018-11-27T02:39:25.000Z</published>
    <updated>2021-03-23T01:25:10.993Z</updated>
    
    <content type="html"><![CDATA[<p>基于有序列表的查找。</p><h1 id="线性查找"><a href="#线性查找" class="headerlink" title="线性查找"></a>线性查找</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">search</span><span class="params">(arr, target)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(arr)):</span><br><span class="line">        <span class="keyword">if</span> arr[i] == target:</span><br><span class="line">            <span class="keyword">return</span> i</span><br><span class="line">    <span class="keyword">return</span> <span class="number">-1</span></span><br></pre></td></tr></table></figure><h1 id="二分查找"><a href="#二分查找" class="headerlink" title="二分查找"></a>二分查找</h1><p>二分查找（也称为折半查找），在给定一个<strong>有序数组</strong>的前提下，取中间的元素（arr[mid]）作为比较的对象，如果target值大于中间元素，则在中间元素的右边继续查找，反之，则在中间元素的左边继续查找。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">binary_search</span><span class="params">(arr, target)</span>:</span></span><br><span class="line">    lo = <span class="number">0</span></span><br><span class="line">    hi = len(arr)<span class="number">-1</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">while</span> lo &lt;= hi:</span><br><span class="line">        mid = (lo + hi) // <span class="number">2</span></span><br><span class="line">        <span class="keyword">if</span> arr[mid] == target:</span><br><span class="line">            <span class="keyword">return</span> mid</span><br><span class="line">        <span class="keyword">elif</span> arr[mid] &gt; target:</span><br><span class="line">            hi = mid - <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            lo = mid + <span class="number">1</span></span><br><span class="line">            </span><br><span class="line">    <span class="keyword">return</span> <span class="number">-1</span></span><br></pre></td></tr></table></figure><h1 id="斐波那契数列查找"><a href="#斐波那契数列查找" class="headerlink" title="斐波那契数列查找"></a>斐波那契数列查找</h1><p>斐波那契数列公式：</p><script type="math/tex; mode=display">F(n) = F(n-1) + F(n-2)</script><p>斐波那契查找步骤如下：</p><p>第一步：从斐波那契数列中找到<strong><em>第一个</em></strong>大于或等于数组长度（n）的数（fibM）</p><p>第二步：当数组中还有未遍历的元素时：</p><pre><code> 1. 将游标cur定义为fib2覆盖部分的最后一个元素 2. 比较目标值与arr[cur]进行比较，如果匹配，立即返回索引，完成查找 3. 如果目标值大于arr[cur]，则把斐波那契数列下降一个单位 4. 如果目标值小于arr[cur]，则把斐波那契数列下降两个单位</code></pre><p>第三步：如果仅余一个元素用于比较，检验fib1是否等于1，如果是，判断该元素是否为target</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fibMonaccianSearch</span><span class="params">(arr, target, n)</span>:</span> </span><br><span class="line">    <span class="comment"># f(n) = f(n-1) + f(n-2)  </span></span><br><span class="line">    <span class="comment"># fib2 + fib1 = fibM</span></span><br><span class="line">    <span class="comment"># fib2 --&gt; fib1 --&gt; fibM</span></span><br><span class="line">    fib2 = <span class="number">0</span>  </span><br><span class="line">    fib1 = <span class="number">1</span> </span><br><span class="line">    fibM = fib2 + fib1 </span><br><span class="line">  </span><br><span class="line">    <span class="keyword">while</span> fibM &lt; n:  <span class="comment"># 从斐波那契数列中找到*第一个*大于或等于数组长度（n）的数（fibM）</span></span><br><span class="line">        fib2 = fib1 </span><br><span class="line">        fib1 = fibM </span><br><span class="line">        fibM = fib2 + fib1 </span><br><span class="line">  </span><br><span class="line">    offset = <span class="number">-1</span></span><br><span class="line">  </span><br><span class="line">    <span class="keyword">while</span> fibM &gt; <span class="number">1</span>:  <span class="comment"># 当fibM等于1，fib2等于0，后面没有元素了</span></span><br><span class="line">        cur = min(offset+fib2, n<span class="number">-1</span>)  <span class="comment"># cur相当于二分查找中的mid</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> arr[cur] &lt; target:  <span class="comment"># target大于arr[cur]，转向cur的右边</span></span><br><span class="line">            fibM = fib1        <span class="comment"># 斐波那契下降一个单位</span></span><br><span class="line">            fib1 = fib2 </span><br><span class="line">            fib2 = fibM - fib1 </span><br><span class="line">            offset = cur       <span class="comment"># cur的右边需要把cur之前的部分加到fib2上</span></span><br><span class="line">  </span><br><span class="line">        <span class="keyword">elif</span> arr[cur] &gt; target:  <span class="comment"># target小于arr[cur]，转向cur的左边</span></span><br><span class="line">            fibM = fib2          <span class="comment"># 斐波那契下降两个单位</span></span><br><span class="line">            fib1 = fib1 - fib2 </span><br><span class="line">            fib2 = fibM - fib1 </span><br><span class="line">  </span><br><span class="line">        <span class="keyword">else</span>: </span><br><span class="line">            <span class="keyword">return</span> cur</span><br><span class="line">  </span><br><span class="line">    <span class="keyword">if</span> fib1 == <span class="number">1</span> <span class="keyword">and</span> arr[offset+<span class="number">1</span>] == target: </span><br><span class="line">        <span class="comment"># 如果仅余一个元素用于比较，检验fib1是否等于1，如果是，判断该元素是否为target</span></span><br><span class="line">        <span class="keyword">return</span> offset+<span class="number">1</span> </span><br><span class="line">  </span><br><span class="line">    <span class="comment"># element not found. return -1  </span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">-1</span></span><br><span class="line">  </span><br><span class="line"><span class="comment"># Driver Code </span></span><br><span class="line">arr = [<span class="number">10</span>, <span class="number">22</span>, <span class="number">35</span>, <span class="number">40</span>, <span class="number">45</span>, <span class="number">50</span>, <span class="number">80</span>, <span class="number">82</span>, <span class="number">85</span>, <span class="number">90</span>, <span class="number">100</span>] </span><br><span class="line">n = len(arr) </span><br><span class="line">target = <span class="number">10</span></span><br><span class="line">print(<span class="string">"Found at index:"</span>, fibMonaccianSearch(arr, target, n))</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;基于有序列表的查找。&lt;/p&gt;
&lt;h1 id=&quot;线性查找&quot;&gt;&lt;a href=&quot;#线性查找&quot; class=&quot;headerlink&quot; title=&quot;线性查找&quot;&gt;&lt;/a&gt;线性查找&lt;/h1&gt;&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td
      
    
    </summary>
    
    
      <category term="algorithm" scheme="https://www.libinx.com/categories/algorithm/"/>
    
    
      <category term="面试" scheme="https://www.libinx.com/tags/%E9%9D%A2%E8%AF%95/"/>
    
      <category term="数据结构与算法" scheme="https://www.libinx.com/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"/>
    
      <category term="二分查找" scheme="https://www.libinx.com/tags/%E4%BA%8C%E5%88%86%E6%9F%A5%E6%89%BE/"/>
    
      <category term="线性查找" scheme="https://www.libinx.com/tags/%E7%BA%BF%E6%80%A7%E6%9F%A5%E6%89%BE/"/>
    
      <category term="斐波那契查找" scheme="https://www.libinx.com/tags/%E6%96%90%E6%B3%A2%E9%82%A3%E5%A5%91%E6%9F%A5%E6%89%BE/"/>
    
  </entry>
  
</feed>
