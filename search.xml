<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[机器学习算法系列（19）XGBoost]]></title>
    <url>%2F2021%2Fmachine-learning-algorithm-series-xgboost%2F</url>
    <content type="text"><![CDATA[XGBoost是GBDT的一种高效的实现，它是由华盛顿大学的陈天奇开发的一个高度可扩展的、端到端的提升系统。近几年XGBoost在各大算法竞赛中取得的成绩一时间可谓风头无两，它取得成功背后在于它在所有场景中的可扩展性，它在处理稀疏数据上的标点也是非常强大的。 1. 回顾提升树（Boosting Tree）提升树（Boosting Tree）是集成学习的代表之一，在之前的文章中我们就提到过，集成学习的本质：单个单个的基学习器表达能力弱没有关系，只要把它们组合起来了就可以很强大了，可以概括为“三个臭皮匠顶个诸葛亮”。 提升树实际上是K个弱学习器的线性组合而得到的加法模型，所谓提升（Boosting）的本意是在上一个弱学习器的基础上更进一步缩小与目标值的距离，使用公式可以表示为（来自陈天奇的论文）： 简单解释一下，数据集中的每一条数据都会落入弱学习器叶子结点的某一个区域，即对应的预测值，然后最终的预测结果等于所有树的预测值之和。如下图，小男孩（代表数据集中的一条数据）最终的预测结果为tree1和tree2预测值之和： 2. XGBoost损失函数的优化求解过程以上就是对提升树模型的简要回顾，下面正式进入XGBoost的部分，首先看XGBoost的损失函数： 回顾GBDT的损失函数： 可以看出，相比GBDT，XGBoost在其基础上添加了正则化项$\Omega(ht) = \gamma J + \frac{\lambda}{2}\sum\limits{j=1}^Jw{tj}^2$，正则化项中的$w{tj}$对应第$t$个弱学习器第$j$个叶子节点的最优值，与GBDT损失函数中的$c$是相同的意思（陈天奇的论文中是用$w$表示的，意思一样），$J$指的是叶节点的个数，$\gamma$和$\lambda$都是超参数。在损失函数中添加正则化项的有助于降低模型的复杂性和防止模型出现过拟合的情况。 接下来问题来了，如何最小化XGBoost的损失函数呢？对于公式（2），如果节点的分裂使用的是均方误差那很好优化，而XGBoost损失函数却无法通过一般的凸优化方法来求解，因为它实质上是一个NP hard的问题，因此XGBoost使用的贪心法来获得优化解，每次节点的分裂都期望最小化损失函数的误差，所以XGBoost的损失函数中加入了当前的树$f_t$，损失函数变成： 然后对该损失函数做泰勒的二阶展开得： 其中$g_i$和$h_i$是第i个样本在第t个弱学习器对$\hat{y}^{(t-1)}$一阶偏导和二阶偏导，由于损失函数中的$l$是常数，在最小化中可以去掉，上面的式子可以进一步写成： 以下是公式（4）的推导改写过程，将n条数据全部代入目标函数后，可以将式子从叶节点的角度改写一下 对于公式（4）的$w_j$的求解比较简单，损失函数对$w_j$求导并令其为0，可得最优解$w_j^{*}$： 然后将最优解代入公式（4），可得： 接着，每次在节点做左右子树分裂split的时候，我们要尽量减少损失函数的损失，也就是说，假设当前节点左右子树的一阶、二阶导数和为$G_L,H_L,G_R,H_R$，我们期望最大化下式（为什么是期望最大化下面会解释）： 整理，得： 这里有一个疑问，上面的这个式子是怎么从公式（6）计算得来的呢？自己想了很久没有想清楚，在B站上找到了一个up主把节点分裂的过程解释得非常到位！看下图，现在假设我们有从1到8的8个样本点。 第一次分裂，左子树两个样本点{7,8}，右子树有六个样本点{1,2,3,4,5,6}，此时的目标函数为： 接下来，我们选择某个特征对右子树做节点分裂，假设分裂的结果为左子树为{1,3,5}，右子树为{2,4,6}，此时的目标函数为： 在节点向下分裂的过程中，我们希望第二步目标函数变得越来越小，故实际上我们期望最大化的是第一步的目标函数减去第二步的目标函数：（这里请重点理解！）： 到此，XGBoost损失函数的优化求解就记录得差不多了。 3. XGBoost的训练过程 计算所有m个样本对于当前轮损失函数的一阶导数$G$和二阶导数$H$ 基于当前节点分裂决策树，初始化score为0，如果当前有$k$个特征： 样本按照第k个特征排好序，放入左子树，然后计算左右子树一阶和二阶导数和 更新score 基于score最大的特征分裂 4. XGBoost类库的参数在梯度提升决策树 的文章的末尾提到了GBDT在sklearn中可以调节的参数，对于XGBoost我们还是以sklearn为例，其实XGBoost和GBDT可调节的参数都差不多，在下一篇文章中会单独拿出来介绍。 5. GBDT和XGBoost的区别与联系： GBDT是集成学习算法，而XGBoost是GBDT的一种高效工程实现 对比GBDT的损失函数，XGBoost还加入了正则化部分，防止过拟合，泛化性能更优 XGBoost对损失函数对误差部分同时做了一阶和二阶的泰勒展开，相比GBDT更加准确 GBDT的弱学习器限定了CART，而XGBoost还有很多其他的弱学习器可以选择 GBDT在每一轮迭代的时候使用了全部的数据，而XGBoost采用了跟随机森林差不多的策略，支持对数据进行采样 GBDT没有设计对缺失值的处理，XGBoost能够自动学习出缺失值的处理策略 参考： XGBoost算法原理小结 - 刘建平Pinard - 博客园 XGBoost的技术剖析_哔哩哔哩 (゜-゜)つロ 干杯~-bilibili https://arxiv.org/pdf/1603.02754.pdf]]></content>
      <categories>
        <category>Machine-Learning</category>
      </categories>
      <tags>
        <tag>机器学习算法系列,集成学习,XGBoost,bagging</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《Spark快速大数据分析》思维导图及笔记]]></title>
    <url>%2F2020%2Fspark-big-fast-data-analysis%2F</url>
    <content type="text"><![CDATA[《Spark快速大数据分析》思维导图及笔记 spark定义快速、通用的集群计算平台2009年诞生于加州大学伯克利分校主要特点：内存运算，快！第一章：组件spark core 任务调度 内存管理 错误恢复spark sqlstreaming 实时数据进行流式计算：服务器日志、消息队列mllib 支持机器学习spark支持任何实现Hadoop接口的存储系统：文本文件、SequenceFile、Avro、Parquet第二章：核心概念每个应用都由一个驱动器程序发起集群上的并行操作驱动器程序通过sparkcontext对象访问spark val conf = new SparkConf().setMaster(“local”).setAppName(“My App”)val sc = new SparkContext(conf) # 初始化一个sparkContext 参数 setMaster()集群URL local单机执行 指定集群地址则集群执行 setAppName()应用名 spark-shell启动的时候已经自动创建了一个sparkcontext对象，即sc 调整输出日志级别 日志设置文件的模版log4j.properties.template log4j.rootCategory控制输出级别 每个spark应用都必须有一个驱动程序来启动，spark-shell在启动的时候就相当于开启了驱动程序 spark api会把一些基于函数的操作(filter)也会发送到集群上执行驱动器程序管理多个执行器（executor）节点 spark api会把函数发送到各个executor节点上，实现代码在多个节点上运行Maven 类似于Python中的pip，它是一个包管理工具，只需要将需要安装的包和版本号写在pom.xml文件中 比Python的pip更严格！第三章：RDD编程RDD定义 弹性分布式数据集 不可变的分布式对象集合创建RDD 方法一：读取外部数据集 val lines = sc.textfile(“path/to/file”) 数据并没有读取进来，只有在必要的时候才会读取 方法二：对一个集合进行并行化（一般在spark shell调试用的多） val lines = sc.parallelize(List(“pandas”, “i like pandas”)) 不应把RDD看做存着特定数据的数据集，而要把RDD当做一系列转化操作、计算步骤方法的列表，只有在必要的时候才执行特性 惰性 只有真正用到时才会真正计算 弹性 当保存RDD数据的一台机器失败时，spark可以利用弹性重新算出丢掉的分区 使用spark把rdd缓存到内存，重复使用RDD支持操作 转化（transformation）：针对各个元素 filter() 不会修改原有的RDD，它会将满足条件的RDD放入新的RDD中返回 map() 函数返回的结果作为新的RDD返回 返回的还是rdd 行动（action） first() take() count() collect() 非常消耗内存，不适合大规模数据集 通常在单元测试中用 通常有求和、求元素个数以及其他聚合操作，所以返回的是其他数据类型 伪集合操作 distinct 因为它会遍历所有的数据，所以开销很大 union intersection substract sample cartesian()笛卡尔积 开销巨大，比如物品之间的相似度计算开销就非常大持久化（缓存） 对重复使用的RDD进行持久化persist 如果RDD不会被重用，那么就没有必要对RDD持久化 cache() 与使用默认存储级别调用 persist() 是一样的 cache将rdd缓存到内存 persist则有多种缓存策略 StorageLevel MEMORY_AND_DISK:如果数据在内存中放不下,则溢写到磁盘上 MEMORY_AND_DISK_SER:如果数据在内存中放不下,则溢写到磁盘上。在内存中存放序列化后的数据第四章：键值对操作定义：kv操作 行动操作 reduceByKey：合并有相同键的值 combineByKey 会遍历分区的每一个元素 每个分区都是独立处理，对于同一个键会有多个累加器 groupByKey：对有相同键的值分组 转化操作数据分区 通过控制rdd分区方式减少通信开销第五章：数据读取与保存spark基于Hadoop生态圈构建，能访问s3、HDFS、HBASE等文件格式与存储系统spark-shell在启动的时候自动创建了sc（spark context）读取 文件系统 hdfs集群 本地文件系统 CSV、压缩文件格式 数据库 JDBC SequenceFile SequenceFile 是由没有相对关系结构的键值对文件组成的常用 Hadoop 格式 val data = sc.parallelize(List((“Panda”, 3), (“Kay”, 6), (“Snail”, 2)))data.saveAsSequenceFile(outputFile) HDFS第六章：Spark编程进阶基于分区进行操作 基于分区对数据进行操作可以避免为每个数据元素进行重复配置工作 foreachPartitions() mapPartitions()第七章：在集群上运行Spark运行架构 Spark采用的主从架构，一个Driver节点负责协调各个分布式节点Executor，Driver和Executor一同组成了Application，Spark的Application通过集群管理器（Cluster Manager）启动。Driver 将用户程序转为任务 为Executor调度任务Executor 负责运行任务 任务之间相互独立，及时某个Executor崩溃，任务也能继续执行第八章：Spark调优与调试RDD并行度 并行度太低集群资源浪费 数据shuffle的时候通过指定分区数量可以降低数据的通信成本 rdd在经过filter操作之后会产生很多空的分区或者数据很少的分区，可以通过合并降低分区数量提高性能。coalesce()内存管理 rdd存储，默认分配60%空间 数据shuffle与聚合，默认分配20%空间 用户代码，默认分配20%空间第十章：streaming使用离散化流（discretized stream）作为抽象表示，叫做DStream输入源：Flume、Kafka、HDFSstreaming 执行过程 启动接收器 从输入源收集数据并保存为rdd 将收集到的数据复制到另一个执行器进程保障容错性 驱动器程序运行spark作业处理收集的数据操作 转化 无状态转化操作 有状态转化操作 跨时区的 输出 保存为文件 保存为sequencefile 使用foreachRDD()存储到外部系统流数据 streamingcontext实例 zookeeper主机列表 消费组的名字 思维导图]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
        <tag>Scala</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark读取ElasticSearch数据——聚合查询]]></title>
    <url>%2F2020%2Fspark-elasticsearch-aggregation-query%2F</url>
    <content type="text"><![CDATA[本文主要介绍如何通过Spark读取ES中的数据，并对ES进行聚合查询。 我使用到的Scala、Spark和es的版本信息： 12345678910111213141516171819202122&lt;properties&gt; &lt;scala.version&gt;2.11.12&lt;/scala.version&gt; &lt;spark.version&gt;2.4.0-cdh6.3.1&lt;/spark.version&gt;&lt;/properties&gt;&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.scala-lang&lt;/groupId&gt; &lt;artifactId&gt;scala-library&lt;/artifactId&gt; &lt;version&gt;$&#123;scala.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.scala-lang&lt;/groupId&gt; &lt;artifactId&gt;scala-actors&lt;/artifactId&gt; &lt;version&gt;$&#123;scala.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.elasticsearch&lt;/groupId&gt; &lt;artifactId&gt;elasticsearch-spark-20_2.11&lt;/artifactId&gt; &lt;version&gt;6.5.4&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 下面是Spark读取es数据的测试代码，大家可以参考一下，主要有三种方法，用到了es的高阶API（EsSparkSQL、EsSpark.esRDD）和低阶API，建议使用方法一和方法三，方法二在测试中发现es的query语句不生效，测试了很多遍也没有通过。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111import java.net.InetAddressimport java.util.Propertiesimport org.apache.spark.sql.SparkSessionimport org.apache.spark.sql.functions.maximport org.elasticsearch.action.search.SearchTypeimport org.elasticsearch.common.settings.Settingsimport org.elasticsearch.common.transport.TransportAddressimport org.elasticsearch.search.aggregations.AggregationBuildersimport org.elasticsearch.search.aggregations.metrics.max.InternalMaximport org.elasticsearch.spark.rdd.EsSparkimport org.elasticsearch.spark.sql.EsSparkSQLimport org.elasticsearch.transport.client.PreBuiltTransportClientobject SparkEsDemo &#123; def main(args: Array[String]): Unit = &#123; run() &#125; def run(): Unit = &#123; val sparkSession = SparkSession .builder() .appName("SparkEsTestDemo") .config("es.nodes", "xxx.xxx.xxx.xxx") .config("es.port", 9200) //重试5次（默认3次） .config("es.batch.write.retry.count", "5") //等待60秒（默认10s） .config("es.batch.write.retry.wait", "60") //es连接超时时间100s（默认1m） .config("es.http.timeout", "200s") .getOrCreate() // 方法一：通过将es的数据读取为dataframe val esResource = "esIndex/esType" // 换成你自己的索引和索引类型 val esDf = EsSparkSQL .esDF(sparkSession,resource = esResource) // resource参数填写es的索引和索引类型 .agg(max("fieldName")) // 到了这里就相当于直接将es的数据转化为dataframe直接做计算了，很直观 .withColumnRenamed("max(fieldName)","maxFieldName") .na.fill(0) esDf.show(false) // 方法三：通过编写Query查询语句查询es数据，返回的是rdd数据 val query = s"""&#123; | "query": &#123; | "bool": &#123;&#125; | &#125;, | "size": 0, | "aggs": &#123; | "max_price": &#123; | "max": &#123; | "field": "price" | &#125; | &#125; | &#125; |&#125;""".stripMargin val esRdd = EsSpark .esRDD(sparkSession.sparkContext, esResource, query) // https://github.com/elastic/elasticsearch-hadoop/issues/276 esRdd.take(10).foreach(println) esRdd.take(10).foreach(f =&gt; &#123; val map:collection.Map[String,AnyRef] = f._2 for (s &lt;- map) &#123; println(s._1 + s._2) &#125; &#125;) // 方法三：通过es客户端的低阶API实现聚合查询 // https://www.cnblogs.com/benwu/articles/9230819.html val prop = new Properties() val esClient = initClient(prop) val searchResponse = esClient .prepareSearch("recommend_smartvideo_spider_acgn") .setTypes("smartvideo_type_user_irrelative") .setSearchType(SearchType.DFS_QUERY_THEN_FETCH) .setExplain(true) .setSize(0) .addAggregation(AggregationBuilders.max("max_cursor").field("inx_cursor")) .setFetchSource(false) .execute().actionGet() val maxValue:InternalMax = searchResponse.getAggregations.get("max_cursor") println(maxValue.value()) &#125; /** * 初始化es客户端 * @param prop es配置类 * @return */ def initClient(prop: Properties): PreBuiltTransportClient = &#123; val setting = Settings.builder() .put("cluster.name", prop.getProperty("es.cluster.name")) .put("client.transport.sniff", true) .build() val client = new PreBuiltTransportClient(setting) val hostAndPortArr = prop.getProperty("es.host&amp;port").split(",") for (hostPort &lt;- hostAndPortArr) &#123; val hp = hostPort.split(":") if (hp.length == 2) &#123; client.addTransportAddress( new TransportAddress(InetAddress.getByName(hp(0)), hp(1).toInt) ) &#125; else &#123; client.addTransportAddress( new TransportAddress(InetAddress.getByName(hp(0)), 9300) ) &#125; &#125; client &#125;&#125;]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
        <tag>Scala</tag>
        <tag>elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark编程基础（Scala版）笔记]]></title>
    <url>%2F2020%2Fspark-coding-scala%2F</url>
    <content type="text"><![CDATA[记录了部分章节的笔记，这本书语言平时易懂，看得出来作者是很用心地写书，不是照着官方文档翻译，我推荐林子雨老师的这本书作为入门Spark的读物。 第二章：Scala语言基础什么是Scala ? 混合编程范式语言：函数式编程和面向对象编程 每个值都是对象，每个操作都是方法调用 兼容Java数据类型与变量 操作符实际也是方法：5 + 3和5.+(3)等价 尽量使用括号理清操作符的优先级别 print()和println()的区别是后者输出结束时会默认加上换行符控制结构 三元表达式：val a = if (6&gt;0) 1 else -1 for循环 for (变量 &lt;- 表达式) {语句块} 变量 &lt;- 表达式 被称为生成器 守卫式：for (变量 &lt;- 表达式 if 条件表达式) {语句块} 异常处理结构 try-catch breakable数据结构 数组 可变 可索引 元素必须是相同类型 元组 可以容纳不同类型 容器 定义序列、映射、集合等数据结构 序列 列表list 采用链表结构，注意时间复杂度 一旦定义便不可变 元素必须是相同类型 range 带索引的不可变数字等差数列 集合 无索引 元素不重复 映射 推荐使用get()方法面向对象编程 类（class） 理解为“模板”，定义好类之后就能通过new关键字创建对象 语法：def 方法名(参数列表): 返回结果类型={方法体} 所有的方法都是不可变的 允许方法重载 可见性：privated成员只对本类型和嵌套类型可见，protected成员对本类型和其他继承类型都可见 主构造器和辅助构造器this区别：辅助构造器返回的类型为Unit、辅助构造器最终都始于对主构造器的调用 对象（object） 单例对象的定义与类的定义类似，只是用object关键字替换了class关键字 无法被实例化，单例 单例对象与某个类具有相同的名称时，则单例对象被称为“伴生对象”，这个类叫做“伴生类” 伴生对象和伴生类必须位于同一个文件下 调用伴生类的apply方法创建实例无需使用new关键字 apply方法也被称为工厂方法，定义在类中实现自动调用，当用户在创建实例时，无需使用new关键字就能生成对象 继承 如果一个类包含没有实现的成员，则必须使用abstract修饰，并定义为抽象类 子类的主构造器必须调用父类的主构造器或者辅助构造器函数式编程 映射操作 map flatmap 规约操作 reduce flod第三章：Spark的设计与运行原理基本概念 RDD RDD采用惰性计算，在“行动操作”之前不会发生真正的计算 RDD是一种数据抽象，它代表了一系列的转换处理，省略了大量的中间结果的存储，降低IO的开销，所以速度才会很快 容错性高 RDD分区之间会有不同的依赖关系，DAG调度器则会根据RDD之间的宽依赖或者窄依赖关系把DAG图划分为不同的阶段 DAG（有向无环图） Executor 多线程 io性能高 应用（application） 作业（Job） 阶段（Stage） 任务（Task） 依赖关系 宽依赖：RDD之间是一对多的关系，所以失败之后恢复的开销比较大 窄依赖：RDD之间是一对一的关系，失败之后恢复更加高效 DAG调度器遇到宽依赖就断开，因为窄依赖不会设计shuffle可以实现流水线操作 部署模式 客户端模式 集群模式架构设计 Spark+YARN：首先创建一个SparkContext对象，然后到YARN申请资源，YARN给Excutor分配好资源之后，启动Excutor进程，SparkContext构建DAG图并分解为多个阶段的任务，并把任务分发给Excutor执行。第五章：RDD编程核心概念 RDD是Spark的核心概念，它是一个只读的、可分区的分布式数据集，这个数据集的全部或部分可以缓存在内存中，可在多次计算间重用如何创建 从文件系统中创建 通过并行集合创建：sc.parallelize()操作 转换 flat(),map(),flatMap(),groupByKey(),reduceByKey() 行动持久化(persist)为什么要分区？ 增加并行度 减少通信开销 RDD 分区的一个原则是使分区的个数尽量等于集群中的 CPU 核心（Core）数目。 可以手动设置分区的数量，主要包括两种方式：（1）创建RDD时手动指定分区个数；（2）使用reparititon方法重新设置分区个数。 自定分区的方法 哈希分区 区域分区第七章：Spark Streaming概述 静态数据和流数据 批量计算和实时计算 数据分散存储在不同的机器上，因此需要实时汇总来自不同机器上的日志数据。数据源 Kafka Broker：Kafka集群的服务器 Topic：Kafka消息的类别 Partition：每个Topic包含一个或者多个Partition Producer：生产者，负责发布消息到Broker Consumer：消费者 Consumer Group Flume HDFS执行流程 离散化的数据流（DStream）被切分成一段一段，每段数据转换为RDD，然后可以针对RDD做相对应的操作，对DStream的操作最终都转化为对RDD的操作 对比Storm的区别在与Spark Streaming无法实现毫秒级响应，但是RDD的容错率更高效思维导图]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
        <tag>Scala</tag>
        <tag>Spark编程基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一次配置深度学习主机和安装TensorFlow2.0 GPU的经历]]></title>
    <url>%2F2019%2Fexperience-of-deep-learning-computer-and-tensorflow-gpu%2F</url>
    <content type="text"><![CDATA[记录一次配置深度学习主机和安装TensorFlow2.0 GPU的经历。 配置深度学习主机工欲善其事必先利其器，既然是深度学习，如果没有GPU加持那还算是深度学习吗？不过，GPU向来价格比较昂贵，一张近一年内新上市的GPU价格动辄几千，假如你不是专门的研究者或者没有企业级别的需求，我的建议是没有必要暂时花费这笔钱，因为我自己之前就吃过这些苦头，当时一门心思买昂贵的设备用来学习，可是大多是时候都是在吃灰，自己的数据集根本不需要那么多资源。 所以，我的建议是，我们可以选择那些上市时间有两年但是算力还不错的二手显卡，如果能买到全新的那就更好了，为什么这么做？因为你想想，前两年深度学习的任务不也是使用的这些设备吗，这些GPU一样在那时也可以满足我们的需求，当然，二手的显卡需要我们仔细甄别，最好选择那些信誉、评价比较高而且真实的卖家。 当我配置主机的时候我一直保持几个原则： 实用。比如RGB灯光和五颜六色的装饰真的不会给你的内存条带来什么加成，内存条就是内存条，它不是装饰；另外，CPU能买散片就买散片，为什么？如果现在咱们能具备仿制出CPU的能力也不会受制于人了。 货比三家。同一件商品在不同平台和不同卖家的售价往往有很大的区别，在厂家品控合格的前提下，购买低价的一般不会翻车。 需求。自身的需求，我看了许多的装机视频，有些人动辄就是花费上万购置设备，装完机之后就是用来看看视频和刷刷网页，那我看就完全没有必要了。 考虑用于深度学习的机器，无非有两种选择：笔记本或者台式机，前者便携省心，但价格相对昂贵，后者不易携带，然而具备价格优势，这两者似乎天生具备不可调和的矛盾。那么会不会有一种方案能同时满足这两个优势呢，既便携且价格不那么昂贵，答案是肯定的。我也是这一阵在搜集资料的时候才发现的，居然还有itx机箱这个选择，就是支持的主板为17cm*17cm尺寸大小，整机可以放入一般的双肩背包。 主机中花费最多的部分就是CPU和GPU了，这两者几乎占据了装机费用的一半，所以你的心理预期与这两个因素强烈相关，比如以我为例，刚开始我的心理预期是4000左右，现在一般的CPU散片至少得1000了，所以这就决定了我的显卡上不能投入太多，我一开始的选择是gtx750ti，算力5.0，能满足一般的需求，但是显存太小了，而且这块显卡还是二手的。 后来我把我的配置单发到V站社区，社区的给我的反馈是GPU的显存太小了，做不了什么用，至少得1060，后来我权衡了一下确实2G的显存有点小。所以，我改变了预计的方案，显卡换成了二手的微星红龙1060，这张显卡我在16年装机的时候就购入过，品控方面经得住考验，只是二手的微星红龙现在还能卖1000出头。 但是后来发现微星红龙显卡跟我的itx机箱不匹配，它太宽了，我的机箱支持的显卡宽度最大为13.2cm，而微星红龙的宽度将近14cm，所以死活也放不进机箱，无奈只能临时把微星红龙退了（红龙的造型真的很酷啊！！！），火速买了一张全新的映众1060显卡，因为我看了评测，这款显卡的散热比较强，我觉得与体积比较小的mini机箱比较匹配。 CPU一开始的选择是9400，后来发现我没有用核显的必要，所以就选择了9400f，也就是不带核显的版本，免除后续安装驱动的烦恼。 所以，整个下来我这台用于深度学习的主机的配置大致是：i5 9400f+16G内存+480G SSD+GTX 1060 6G+sanc 2k显示器，整个花费差不多5000元。 在Ubuntu18.04.1上安装TensorFlow2.0-GPU因为我配置这台机器的目的是为了接触TensorFlow2.0，所以使用GPU版本的TensorFlow2.0是必须的，我日常的开发环境均是基于Linux桌面版，而英伟达支持的桌面发行版又仅仅限于几类。在这之前我曾经尝试在deepin上安装驱动，结果把系统给整崩溃了，所以我告诫各位不要在日常开发环境和英伟达不支持的桌面环境下安装驱动，你永远不知道前面会有多少坑要踩。 CUDA开发环境依赖于与主机开发环境（包括主机编译器和C runtime库）的紧密集成，因此仅在已获得此CUDA Toolkit版本合格的Linux发行版中受支持，换言之，如果NVIDIA官方不支持你使用的Linux发行版，你也无法使用CUDA加速！ 不得不说，在Linux平台装英伟达的驱动是一个非常劳心费神的事情，这中间有无数的坑等着你去踩，如果你使用的Windows可能半天就弄明白了，如果你是Linux用户那个在这个基础上再乘以5吧，我花了差不多20多个小时才弄明白，中间系统重装了5、6次，所以这也是为什么我建议你在一台全新机器上进行这些实验，千万不要在开发环境中尝试这些操作，它有可能导致无法开机和资料丢失的情况发生。 五个认知： 本教程仅仅针对Ubuntu18.04.1的单显卡用户，即无核显，只有独显 CUDA自带显卡驱动，所以只需要安装好了CUDA之后附带英伟达的驱动也随之安装成功，nvidia-smi命令可以检验驱动是否安装成功 强烈建议你在一台装有全新的Ubuntu系统的设备上安装GPU驱动！！！强烈建议Ubuntu的安装的版本为18.04.1，而不是最新发布的是18.04.3，最新版本的18.04.3内核版本（5.0.0）过高，而支持CUDA10.0的内核是低于它的 CUDA版本已经升级到了10.1，然而TensorFlow2.0目前只支持CUDA10.0，所以你如果想要在TensorFlow2.0中使用GPU加速，那么你别无选择，只能安装CUDA10.0！！！ 英伟达官方对Linux社区的支持力度太弱了，so，Fuck NVIDIA!!! 三个前提： 拥有至少一张或者多张支持CUDA加速的GPU：NVIDIA官方列出了支持CUDA加速的GPU设备列表（CUDA GPUs | NVIDIA Developer），你可以根据你的显卡型号查看是否支持CUDA加速已经当前的算力。我当前是一张GTX1060 6G的显卡，算力6.1 lspci | egrep &#39;3D|VGA&#39;：查看你当前的核显和独显设备 你需要在TensorFlow2.0中获得GPU加速吗？如果是，那么目前你只能安装CUDA10.0；如果不是，你仅仅是PyTorch用户，那么根据你Ubuntu的内核版本选择CUDA10.0或CUDA10.1 确认Ubuntu系统版本和内核版本，并选择安装相对应的CUDA版本（这一步非常重要！！！）：比如都是Ubuntu 18.04（18.04.1），CUDA10.0就只支持4.15内核版本，如果你安装的是最新的Ubuntu18.04（18.04.3），其内核版本是5.0.0，所以你需要安装CUDA10.1。否则会报错，并提示你要安装最新版本的驱动：NVIDIA-SMI has failed because it couldn&#39;t communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running. CUDA10.0支持的Linux系统和内核版本：https://docs.nvidia.com/cuda/archive/10.0/cuda-installation-guide-linux/index.html CUDA10.1（最新）支持的Linux系统和内核版本：https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html uname -r：确认你当前Ubuntu使用的内核版本，我的是4.15.0-20-generic lsb_release -a：确认Ubuntu版本，我的是18.04.1 所以，我需要安装CUDA10.0版本 八步支持TensorFlow2.0 GPU加速 卸载已有的CUDA和英伟达驱动，如果你是全新系统，请忽略这一步 123# Purge existign CUDA firstsudo apt --purge remove &quot;cublas*&quot; &quot;cuda*&quot;sudo apt --purge remove &quot;nvidia*&quot; 进入CUDA归档列表：CUDA Toolkit Archive | NVIDIA Developer，选择在“两个前提”确认你所要安装的CUDA版本，点击进入，并按照提供的步骤逐步执行 | CUDA10.0支持的发行版和内核 | CUDA10.1支持的发行版和内核 | | —————————————————————————————— | —————————————————————————————— | | | | 安装Anaconda：清华大学镜像源下载链接:清华大学开源软件镜像站，使用Anaconda安装cuda10.0工具包：conda install cudatoolkit=10.0 添加nvcc环境变量（nvcc是英伟达的编译器）：sudo gedit ~/.bashrc，在文件的末尾添加以下三行，保存： 123export CUDA_HOME=/usr/local/cudaexport PATH=$PATH:$CUDA_HOME/binexport LD_LIBRARY_PATH=/usr/local/cuda-10.0/lib64$&#123;LD_LIBRARY_PATH:+:$&#123;LD_LIBRARY_PATH&#125;&#125; 执行source ~/.bashrc，使环境变量生效，接下来，验证CUDA编译器是否安装好，输入nvcc -V 重启机器：sudo reboot，终端输入nvidia-smi，可以查看显卡的型号和驱动版本，如果有返回值则证明英伟达的驱动安装成功！然而，此时我们高兴得还为时过早，因为此时的NVIDIA还没有为TensorFlow2.0提供GPU支持，我们还需要安装cuDNN 安装cuDNN7.6.5版本：根据页面选择CUDA10.0对应cuDNN，下载： 12345678# 解压，并把文件拷贝到一个新的文件夹下：/home/yourname/software/cudnn/tar -zxvf cudnn-10.0-linux-x64-v7.6.5.32.tgz# 设置环境变量sudo vi ~/.bashrc# 末尾添加export LD_LIBRARY_PATH=&quot;/home/yourname/software/cudnn/lib64:$LD_LIBRARY_PATH&quot;# 环境变量生效source ~/.bashrc pip install tensorflow-gpu，终端打开iPython，验证TensorFlow2.0是否已经获得GPU支持 123import tensorflow as tf# 如果返回true则表示TensorFlow2.0已经获得GPU支持，成功tf.test.is_gpu_available()]]></content>
      <categories>
        <category>DeepLearning</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>装机</tag>
        <tag>tensorflow2.0</tag>
        <tag>tensorflow-gpu</tag>
        <tag>深度学习主机</tag>
        <tag>cuda10.0</tag>
        <tag>nvidia drivers</tag>
        <tag>nvidia</tag>
        <tag>英伟达驱动安装</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Scala Spark取出DataFrame中列中的值]]></title>
    <url>%2F2019%2Fgetting-values-from-dataframe-in-spark%2F</url>
    <content type="text"><![CDATA[Scala Spark取出DataFrame中列中的值 12345678910111213141516171819202122232425scala&gt; val df = Seq( | ("one", 2.0), | ("two", 1.5), | ("three", 8.0) | ).toDF("id", "val")df: org.apache.spark.sql.DataFrame = [id: string, val: double]scala&gt; df.show()+-----+---+| id|val|+-----+---+| one|2.0|| two|1.5||three|8.0|+-----+---+scala&gt; df.select("id").collect().map(_(0)).toListres6: List[Any] = List(one, two, three)scala&gt; df.select("id").rdd.map(_(0)).collect.toList res7: List[Any] = List(one, two, three) scala&gt; df.select("id").map(_.getString(0)).collect.toList res8: List[String] = List(one, two, three)]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
        <tag>Scala</tag>
        <tag>SparkSQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[浅析Spark Architecture：Shuffle（二）]]></title>
    <url>%2F2019%2Fspark-architecture-shuffle-02%2F</url>
    <content type="text"><![CDATA[在浅析 Spark Architecture：Shuffle（一） | Thinking Realm这篇文章中我主要向大家介绍了Spark Shuffle的运行原理和随着Spark升级导致Shuffle运行机制的变化。 而这篇文章主要介绍在Spark中哪些操作会触发Shuffle、Shuffle的bypassMergeThreshold运行机制和4个与Shuffle相关的参数。 何时会触发Spark Shuffle操作？首先，从字面上来理解，Shuffle的意思就是“洗牌”，就是要把原来混乱的数据重新整理，而往往数据又不是分布在同一个地方的，在这个过程中必然会涉及到数据的移动，所以不难理解Shuffle是一个非常消耗资源的操作，通常可以通过数据分区来降低Shuffle带来的网络传输开销。 在Spark中，map、filter、union操作不会触发Shuffle操作，因为这些操作都是针对单个数据本身的改变，数据与数据之间并不会发生关联或者交换操作。而诸如分区操作如repartition 、coalesce或者groupByKey、sortByKey等ByKey的操作一般会触发Shuffle，groupByKey会对数据做分组处理，而sortByKey需要比较数据与数据之间的先后顺序。 类型 repartition repartitionorcoalesce ByKey groupByKeyorsortByKey join 推荐两个链接：第一个说的是partitionBy和repartition之间的区别，第二个解释在Spark中哪些操作会引发Shuffle。 Spark shuffle – Case #1 – partitionBy and repartition – Tantus Data mapreduce - When does shuffling occur in Apache Spark? - Stack Overflow Spark（&gt;1.2.0）Shuffle的bypassMergeThreshold运行机制从Spark 1.2.0开始，Spark Shuffle默认的算法便变为了sort，可以通过spark.shuffle.manager选择相应的Shuffle算法。在上一篇文章中有提到过，Sort Shuffle的原理与Hadoop Shuffle有着相似的实现逻辑，Map端只会输出两个文件，分别是数据文件和记录结果数据的索引文件，由此，Reduce端就很容易根据索引文件找到记录结果的数据文件位置。 值得注意的是，最新版本的Spark在Sort Shuffle机制也并不完全只是Sort Based，在SortShuffleManager下有一个spark.shuffle.sort.bypassMergeThreshold参数比较有意思，它主要用于决定当Reduce端的任务不超过Threshold值的时候采用类似Hash Based的Shuffle机制，即直接将Map端的文件先分别写入单独的文件，但是它又跟Hash Based不完全相同，它在最后一步还是会将这些文件合并成为一个单独的文件。 举个例子比较好理解，如果说你要从A城市出发去B城市，现有两种选择：打车和坐火车，打车比较灵活适合中短距离，距离太远则不经济，火车价格低廉适合距离长距离。如果A城市和B城市之间的距离大约50公里以内，那么我建议你还是打车比较合理，毕竟打车比较灵活，可以决定自己的时间。而当距离超过100公里，那现在就有必要考虑坐火车了。 Hash Based Shuffle之于Sort Based Shuffle正如打车和 坐火车的关系，Hash Based适合数据量不是特别大的计算任务，此时它会比Sort Based更快；而数据量很大的情况下，Sort Based就更胜一筹，Hash Based会把大量的Map结果写入内存，会相当耗费资源，给GC造成了巨大的压力，得不偿失。下图描述了bypassMergeThreshold运行机制下SortShuffleManager选择类似Hash Based的Shuffle原理（图片来源：Spark性能优化指南——高级篇 - 美团技术团队）。 Spark Shuffle的spark.shuffle.sort.bypassMergeThreshold参数正是为了兼顾Hash Shuffle在小数据集上的优异表现而设置的，spark.shuffle.sort.bypassMergeThreshold参数默认为200，当Map端的任务数量小于200时，此时的Shuffle选择的是Hash Shuffle，也就是先将大量的中间数据文件写入内存并且不排序，只是在最后每个Map task都会把中间的数据文件再汇总为一个数据文件给Reducer，这样一来大大提高运行的效率。 所以，我这里给出的建议是，如果集群的GC压力比较大，并且处理的是需要进行排序的Shuffle操作比如sortBy，可以适当地减小bypassMergeThreshold的值，选择Sort Based Shuffle。 与Spark Shuffle调优相关的参数 spark.shuffle.spill：溢写操作，默认打开，决定当内存不够用时将数据临时写入磁盘 spark.shuffle.memoryFraction：决定了当Shuffle过程中使用的内存达到总内存多少比例的时候开始启动溢写（spill）操作，默认是0.2，也就是说，Executor默认只有20%的内存用来进行该操作。shuffle操作在进行聚合时，如果发现使用的内存超出了这个20%的限制，那么多余的数据就会溢写到磁盘文件中去，此时就会极大地降低性能。建议当内存不够用的时候可以适当地降低这个参数的值，可以避免出现OOM spark.storage.memoryFraction：用于设置RDD能在Executor内存中持久化能占的比例，默认是0.6，当代码中的持久化操作比较多时，可以适当提高该参数的值，反之，当GC频繁导致内存不够用的话，就需要降低该参数，将数据直接写入磁盘 spark.shuffle.spill.compress / spark.shuffle.compress：决定是否对Spill的中间数据，最终的shuffle输出文件进行压缩操作，默认打开 在默认情况下，Spark 会使用 60％的空间来存储 RDD，20% 存储数据混洗操作产生的数据，剩下的 20% 留给用户程序。用户可以自行调节这些选项来追求更好的性能表现。如果用户代码中分配了大量的对象，那么降低 RDD 存储和数据混洗存储所占用的空间可以有效避免程序内存不足的情况。 推荐阅读 Shuffle 相关 — SparkConfig 0.1 documentation SPARK SHUFFLE TUNING – experience@imaginea Apache Spark Developers List - Why is spark.shuffle.sort.bypassMergeThreshold 200? RDD Programming Guide - Spark 2.4.4 Documentation]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>bigdata</tag>
        <tag>spark</tag>
        <tag>mapreduce</tag>
        <tag>shuffle</tag>
        <tag>spark shuffle</tag>
        <tag>hadoop shuffle</tag>
        <tag>sort shuffle</tag>
        <tag>hash shuffle</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[浅析Spark Architecture：Shuffle（一）]]></title>
    <url>%2F2019%2Fspark-architecture-shuffle-01%2F</url>
    <content type="text"><![CDATA[作为一个接触Spark将近一年的数据挖掘工程师，Spark在处理海量数据上游刃有余的表现就强烈的吸引着我，当我在使用Spark完成数据项目、训练模型任务的过程中通常会遇到各种各样的问题，相信这些问题每个Spark的新手都会遇到过，有的时候调高driver memory或者executor memory，又或者稍微改动一下代码然后就可以了，但是却始终对其背后的原理不甚了解，这不符合一个合格工程师的应该有的职业素养。 如何理解Spark Shuffle？在MapReduce中，Shuffle作为连接Map和Reduce的桥梁，它是一个非常占用磁盘I/O和网络传输开销的过程，为什么这么说呢？Shuffle的过程分为Shuffle Write和Shuffle Fetch，其中Write对应的是Map端的写入，Map任务的结果会写入到文件中，而Fetch对应的是Reduce端的读取来自Map端任务的结果数据文件。从而，Shuffle的过程便有了这“一写一读”的操作，所以就不难理解为什么Shuffle为何这么占用磁盘和网络传输了。 如果对于Spark的新手可能不太理解Shuffle的过程，下面我以人口普查统计为例，人口普查统计它肯定不是由国家统计局派出一个团队，然后逐一去各个地区统计，这样统计工作就永远都完不成了。虽然大多数人跟我一样都不是人口统计学专业相关的，但是常识告诉我们，它肯定是国家统计局把各个省的统计局通知到北京来，大家一起开一个会，讨论本次统计工作要完成什么任务，需要统计哪些人口指标，最后各省将各自的统计结果递交给北京汇总。 Spark中所谓的Shuffle就发生在北京汇总（聚合）各省递交统计结果的过程中，具体一点说，比如湖南省和江西省的统计结果中有都有年龄分布和性别分布的统计任务，这两个省份把统计任务完成之后写成统计报告文件，然后国家统计局再一个一个读取省份的统计数据报告，国家统计局会把年龄分布的数据交给一个小组汇总处理，把性别分布的数据交给另外一个数据处理。从这个例子中，各个省份的数据统计工作就对应占用磁盘I/O，大量来自地级市、县级单位、乡镇的统计数据不断“写入”省统计局，而向国家统计局递交数据的过程对应网络传输开销，不同类别的数据需要送到对应的小组汇总处理。在这个例子中，不难理解省份对应Map端，国家统计局对应Reduce端。 Hadoop和Spark在Shuffle上的区别Hadoop的Shuffle过程是Map端的结果首先被写入缓存，当缓存满了之后就启动溢写操作（spilling process），把缓存的数据再写入磁盘，并清空缓存。而Spark 1.01之后，Spark Shuffle的Map任务就不写入缓存了，而是直接写入磁盘文件，最新版本的Spark和老版本的Spark Map端写入磁盘的方式又有所区别。 再者，两者在Map端处理数据的方式也有区别，Hadoop Shuffle的Map端是Sorted -based，会对数据进行排序和合并了之后再写入磁盘文件。Spark Sorted-Based Shuffle在Mapper 端是排序的，包括partition的排序和每个partition内部元素的排序！但在 Reducer 端是没有进行排序，所以Job的结果默认不是排序的。Sorted-Based Shuffle采用了 Tim-Sort排序算法，好处是可以极为高效的使用Mapper端的排序成果全局排序。 新老版本Spark Shuffle运行的区别在Spark 1.2.0之前的Spark Shuffle默认是Hash based，老版本Spark Shuffle的每一个Map任务会根据Reduce任务的数量创建相应的桶（Bucket），Map的结果会写入到这些桶里面，因此桶的数量是m×r，所以Shuffle的过程会产生大量的小文件，导致大量内存消耗和GC 的巨大负担。所以，此时Spark最为紧要的问题是解决Map端生成大量文件的弊端，减轻GC负担。 而新版本（从Spark 1.2.0开始）的Spark Shuffle逐渐抛弃Hash based，拥抱Sort based（默认），它与Hadoop Shuffle思想一致。在Spark2.X版本中只有SortShuffleManager，已经没有了Hash-Based Shuffle Manager 了。它的设计思想则是每个Map任务只创建一个桶，意味着Map任务的结果只写入两个文件，数据文件和索引文件，一个数据文件用于记录Map任务的结果，另外一个索引文件用于记录数据文件中的分区信息。 所以，对比新老版本的Spark Shuffle，老版本的Shuffle所产生的文件数量是新版本的r/2倍$(\frac{mr}{m2})$，新版本的Spark Shuffle为文件系统减少了不少压力。 推荐文章 [Spark性能调优] 第三章 : Spark 2.1.0 中 Sort-Based Shuffle 产生的内幕 - 無情 - 博客园 Spark性能优化指南——基础篇 - 美团技术团队 RDD Programming Guide - Spark 2.3.0 Documentation Apache Spark源码走读之24 -- Sort-based Shuffle的设计与实现 - 徽沪一郎 - 博客园 SparkInternals/4-shuffleDetails.md at master · JerryLead/SparkInternals · GitHub Spark Architecture: Shuffle | Distributed Systems Architecture]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>bigdata</tag>
        <tag>spark</tag>
        <tag>mapreduce</tag>
        <tag>shuffle</tag>
        <tag>spark shuffle</tag>
        <tag>hadoop shuffle</tag>
        <tag>sort shuffle</tag>
        <tag>hash shuffle</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux创建IDEA快捷方式]]></title>
    <url>%2F2019%2Fidea-quick-start%2F</url>
    <content type="text"><![CDATA[先前每次启动idea都要到bin目录下执行./idea.sh脚本，比较麻烦，故直接在桌面创建快捷方式，点击图标便可以直接启动idea。 123456789101112131415# 创建快捷方式touch idea.desktop# 编辑此文件vi idea.desktop# 添加以下内容[Desktop Entry]Name=IntelliJ Ultimate IDEA # 桌面上显示应用的名字Comment=IntelliJ Ultimate IDEA # 鼠标悬浮在图标上时显示的名字Exec=/home/lb/software/idea-IU-192.6817.14/bin/idea.shIcon=/home/lb/software/idea-IU-192.6817.14/bin/idea.pngTerminal=falseType=ApplicationCategories=Developer;]]></content>
      <categories>
        <category>教程</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>idea</tag>
        <tag>快捷启动</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[启动Scala REPL报错：java.lang.NoClassDefFoundError:javax/script/Compilable]]></title>
    <url>%2F2019%2FScala-repl-error%2F</url>
    <content type="text"><![CDATA[启动Scala REPL报错: Exception in thread &quot;main&quot; java.lang.NoClassDefFoundError: javax/script/Compilable 原因是Scala不兼容JDK 10，解决方案是安装JDK 8并将其设置为默认使用，即： 1234$ sudo apt-get install openjdk-8-jdk$ sudo apt-get install openjdk-8-jre$ sudo update-alternatives --config java$ sudo update-alternatives --config javac 选择2所示的JDK 8即可 12345678lb@hw:~$ sudo update-alternatives --config java有 2 个候选项可用于替换 java (提供 /usr/bin/java)。 选择 路径 优先级 状态------------------------------------------------------------* 0 /usr/lib/jvm/java-10-openjdk-amd64/bin/java 1101 自动模式 1 /usr/lib/jvm/java-10-openjdk-amd64/bin/java 1101 手动模式 2 /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java 1081 手动模式]]></content>
      <categories>
        <category>教程</category>
      </categories>
      <tags>
        <tag>scala</tag>
        <tag>java</tag>
        <tag>REPL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[余弦相似度与皮尔逊相关系数之间的比较]]></title>
    <url>%2F2019%2Fdifference-between-cosine-similarity-and-pearson-similarity%2F</url>
    <content type="text"><![CDATA[余弦相似度（也叫作余弦距离）和皮尔逊相关系数是数据挖掘中很常见的两种相似度计算方式，除此之外还有欧式距离、Jaccard距离、曼哈顿距离等计算方法。 本文主要讨论余弦相似度和皮尔逊相关系数之间的区别，首先，这两者返回的结果（相似度、相关系数）都介于0到1之间，并且都是值越大代表越相似，直觉上看起来两种算法之间并没有什么区别，然而实际并非如此，在用户的评分维度差距比较大的场景下，余弦相似度得到的结论可能不太合理，这篇文章的目的就是为了把余弦相似度和皮尔逊相似系数之间的区别解释清楚。 余弦相似度余弦相似度通过计算两个向量的夹角的余弦值来度量它们之间的相似性，在计算文档相似度中应用非常广泛，计算公式如下： cos(a,b)=\frac{a·b}{||a||·||b||}其中，上式中的分子是向量a与向量b的内积，分母是向量a与向量b模的乘积，可以肯定的是分母一定是大于零的，所以余弦相似度的正负性完全由分子决定。 举个例子，比如向量a为(1,2,4)，向量b为(-1,0,2)，那a和b之间的余弦相似度是这样计算的： 首先，计算内积：$1*(-1)+2×0+4×2=7$ 然后，计算向量模的乘积：$\sqrt{(1+4+16)}*\sqrt{(1+0+4)}$ 最后相除得向量a与b的余弦相似度为0.683，说明两个向量比较相似。 调整余弦相似度（修正余弦相似度）然而，余弦相似度也有它的不足，比如在推荐业务中通常基于用户对物品的评分计算用户之间的相似度，它无法把不同用户的打分维度也纳入到相似度计算中来。 比如用户A、B和C对三个物品的评分分别为（1,4,0）、(3,5,1)和(8,9,2)，用余弦相似度计算得到A和B用户之间的相似度为0.874 (1*8+4*9+0*2)/(\sqrt{1^2+4^2+0^2}*\sqrt{8^2+9^2+2^2})然而，从数据上来看A用户对这两个物品的喜好程度都不如B，直觉告诉我这两个用户不太可能有这么相似，因为他们的口味相差实在有点大，正如男生和女生对篮球和运动物品的喜好程度是不一样的，所以用上述的计算方法不太合理。 一般针对评分内容的相似度计算会用到改进后的余弦相似度算法——调整余弦相似度（Adjusted Cosine Similarity），计算公式为： sim(a,b)=\frac{\sum_{i\in{I}}(R_{i,a}-\overline{R}_i)(R_{i,b}-\overline{R}_i)}{\sqrt{\sum_{i\in{I}}(R_{i,a}-\overline{R}_i)^2}\sqrt{\sum_{i\in{I}}(R_{i,b}-\overline{R}_i)^2}}其中$\overline{R}_i$为物品$i$得分的平均值，由上文得物品1的平均得分为4，物品2的平均得分为6，代入上述计算公式： \frac{(1-4)*(8-4)+(4-6)*(9-6)+(0-1)*(2-1)}{\sqrt{(1-4)^2+(4-6)^2+(0-1)^2}*\sqrt{(8-4)^2+(9-6)^2+(2-1)^2}}得调整余弦相似度为-0.996，得出用户A和用户C之间根本就不相似，这与余弦相似度得出的结论完全相反，这是因为A和C对3个商品的偏好差距太大了，余弦相似度没能捕捉到这种差异，调整余弦相似度正好弥补了它的缺陷。 当然，如果你再计算一遍A和B的余弦相似度和调整余弦相似度，你会发现A和B的差异其实不大，从打分的差异上也可以看出来这一点。 从计算过程来看，调整余弦相似度和余弦相似度的计算方法非常相似，只是用户对物品的打分都减去了该物品所有得分的平均值，这一步等价于对数据做了标准化的处理。 另外，把上式中的符号稍微修改一下就能应用于计算物品之间的相似度（把人和物品对调一下），公式如下： sim(i,j)=\frac{\sum_{u\in{U}}(R_{u,i}-\overline{R}_u)(R_{u,j}-\overline{R}_u)}{\sqrt{\sum_{u\in{U}}(R_{u,i}-\overline{R}_u)^2}\sqrt{\sum_{u\in{U}}(R_{u,j}-\overline{R}_u)^2}}其中$\overline{R}_u$为用户$u$打分的平均值。 皮尔逊相关系数sim(a,b)=\frac{\sum_{i\in{I}}(R_{i,a}-\overline{R}_a)(R_{i,b}-\overline{R}_b)}{\sqrt{\sum_{i\in{I}}(R_{i,a}-\overline{R}_a)^2}\sqrt{\sum_{i\in{I}}(R_{i,b}-\overline{R}_b)^2}}其中，$\overline{R}_a$和$\overline{R}_b$代表各自打分的平均值。 接着上文，我们试着计算A与C之间的皮尔逊相似系数，$\overline{R}_a$和$\overline{R}_c$分别为1.67和6.33，代入上个式子中 \frac{(1-1.67)*(8-6.33)+(4-1.67)*(9-6.33)+(0-1.67)*(2-6.33)}{\sqrt{(1-1.67)^2+(4-1.67)^2+(0-1.67)^2}*\sqrt{(8-6.33)^2+(9-6.33)^2+(2-6.33)^2}}计算得到的结果是0.782，说明a，b两个用户之间的皮尔逊相似度为0.782，相比余弦相似度计算得到用户高得离谱的相似度稍显正常一点。 对比皮尔逊相关系数实际上可以看作是一种特殊的余弦相似度计算方式，如果从公式的外形上给三个算法论资排辈的话，皮尔逊相关系数应该是调整余弦相似度的堂兄弟，是余弦相似度的亲儿子。 从上文中举的例子，我们也提到过余弦相似度在面对评分维度差距较大的情形下计算会不太准确，所以建议在用户评分维度差异比较大的场景下优先使用调整余弦相似度和皮尔逊相似度，这一步相当于也是先对数据做了标准化处理。 如何理解皮尔逊相关系数（Pearson Correlation Coefficient）？ - 微调的回答 - 知乎]]></content>
      <categories>
        <category>Machine-Learning</category>
      </categories>
      <tags>
        <tag>统计学</tag>
        <tag>相似度</tag>
        <tag>余弦相似度</tag>
        <tag>皮尔逊相似度</tag>
        <tag>推荐业务</tag>
        <tag>皮尔逊相似系数</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2019阅读记录]]></title>
    <url>%2F2019%2Freading-2019%2F</url>
    <content type="text"><![CDATA[阅读在短期内可能不会给你的生活带来任何改变，但是只要你长期坚持下去，你会逐渐爱上这项脑力运动，它给你的回报也会越来越大。 从去年抛弃朋友圈到今年年初放弃微博，我可以说差不多已经脱离社交网络了，脱离社交网络给我的生活带来最大的改变在于我可以腾出更多的时间在阅读软件上而不是沉迷于微信微博，上半年列出的这些书大多是在通勤的地铁、午间休息和晚上下班回家之后看完的。 我爱阅读，我爱思考。 穷查理宝典 (豆瓣)芒格非常强调跨学科知识的重要性，如果要预测一个指标，仅仅用一个自变量是不够的，它需要综合物理、生物、数学等多个学科的知识，也就是我们需要一种把问题“函数化”的思维，找准你的问题，定义你的自变量，然后找到一个最佳的模型去拟合它。他在书中特别提到了心理学在投资决策中的作用，优秀的投资人必须得懂点心理学，但也不要被心理学误导。芒格在书的下半部分用了大量篇幅黑了好几把经济学，说经济学仅仅是为了自己的结论好看而枉顾其在现实中的表现。他在书中推荐的书籍我也比较感兴趣。 叫魂 (豆瓣)叫魂（偷取别人的灵魂）是发生在1768年的一桩可有可无的民事纠纷案件，但是由于当事人涉嫌在官方的控制之外擅自与神灵发生某种联系，从而牵动了统治者的神经，统治者一方面不愿意看到它的存在，另一方面又不方便放下身段与其公开争夺对神的解释权，否则便是承认了它的存在。由于根本就没有人能够偷取别人的灵魂，所以从一开始这场纠纷就注定没有结局，是乾隆一手将它变成了一场彻头彻尾的闹剧，他发现自己根本没有办法掌控偌大的官僚集团，只有不断地呵斥底下的官员去抓取一个子虚乌有的人，自然是乾隆皇帝和官员们一次又一次被打脸，当时官僚体系的弊端通通暴露了出来。 可以说叫魂是乾隆时代一次对官僚体系的大考吧，考试检验的结果自然是无法直视，对比同时代的西方国家，早就已经在民主的路上走得越来越远，偌大的中国仍然还在纠结于那些威胁统治合法性的力量，差距太大了，其实差距倒不是问题，问题是自己还不知道，要是早一点把这个帝国从睡梦中敲醒就好了！ 乾隆真的好难，下边的官员不对他说真话，他得花费大量的精力辨别官员报告的真假，有什么破解之法？如果说缺乏监督，那会也有都察院，可为什么不能起作用，都流于形式了呢？ 增长黑客 (豆瓣)对于不知道如何做用户增长的小白是一本不错的入门书，肖恩介绍一些浅显的增长概念、帮助增长的手段很有用，看完后值得偶尔去翻一翻。这本书是我年初的时候在地铁上看完的，可以说对于一个想要把自己的水平提升一个档次的公司，这本书是一本不错的参考书，当然，最主要的还是取决于公司的控制人是否有数据驱动、不断尝试的意识。以后，除了技术方面的书籍，产品设计、用户增长的书籍也要多多接触。 显微镜下的大明 (豆瓣)每个小人物都是明帝国的一个红细胞，源源不断地为他输送血液，只是由官僚机构组成的各个器官之间为争夺血液打得不可开交，久而久之，大脑便供血不足，等到大脑极度缺血那一天也是帝国倒下之时。从这本书开始逐渐对明朝的历史有了一点兴趣。 13 67 (豆瓣)在往返的火车上看完了13 67，这是我看过的第一本推理小说，我可以给五分吧！陈浩基以六个案件构成了这本小说，初读可能觉得是相互独立的，然而在时间上实际是串联在一起的，描绘了香港回归前的香港员警和市民之间的景象。我觉得很适合拍成经典的港版警匪片，特别是刚刚看完的最后一章阿七（关振铎）和我（第一节出现的阿棠）合力阻止的一场针对香港警务处长的爆炸事件非常有画面感，还有看到第六章的两位主人公在第一章中时隔多年再次“交手”，不禁暗自赞叹微笑。总之强烈推荐，可能是最开始和最后，我对第一和第六的章节印象最深，火车上网络不好，想到什么再补充。 十六世纪明代中国之财政与税收 (豆瓣)无财政制度可言：十六世纪明代可以说没有财政制度可言，仅仅只有效率低下的管理而已，有明一代甚至没有出现过一个改革派的人物，无论是皇帝还是大臣始终无比忠诚地遵守着明太祖在开国之初定下的那一套强制对全国的财源进行统一的管理制度，这种过分简化的危险毫无疑问无法适应资源、地貌、经济发展水平存在巨大差异的帝国。过度的简化实际上带来的是无穷的混乱，但是对于各种问题，明朝政府总是采取应付的态度，无意根本解决问题，这就会使这些问题蔓延开来。 错误的经济思想：明太祖出身于底层，可能是早年的生活让他对贫困有着深刻的认识，从他的低税赋的经济思想中不难看出他的出发点是抱着对百姓的体恤的，然而实际上很多政策却适得其反，百姓并未因此而减轻负担，反而国家税收的减少导致增加的政府管理成本最终全部转移到了底层身上，事实上明代的税赋并不轻松，可谓越减负担越重，这些现实帝国的拥有者可能也意识到了，但没有发现他们有变革的动力。 其他财政制度、税收政策的细节就不再总结了，毕竟我不是财政专业的研究者，从书中结尾作者罗列的相关资料，黄仁宇在这本书上应该花费了相当多的时间，并且他的大历史观也特别适合混乱的明代财政与税收的话题。 人类简史 (豆瓣)20万年前，智人在东非演化，走出非洲后上演了一幅波澜壮阔的崛起之路，如果说所到之处寸草不生可能有点夸张，但是他们对于自然的改造能力远远超过了自然自身演变的速度。看看下面的这张图谱，智人占领世界的脚步从未停止：4.5万年 智人抵达澳大利亚。澳大利亚巨型动物绝种。3万年 尼安德特人绝种。1.6万年 智人抵达美洲。美洲巨型动物绝种。1.3万年 弗洛里斯人绝种。智人成为唯一存活的人类物种。智人相对其他物种有一个致命的优势，智人具备能够想象即抽象的能力，它使得智人能够在客观之外抽象出主观的概念，后来便有了“我们”和“他们”，相比局限于实体的尼安德特人，智人的想象力对于他们来说简直是降维打击，毫无还手之力，智人在认知革命中取得了胜利。我觉得这是本书最精彩的部分，后期的论证则有些乏力，摘出几个有意思的话题：农业革命：是人类驯化了植物还是植物驯化了人类？科学革命&amp;科学与帝国的联姻：同是航海探险，无论是规模还是技术能力远胜欧洲的明帝国为什么没有给之后的世界发展进程造成同等规模的影响？是因为天朝本来便可自给自足还是囿于道德规范而不方便放下架子去掠夺？又或者是还是沉浸在自己是世界中心的幻想中？都是比较有意思的问题。在看这个部分的时候一度让我有点像在看高中语文阅读题的错觉。后期发力不足，论述似乎有些乏力，蜻蜓点水般的单薄案例无法支撑如此宏大的论题，强行政治正确的论证也很难有说服力，属于看过就会忘的那种书。听说人类简史是赫拉利简史三部曲之一，作者的另外两本简史，我估计也不会看了，三分不能再多了。 南腔北调 (豆瓣)把南腔北调看完了，自己对语言的理解有些改变了，语言不仅是沟通的工具，而且还是文化传承、地域差异的一种表现，从语言中发现历史是一个比较有趣的方法。这本书给3分吧，这类随笔类型的作品我更推荐辉格，比如他写的沐猿而冠更有启发性，也可能跟我不是语言学专业有关吧。 棋王·树王·孩子王 (豆瓣)除了主人公，最佩服的是“脚卵”，这人有股浩然之气，气度不凡。 中国国家治理的制度逻辑 (豆瓣)这是本年度耗费阅读时长最长的一本书。 作者在一开始的时候指出，对于中国这样一个人口、地域超级大的规模国家，治理模式是相当复杂的，我并不认为大规模是一种所谓的借口。中国国家目前的治理逻辑相比之前帝国模式并没有太大的区别，依旧是中央集权、资源向上集中的模式。作者根据自己的田野调查，多次讨论了这种模式的弊端和优势，并指出如果不处理弊端的话，卡里斯马权威将再次会登上历史舞台。这本书给我带来最有收获的观点是有关于国家政府权力的合法性解释；以及运动型治理模式解释为什么经常会有各种各样的活动，比如先前的打老虎、近来的扫黑除恶运动；还有作者关于为什么地方政府偏爱资源密集型项目的解释也很精彩。 摘一：有关中国国家规模及其政治意义诸问题在公共舆论界已多有讨论，但在学术界和政策研究界尚未得到应有的重视。在比较研究的众多讨论中，学者经常引用评价不同国家的治理模式，如新加坡模式、日本模式等等。这些讨论大多忽略了一个重要的维度，即国家治理的规模。例如，新加坡整个国家的领土和人口规模约相当于中国的一个中等城市，韩国的人口仅为江苏省人口的三分之二左右，而日本的岛国特点和民族同质性与中国历史形成的辽阔国土、多元文化中心格局也相去甚远。中国国土面积接近整个欧洲，而人口为其两倍强。 摘二： 从国家的视角来审视当代中国的政治过程，我们的问题是：如此负荷累累的一统体制是如何维系的？从国家治理的种种实践来看，以下两个机制尤为突出：第一，一个严密有序的官僚组织制度贯彻自上而下的行政命令和政策意图，从而确保不同属地与中央政府的步调一致；第二，以认同中央权力为核心的价值观念制度，在政府内部官员和社会文化中建立和强化对中央政权的向心力（金观涛、刘青峰 2011 ，陈旭麓 1991 ）。简言之，维系一统体制的两个核心组织机制，一是官僚制度，二是观念制度。 摘三： 在韦伯的讨论中，“支配是指这样一种情形：支配者所明示的意志（‘命令’）旨在影响他人（被支配者）的行为，且确实对其行为产生了一定社会意义上的影响，即被支配者犹如把命令的内容当作自己行为的准则。从这一关系的被支配者一端来看，这一情形可称为‘顺从’” 在家长（产）制支配形式中，传统权威通过人们对家长或首领权力遵从的传统习俗为其合法性提供了基础。因此，弘扬传统的礼节仪式等制度设施不断强化人们对传统权威的认同，延续了传统权威的合法性基础。在官僚制中，法理权威与正式程序之间有着密切关系，故特别强调维护程序公正、法理面前人人平等的基本原则，以此作为其行使权力的合法性基础。 卡理斯玛权威建立在领袖的超凡禀赋之上，因此，如何不断创造“奇迹”以显示其超凡禀赋和如何保持“追随者”对这一禀赋的认可与服从，成为其合法性基础的关键所在，也诱发了相应的制度安排。 金家政权对应卡理斯玛权威，中国目前对应的是第一种支配形式，以上是韦伯对于三种支配形式的定义 摘四： 官僚体制的常规机制越发达，其组织失败越是凸显，从而更有可能诱发运动型治理机制；而后者的启动有可能造成失控局面，打断社会正常节奏，迫使国家治理回归常规轨道之上。我们也看到，运动型治理机制正面临着危机，越来越不适应现代社会的演变，受到社会多元化趋势的挑战。然而，无论国家治理的意识形态如何选择，无论国家领导人如何更替，面临的组织困难都是一样的。欲从根本上解决运动式治理带来的问题和危机，必须找出新的替代机制来应对、治理官僚体制和常规机制的组织失败。若基本治理逻辑未变，替代机制缺失，则运动型治理机制不废。 摘五：官僚体制的常规机制越发达，其组织失败越是凸显，从而更有可能诱发运动型治理机制；而后者的启动有可能造成失控局面，打断社会正常节奏，迫使国家治理回归常规轨道之上。我们也看到，运动型治理机制正面临着危机，越来越不适应现代社会的演变，受到社会多元化趋势的挑战。然而，无论国家治理的意识形态如何选择，无论国家领导人如何更替，面临的组织困难都是一样的。欲从根本上解决运动式治理带来的问题和危机，必须找出新的替代机制来应对、治理官僚体制和常规机制的组织失败。若基本治理逻辑未变，替代机制缺失，则运动型治理机制不废。 摘六： 2007年我们的研究人员参与和跟踪了中国中部某省D县计划生育部门年终考核，观察了省、市、县三级政府进行的三次年终考核过程。第二个案例故事发生于环境保护政策领域，2008年我们的研究人员在中国北方某省Y市环境保护局参与和跟踪了该地区接受中央政府和省级政府考核检查的过程。每次考核检查持续三至十五天不等，研究人员全程参与考核检查过程，包括考核检查的制度安排、组织实施过程，尤其是这个过程中上下级政府讨价还价、应对考核后果等诸多策略性行为。在调查过程中，田野研究人员参与政府日常工作如帮助工作人员完成统计报表、撰写报告等，有时还参与了部分非正式活动（如私下聚餐等），也参与了当地政府部门应对来自上级政府的考核检查的过程。 组织决策的一统性加剧了执行过程的灵活性：当决策权力以及资源向政府上层机关集中时，自上而下的决策和随之而来的资源分配就更需要依赖漫长的行政链条和基层政府的“灵活执行”实施之，共谋行为的组织基础和制度环境则应运而生。简言之，基层政府共谋行为是中央集权决策过程所付出的代价。 之前我以为权力越向上集中，执行的灵活性则越小，后来发现不是，越严格则越存在更多trade off的空间 刚看完第九章，主要写的是两个村在“村村通”项目中如何举债修路、背负债务和“违规挪用资金”偿还债务的故事。想起来十几年前我们村当时修路的情形，很相似，县政府不会给每个村拨款修路，想修路自己想办法去。当时我们村的书记跟书中的康书记很相似，路子广、有魄力，他把村里两条主干道旁边种植了二十几年的杉树全部卖掉，然后按照每户分担自家门前路段的修路成本（家里有劳动力的出劳动力没有劳动力的出钱）、修路人的伙食和茶水，然后还有一些弄不清楚的资金来源，最后奇迹般地比其他大村把路先修好了。还有另外一个事情是有一年，我们村着遇到洪水，然后他不知道是动什么什么社会关系，居然来了几卡车部队来帮助我们抗洪。在基层当村委书记没点路子是做不成的。不过这个村支书的做法有的人赞同有的人也不赞同。很可惜的是这个书记得癌症死了。 统计学 (豆瓣)贾俊平版本统计学复习笔记 | Thinking Realm 长安十二时辰 上 (豆瓣)马伯庸真的是一位认真的作家，看完《显微镜下的大明》之后还没有这么认为，不过从长安十二时辰让我确信了这一点。先不说小说的架构，单单从马伯庸在书中对于一千多年前的唐朝政治、风俗、军事、建筑娓娓道来这一点就让人佩服。马伯庸承认自己是从美剧反恐24小时中得到了灵感，将本书分为24个章节，一个章节对应一个小时，所以作者必须对时间有很好的掌控能力。张小敬原本是历史中华阴尉姚汝能笔下一个平平常常的人物，然而却在马伯庸的笔下无比放大，成为了一个24小时内拯救长安的英雄，可谓不辱使命。不过毕竟是小说，张小敬在连续24小时精神高度紧张下还能继续格斗、做出判断有点超乎寻常了。小说中的另外一个人物龙波也就是萧规留给我的印象不亚于张小敬，甚至在后半段有些“抢戏”，类似《蝙蝠侠：黑暗骑士》的小丑，锋芒甚至盖过了主角，不过他没有张小敬的那般犹豫，至始至终都不为自己的选择后悔，“每个人都要为自己的选择负责”这句在书中正派人物口中反复吐出的话我觉得放在他身上再确切不过了，张小敬、李泌难道就没有后悔自己的决定吗？再说小说的结构，可能是先前的悬念、打斗细节兜得太大，以至于后程作者似乎也无法掌控，灯楼爆炸的细节、背后大 boss从李相-李亨-安禄山三者之间来回切换，最后却是贺知章身边的义子贺东，然而他在全书的出现不过寥寥数次，读者无论如何也不会想到会是他，读到这里有些诧异又有些遗憾。总的来说，这本书还是值得一读的，至于最近火爆的网剧我看了几集，剧中人物名字全部被改，演员的功力无法驾驭如此丰富的人物，剧情也不是小说那般进行，给人印象深刻的就是服装道具布景，十分用心。 腾讯传 (豆瓣)第一次如此全面地对腾讯崛起的历史有了全面的理解，依旧记得第一次听到QQ提示音的惊喜，第一次打开微信的那种新鲜感。吴晓波的叙事能力一流，上部分腾讯初创时期创始人的各种有趣的故事十分吸引人，但估计大部分不太可考吧。中部分则叙述了腾讯如何在web、移动互联网时代快速崛起的历程，当然我觉得“一个人的命运当然要靠自我奋斗，但是也要考虑历史的进程”，腾讯与周鸿祎、微博、盛大厮杀的过程似乎在一开始就决定了胜负，腾讯拥有巨大的用户基数，翻手为云覆手为雨，它更像是一个军和一个师之间的战争，赢了不值得炫耀，输了才丢人。如果想跟腾讯竞争，最好用其他兵种打击，如今短视频头条系的异军突起就是最好的证明。毕竟是官方委托写的传记，自然敏感的地方无法抒发作者自己的见解。比如3Q大战，难道周鸿祎就有那么不堪吗？（我不是360的用户，360太恶心了，现在也与360彻底无缘），周鸿祎好不容易从移动互联网杀出一条免费杀毒软件的血路，到手的肉眼看就被别人抢走了，求和不成下他才发动与腾讯的战争估计也是无奈之举，“王侯将相，宁有种乎？”，我认为360如海明威笔下的老人虽败犹荣。 邓小平时代 (豆瓣)三起三落、广场事件、改革开放是邓小平的政治生涯最为重要的三段经历，不好好研究就无法读懂邓小平。先前我以为自己在墙外看了一些纪录片和评论就对事件有了比较完整的认识，读了书之后后来发现完全不够，所以以后会更谨慎地去评论。不过正如邓公在事件发生后5天对军区干部的谈话所说的“这也许是一件好事”、“迟早是要来的”，有人说时间会向我们证明一切，如今正好30年过去了，但是我觉得还不够，目前还没有发展到能自信地评价那些历史事件。不过可以肯定的是如果当年没有果断地处理，一场血雨腥风是无法避免的。在微信上架的邓小平时代是删节版，关键处有网友补上了原文，注意。 枪炮、病菌与钢铁 (豆瓣)今年以来看过最佳的一本社科书籍，无论是作者的论证、写作技巧、知识视野都属于一流，强烈推荐。 西班牙人胜利的秘密： 西班牙的新式武器如枪炮、马匹、钢刀对印加帝国的士兵来说具有极强的心里震撼作用 印加帝国缺乏文字这一准确的沟通工具，故无法获得真实的情报 西班牙人带来的流行感冒、天花摧毁了印加帝国绝大多数没有抗体的人 俘虏了阿塔瓦尔帕基本上控制了8万士兵，不得不说这样的中心化的组织设计太脆弱了 从一开始摆出人类起源于非洲的事实，然后叙述人类如何走出非洲，按道理起源越早应该具备更大的先发优势，读者这时便会有疑问，为什么现代的非洲会落后于其他大陆呢？在第二章，作者接着举例毛利人到达新西兰后快速发展的故事，说明非洲大陆领先500万年的的优势在100年面前微不足道，读到这里读者又会有疑问，那同时期的人类为什么又会有差异呢？作者又说起了毛利人和莫里奥里人的故事，这两个人群同时从波利尼西亚群岛走出来，结果却发展成为了实力悬殊的两个群体，前者把后者按在地上摩擦。作者觉得这样还不够信服，又举了162个西班牙人征服印加帝国的故事，听完这个故事你总该被说服了吧？不过读者可能还会有疑问，为什么总是欧洲人具有优势，新大陆的人却总是挨揍呢？作者每论证完一个论点又抛出一个新的问题，让人停不下来，戴蒙德的写作技巧真是赞啊！ 我想看到这里我应该被作者的论证说服了，作者在本章解释了为什么新几内亚、澳大利亚和加利福尼亚原本存在有利于粮食生产的地区却没有出现，这不能怪生活在那里的新几内亚人、澳大利亚人没有作为，他们只是没有新月沃地得天独厚的丰富作物资源、适合植物生长的气候条件、可供驯化的大型哺乳动物。事实证明，粮食作物的驯化也在不久后在这些地区出现，但是太晚了，还没有等到驯化完，来自欧洲的殖民者就已经到来了。 所以，一开始读这本书的时候我很怀疑作者能不能讲好为“种族歧视”辩护这样一个政治正确的话题，看到这里我放心了。 心流 (豆瓣)这是一本值得经常翻看的书。 作者在书中提到的心流的概念，心流指的是我们内心的一种和谐的状态。在大多数情况下，我们的意识并不是完全能受我们大脑的控制，而心流便是一种能够控制你自己的美妙体验。 作者在书中是这么解释的： 第一，注意力。他说：体验过心流的人都知道，那份深沉的快乐是严格的自律、集中注意力换来的。第二，有一个他愿意为之付出的目标。那目标是什么不要紧，只要那目标将他的注意力集中于此。第三，有即时的回馈。第四，因全神贯注于此，日常恼人的琐事被忘却和屏蔽。第五，达到了忘我的状态。 你可以理解为沉浸式的体验，比如你沉迷于阅读一本书，并从里面学到了许多有意思的东西，从而继续想读下一本书，然后继续在这一种体验中获得快乐，再往大的说可以认为是天人合一的状态。跟中国文化里的不以物喜，不以己悲，庄子所言的：臣以神遇而不以目视，官知止而神欲行，有异曲同工之妙。 心流有好坏之分，并不是你完全投入了，然后沉浸在那种体验里面就是一个好的心流。好的心流应该是复杂化和独特化的统一，不然你的目标总是在原地踏步，并且不是基于你的一个自我去设计的，那么它就不是一个好的心流。只有具备了这两个特征，心流才有助于帮你提升生活品质。 心流有高低之分，复杂的自我是高级心流所具备的特征，独特性自我则是高级心流的必然产物。人们可以通过不断实现自己的目标以复杂化自我。 如何去获取心流呢？ 首先，你得要有一个明确的目标，也就是说你要预期自己要达到什么样的状态，然后在状态完成的过程中需要不断地获得及时的反馈，这些反馈能够帮助你坚持到任务和目标顺利完成的那一刻。 在完成目标的过程中，你需要非常专注，也就是努力使自己达到一种忘我的状态，当然这种状态是非常难以拥有的。 在电影功夫熊猫2中有一个场景，阿宝去问他的师傅在做什么，他师傅说我在寻找内心的平静，即inner peace，那么什么是内心的平静呢？Master Shifu是这么回答的： It was the most painful, mind destroying, horrible moment I have ever experienced. But once I realized the problem was not you, but within me. I found Inner Peace, and was able to harness the flow of the universe. 这曾是最痛苦，令人心碎，可怕的时刻，我之前从来没有体验过的。但自从我发现问题并不在于你，而是在于我，我领悟到了心如止水，从而能够利用宇宙之量。 这个回答或许有点禅的意味。 Your mind is like this water, my friend. When it is agitated, it becomes difficult to see. But if you allow it to settle, the answer becomes clear. 只有你不断地专注于某个设计的目标，然后自己制定计划去逐步的实现。它在实现计划的过程中能够有及时的反馈。知道自己要什么，并朝这个方向努力的人，感觉、思想、行动都能配合无间，内心的和谐自然涌现。 这个星期打算再把功夫熊猫2看一遍，其实这部电影的inner peace在我迷茫的时刻曾经让我思考了许久，到底什么才是心如止水。现在我大概明白那时一种合一的状态，你在空间中是一种流畅运行的态势，你能流畅地控制自我以及外物，正如master shifu能让一滴水在其手上自如流动最后无声重回水的怀抱。 豆瓣书评：心流=inner peace（心流）书评 理性乐观派 (豆瓣)持有信心并非是一种浅薄 总结下来，里得利认为交换和专业化的分工是人类经济不断进步的主要原因。 他在这本书里不厌其烦地搜集了大量的材料予以佐证自己为何对未来拥有理性的客观态度，从20万年前到现在，从经济、历史、化学再到地理，学科涉及面之广令人叹服，所以里得利的视野值得肯定。 书中提到 “费马大定理的知识一点也不假，类星体也的确是遥远的星系，可它们恐怕永远也不会增加国内生产总值，不过，钻研这些知识，兴许可以提高某个人的生活质量。” 让我想起为什么古代中国没有兴起像工业革命这般的技术狂潮，而是对于新技术的研究和出现总是保持戒备心理。工业革命的爆发是建立在无数人极小的改进上的，如果没有对技术持有热衷或者仅仅抱有功利心，那么永远都无法占据技术的领先地位。联想到目前的中国男篮，我的看法也是如此。 最后驳斥没有头脑的激进环保主义者，对于大多数人根深蒂固的悲观环境态度是一次沉重的打击，我们的环境并没有想象的那么糟糕，甚至比以前更好了。当然，作者所引用的材料大多没有给出来源，我对可信度持保留态度。 豆瓣书评：持有信心并非是一种浅薄（理性乐观派）书评 事实 (豆瓣)沉默的大多数 (豆瓣)Spark编程基础（Scala版） (豆瓣)无敌舰队 (豆瓣)]]></content>
      <categories>
        <category>思想王国</category>
      </categories>
      <tags>
        <tag>阅读</tag>
        <tag>豆瓣</tag>
        <tag>微信读书</tag>
        <tag>地铁读书</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用Tesseract提取图像中的文字]]></title>
    <url>%2F2019%2FTesseract-ocr%2F</url>
    <content type="text"><![CDATA[针对图像中的标准的印刷体文字，Tesseract或许是一个简单、高效的图片OCR方案选择。 前期准备以Deepin为例，首先安装Tesseract 4.00，python依赖pytesseract和Pillow，并定义好环境变量。 1234567891011121314151617# 1. 安装Tesseract 4.00版本sudo apt-get tesseract-ocr# 2. 安装Tesseract 4.00 中文简体识别模型sudo apt install tesseract-ocr-chi-sim# 查看支持的语言：chi_sim指中文简体、eng指英文字体tesseract --list-langs# 查看目前系统已经安装的字体text2image --fonts_dir /usr/share/fonts --list_available_fonts# 3. 定义环境变量，不然会报错：“Failed loading language &apos;eng&apos; Tesseract couldn&apos;t load any languages!”export TESSDATA_PREFIX=/usr/share/tesseract-ocr/4.00/tessdata/# 4. 安装python库sudo pip install pytesseract# 5. 安装python图片处理库sudo pip install Pillow# 6. 把langdata_lstm和tesseract两个项目clone到本地，这两个项目是为了训练字体模型而准备的，如果仅仅使用默认的模型去识别字体那么不用clone这两个项目也行（项目体积有点大，clone时间会比较长）git clone https://github.com/tesseract-ocr/langdata_lstmgit clone https://github.com/tesseract-ocr/tesseract 生成待训练字体数据集由于中文存在不同的字体形式，如果使用场景中有别的字体，比如隶书，那默认的字体识别模型效果就不会很好，所以有的时候需要根据场景中的字体训练相对应的字体识别模型。以下以生成华文行楷字体的训练数据集为例（前提是机器已经安装好了该字体），然后执行生成训练数据步骤： 123456789~/image_projects/tesseract/src/training/tesstrain.sh \ --fonts_dir /usr/share/fonts \ --lang chi_sim --linedata_only \ --noextract_font_properties \ --langdata_dir ~/image_projects/langdata/langdata_lstm \ --tessdata_dir ~/image_projects/tesseract/tessdata \ --save_box_tiff --maxpages 150 \ --fontlist &quot;STXingkai&quot; \ --output_dir ~/image_projects/model 参数解释： --fonts_dir 字体安装目录 --langdata_dir 上一节中clone下来的langdata_lstm项目 --tessdata_dir 上一节中clone下来的tesseract项目 --fontlist 你需要训练字体的名称 训练12345OMP_THREAD_LIMIT=8 lstmtraining --model_output ~/image_projects/model/STXingkai \ --continue_from ~/image_projects/tesseract/tessdata/chi_sim.lstm \ --traineddata ~/image_projects/tesseract/tessdata/chi_sim.traineddata \ --train_listfile ~/image_projects/model/chi_sim.training_files.txt \ --max_iterations 2000 评估此处的评估结果并不会给出准确率或者相关的评估指标，只会给出原文和识别结果的对比。 123lstmeval --model ~/image_projects/tesseract/tessdata/chi_sim.lstm \--traineddata ~/image_projects/tesseract/tessdata/chi_sim.traineddata \--eval_listfile ~/image_projects/model/chi_sim.training_files.txt 1lstmeval --model STXingkai_checkpoint --traineddata ~/image_projects/tesseract/tessdata/chi_sim.traineddata --eval_listfile chi_sim.training_files.txt 与现有的模型合并1234lstmtraining --stop_training \ --continue_from ~/image_projects/model/STXingkai_checkpoint \ --traineddata ~/image_projects/tesseract/tessdata/chi_sim.traineddata \ --model_output ~/image_projects/model/chi_sim_xingkai.traineddata 现在，查看当前的系统支持的字体模型，已经支持华文行楷chi_sim_xk的识别了。 12# 查看支持的语言：chi_sim指中文简体、eng指英文字体tesseract --list-langs 123456List of available languages (5):osdchi_sim_xkchi_sim_vertengchi_sim 相关链接https://github.com/tesseract-ocr/langdata_lstmhttps://0o0.me/legendary/tesseract-fine-tune.htmlhttps://qianjiye.de/2015/08/tesseract-ocr#outline_0https://iami.xyz/Review-S05-Tesseract-lstm-traning-datafile/https://0o0.me/legendary/tesseract-fine-tune.html]]></content>
      <categories>
        <category>教程</category>
      </categories>
      <tags>
        <tag>Tesseract</tag>
        <tag>ocr</tag>
        <tag>图像文字</tag>
        <tag>pytesseract</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[贾俊平版本统计学复习笔记]]></title>
    <url>%2F2019%2Fstatistics-review-notes%2F</url>
    <content type="text"><![CDATA[前一段时间在工作中发现自己对于统计知识的理解有点不够系统，比如我知道了解数据的形态需要从集中趋势、离散趋势以及形态方面着手，但是对于其中的细节却总是模糊的，比如我无法回答为什么通常用标准差而不是方差来反映数据的离散程度，我也无法回答泊松分布到底是个什么鬼，所以出于非应试的目的，利用差不多一个多月的业余时间，我把贾俊平编写的统计学的前半部分大致的过了一遍，从中选出几点我先前不太明白的地方记录下来，先记录这么多，以后有新的再补上。 参数与统计量参数与统计量的区别：参数是对总体特征的概括性数字度量，因为总体数据通常是不知道的，所以参数一般未知。统计量是描述样本数据的概括性数字度量，因为样本是抽样出来的，所以统计量是已知的，统计量用于估计总体的参数。 数据描述我们一般可以将数据大致分为：数值型、分类型和顺序型数据，顺序型数据就是将数值型数据按照大小排序后的形态，一般从集中趋势、离散趋势和形态度量这三个方面去获得对数据的整体感知。在复习统计学的时候我发现我先前对于数据离散趋势的度量并不是那么准确，比如，对于分类数据来说，有异众比率，顺序数据来说，是四分位差，数值型数据主要是方差和标准差。 其中有一条经验法则挺值得注意的，也就是说在数据对称分布的时候，大多数的数据是在±k个标准差的范围之内，如果某个数据超过了这个范围，那可以把它被认定是离群点。对于不对称分布的数据，则可以根据切比雪夫不等式来知道大多数数据的分布范围。 当我们想要比较两组数据的离散程度，如果均值不同的话是不能直接拿标准差去比较的，因为他们不在同一个量纲下，所以为了消除量纲的差距，我们需要用相对离散程度，即离散系数，标准差与平均数之比 均值与数学期望的区别均值是实验后对统计结果的平均数而数学期望是实验前根据概率分布对实验结果的预测的平均值。因为我们在实验中没办法穷尽所有的可能，所以只能根据概率，分布去预测样本的平均值，即数学期望。 统计量及其抽样分布抽样分布、参数估计和假设检验是统计推断的三个中心内容，在正态分布的总体条件下，统计学下有三大分布：卡方分布、t分布和F分布。这三大分布对后面的参数估计和假设检验至关重要。 还需要记得一条伟大的中心极限定理。中心极限定理的大致意思就是如果我从一个服从正态分布的总体中抽出n个样本，当样本量充分大的时候（经验上一般大于30即可）样本的均值也是服从正态分布。 之前一直不理解泊松分布和指数分布在实际中的应用是如何的，直到看到了阮一峰写得这篇简短明了的文章才恍然大悟。 泊松分布是单位时间内独立事件发生次数的概率分布，指数分布是独立事件的时间间隔的概率分布。 参数检验比较两个群体之间的参数是否有显著差别，比如两个不同行业职工的平均收入水平是否存在差异，一般的逻辑是直接比较均值或者更不合理的是拿一个群体的均值作为代表跟另外一个群体相比较，而从统计学的意义上来说，需要用两个总体参数的检验寻找答案，按照下图属于两个总体参数之差的检验，那么根据样本大小或者方差是否位置分别采用z统计量和t统计量来做检验。]]></content>
      <categories>
        <category>Machine-Learning</category>
      </categories>
      <tags>
        <tag>统计学</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于Spark逻辑回归的流失预测实践]]></title>
    <url>%2F2019%2Fuser-loss-prediction-in-spark-logistic-regression%2F</url>
    <content type="text"><![CDATA[先前的一段时间接手到一个流失用户预测的活，也就是根据某个群体用户的行为数据（动态特征）、自身特征（静态特征），建立一套流失预警的分类模型，预测用户的流失概率。类似于这种机器学习任务，毫无疑问，逻辑回归算法是首选之选。因为在很多场景下的需求问题都可以很容易地转化为一个分类或者预测问题，而逻辑回归的输出值可以无缝地适应这两类问题，如果你想要做分类，那么你控制好门槛值（threshold），如果你想做预测，它输出的概率值很好解释。所以，逻辑回归这个算法在机器学习中有点AK-47的味道：简单易懂、适应场景广泛、容易优化，如果效果不好，尝试增大数据集、增加几个特征、调节参数这些常见优化方法，一般来说，应对离线场景下的需求绰绰有余。这篇文章暂时就离线问题作为讨论的话题，因为我还没有特别熟练的实时机器学习模型部署经验。 对比Python引用Scikit-Learn构建机器学习模型和使用Scala在Spark下搭建机器学习模型，两者也有值得注意的地方，比如Spark在训练的时候更加直观，你只需要把所有的特征全部汇集到features一列下就行，然后再指定一列作为label，到此为止就算是成功了一半。 样本选择样本的选择是模型搭建中很关键的一个环节，因为它直接会关系到你的模型的预测对象是否符合需求，比如这篇文章介绍就比较详细：用户增长分析——用户流失预警 - 云+社区 - 腾讯云。举个例子，如果你要预测平台忠实用户的流失概率，那么你首先要考虑的问题便是如何定义忠实的活跃用户，只有先从用户池划定了一个区域，然后所有的分析、特征选取、训练预测都围绕圈定的用户群体来做，否则你的模型做得再好也是徒劳的。我是从以下几个方面来定义活跃用户的： 过去N天登录次数大于M次 连续活跃的天数大于N次 用户的主要功能事件行为 过滤黑名单用户、新增用户、作弊用户 …… 当然，过滤的条件越多，你的SQL也会越来越复杂，你可能需要把SQL层层嵌套，这个环节需要非常细心。 特征提取做完了上述的工作之后，下一步我们需要做的就是如何选取特征了，这也是相当关键的一个步骤（貌似没有不关键的==），garbage in garbage out，选取特征的好坏直接决定了模型的质量。你需要根据你所在的业务场景设计自己的指标体系，我所在项目是娱乐社交平台，关系用户留存的因素我总结为三大类：社交行为、业务行为和消费行为，下图展示的是我在模型中选取的部分特征，其中第一个部分的业务、社交活跃特征最多，总共有40个，这些特征的计算工作依赖前期数据项目人天表的积累，如果之前的这些数据项目没有做好，临时再去计算这些特征需要花费大量的精力；第二和第三个部分的业务行为特征我主要涉及到用户的消费行为，很容易想到一个在平台经常消费送礼的用户应该是不大可能会流失的，而那些消费行为记录为零的用户则很有可能再也不玩APP了；截图中还有一部分特征没有显示出来，这部分的特征我主要选的是用户的一些反馈行为，比如说举报其他用户或使用APP过程中的反馈行为，这些行为数据跟用户的流失存在着很大的关系。总的来说，特征的选取需要贴合实际的业务场景，先理解了业务场景剩下的工作便是“开脑洞”从各个业务表中寻找特征数据。 在正式进入模型的搭建之前，我们最好在心底确定下一个目标，确保喂给训练器的dataframe是一个用户id对应一行特征数据，并且数据还需要符合没有null值和空值、连续型的特征字段取值是double的条件。 模型搭建与训练为了方便阅读代码，我把模型训练部分的代码全部放在文章的末尾了。 另外还遇到的一个问题是性能，当时我总是遇到内存溢出，后来我发现自己对于业务场景的理解有错误，也就是数据的准备不需要及时化，我的业务场景是离线的，所以我不需要像实时预测的需求那样拿到数据之后立马做出判断，所以这两步连接在一块的话就会造成OOM的错误，所以在离线的场景下，我们可以将离线的预测拆分为数据准备、训练、预测三个阶段，或者是根据你的需要分为更多的步骤，这样能减少内存占用的压力。 SparkML还有一个比较方便的就是它的pipeline，pipeline，顾名思义，即流水线。想象你的原始数据需要进行各种转换操作，代码会变得非常冗余难以阅读，在pipeline下，你至于要把需要的转换操作放在pipeline里面即可，12val pipeline = new Pipeline() .setStages(Array(assembler,stdScaler,featureSelector,labelIndexer,lr)) 比如assembler是把所有的特征数值和向量全部汇总到一列下，stdScaler是对特征数据做标准化处理，featureSelector是特征选择器，labelIndexer则是目标变量转换操作，这样就保证了数据和数据操作的代码不会混合到一块，可以说pipeline真的让数据模型的代码变得很整洁！ 网格搜索为了让模型能够训练出一组更好的参数，网格搜索是必不可少的，在Spark中我们可以通过ParamGridBuilder来构建一个参数网格搜索器。在本文中我选择了特征数量、正则化系数、elasticNetParam三个参数作为优化的对象： 特征数量：虽然在模型中引入了很多的特征，但不是每一个特征都起作用，那些作用不大的特征反而会拖慢运行的速度，所以你可以根据实际的情况设置一组特征数量，比如（10,20,30）就是分别从所有特征中挑出10个、20个、30个最有代表性的 正则化系数：默认是0.0（一般业务场景下数据都比较稀疏，所以正则化系数要尽量取小一点，如0.01，0.001） elasticNetParam：是混合了L1和L2正则化系数的参数，公式如下：$\alpha \left( \lambda |w|_1 \right) + (1-\alpha) \left( \frac{\lambda}{2}|w|_2^2 \right) , \alpha \in [0, 1], \lambda \geq 0$，elasticNetParam指的就是公式中的$\alpha$，它代表了L1和L2正则化之间的权衡，当elasticNetParam越接近于1时，L1正则化发挥的作用就越大，这时就会导致特征的权重很多为零，当elasticNetParam越接近于0，L2发挥的作用就越大，会导致特征的权重变得很小。 Spark官方文档解释：Set the ElasticNet mixing parameter. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty. For alpha in (0,1), the penalty is a combination of L1 and L2. Default is 0.0 which is an L2 penalty. 交叉验证除了正则化手段，机器学习中还可以通过交叉验证来防止模型的过拟合，Spark提供了CrossValidator接口用于将训练和测试集数据切分为K折，比如5折交叉验证，CrossValidator会生成5组（训练集，测试集）数据，4/5用于训练，1/5用于验证，然后CrossValidator会根据5个模型计算得到的一个平均评估指标（average evaluation metric）。值得注意的是，交叉验证是非常耗费时间的，比如网格搜索中有两个参数，一共4×2种组合，再加上K折交叉验证，那么就是4×2×k种模型的组合了。 测试模型模型训练完了之后紧接着我们就需要在测试数据集上测试模型，Spark测试模型的步骤如下，用已有的model去transform测试数据集，然后数据集中会新生成rawPrediction、probablity、prediction三列，分别代表原始预测、概率值、预测结果。比如1.79是原始预测值，经过sigmoid转换之后变成了[0,1]之间的概率值，再根据你自己定义的门槛值来确定预测结果是正或反。通常，测试逻辑回归模型的指标有auc，在Spark中可以用BinaryClasssificationEvaluator获得auc的值。 分类任务除了auc指标，还需要考察查准率和召回率，关于这些指标的解释可以参考我的先前的文章：评估机器学习模型：指标解释 | Thinking Realm。而查准率和召回率则需要根据具体业务场景，比如在本文模型的目标是找到那些流失的用户，所以我更关注的是流失用户的查全率，即模型能把实际数据中的流失用户找出来，而总体的准确率能够达到合格的水平就够了。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217object UserLossPredictionDriver &#123; def main(args: Array[String]): Unit = &#123; /** * 1. 加载数据：1 留存 0 流失 * 2. 数据转换 * 3. 训练模型 */ val properties = new Properties() properties.load(new FileInputStream("./properties/user_loss_prediction.properties")) val taskName = properties.getProperty("task.name") // 模型保存路径 val modelSavePath = properties.getProperty("model.save.path") // 外部特征表字段名和表名 val featureTabComField = properties.getProperty("feature.cols") val featureTab = properties.getProperty("feature.tab") val activeUserInfoTab = properties.getProperty("active.user.info.tab") val numTopFeatures = properties.getProperty("num.top.features") val conf = SparkHiveUtil.InitSparkConf(taskName).set("spark.kryo.registrationRequired","false") val sc = new SparkContext(conf) val sqlContext = SparkHiveUtil.InitHiveContext(sc) val predictDt = properties.getProperty("date.time") val dateFormat = properties.getProperty("date.format") var dtArr = Array[String]() dtArr = &#123; if (null == predictDt || "null".equals(predictDt) || "".equalsIgnoreCase(predictDt.trim)) Array(TimeUtil.getYesterday(dateFormat)) else predictDt.split(",") &#125; // 方案一：读取特征数据// var featureDataFrame: DataFrame = null// for (dt &lt;- dtArr) &#123;// val activeUserDataFrame = DataInit.activeUserDataFrame(sqlContext,properties,dt)// val userDeviceDataFrame = DataInit.userDeviceFeature(sqlContext,dt,properties)// val userSongFeedBackDataFrame = DataInit.userSongFeedBackDataFrame(sqlContext,dt,properties)// val userRechargeDataFrame = DataInit.userRechargeBehavior(sqlContext,dt)// val userSendGiftDataFrame = DataInit.userSendGiftBehavior(sqlContext,dt)//// val midDataFrame = activeUserDataFrame.repartition(activeUserDataFrame("user_id"))// .join(userDeviceDataFrame.repartition(userDeviceDataFrame("user_id")),Seq("user_id"),"left")// .join(userSongFeedBackDataFrame.repartition(userSongFeedBackDataFrame("user_id")),Seq("user_id"),"left")// .join(userRechargeDataFrame.repartition(userRechargeDataFrame("user_id")),Seq("user_id"),"left")// .join(userSendGiftDataFrame.repartition(userSendGiftDataFrame("user_id")),Seq("user_id"),"left")// .na.fill(0)// if ( null == featureDataFrame ) &#123;// featureDataFrame = midDataFrame// &#125; else &#123;// featureDataFrame = featureDataFrame.unionAll(midDataFrame.repartition(midDataFrame("user_id")))// &#125;// &#125; // 方案一：读取特征数据 // 方案二：先将特征数据集写入hive表，再读取hive表中的特征字段数据 val featureCols = properties.getProperty("feature.cols").split(",") val featureDataFrameFromHive = sqlContext .sql("""SELECT * FROM dws_base.user_loss_model_feature WHERE dt IN (""" + dtArr.mkString(",") + """) AND user_id &lt;&gt; 0""") .drop("dt") // 去掉dt字段 .na.fill(0.0) .persist(StorageLevel.MEMORY_AND_DISK_SER) // 方案二：先将特征数据集写入hive表 // 预处理数据集 val trainAndTestData = PreprocessData.preprocessData(featureDataFrameFromHive,properties) // 将特征列汇总至raw_features一列下 val assembler = new VectorAssembler() .setInputCols(featureCols) .setOutputCol("raw_features") // 数据标准化 val stdScaler = new StandardScaler() .setInputCol("raw_features") .setOutputCol("std_features") .setWithStd(true) // 将数据集标准化 .setWithMean(false) // 默认是false，原数据集比较稀疏的时候慎用 // 特征数量选择器 val featureSelector = new ChiSqSelector() .setFeaturesCol("std_features") .setLabelCol("class") .setOutputCol("features") // 目标变量转换 // 这里需要注意label到class的转换时，频次出现较高的label会被标记为0 // 所以会出现特征dataframe中label和class不一致的情况 val labelIndexer = new StringIndexer() .setInputCol("class") .setOutputCol("label") // 初始化LR模型 val maxIter = properties.getProperty("max.iter") val regParam = properties.getProperty("reg.param").split(",").map(f =&gt; f.toDouble) val elasticNetParam = properties.getProperty("elastic.net.param").split(",").map(f =&gt; f.toDouble) val numFeatures = properties.getProperty("num.features").split(",").map(f =&gt; f.toInt) val lr = new LogisticRegression() .setMaxIter(maxIter.toInt) .setWeightCol("classWeightCol") .setLabelCol("label") .setFeaturesCol("features") // pipeline val pipeline = new Pipeline() .setStages(Array(assembler,stdScaler,featureSelector,labelIndexer,lr)) // 训练集和测试集数据 val trainData = trainAndTestData(0).cache() val testData = trainAndTestData(1).cache() featureDataFrameFromHive.unpersist() // 网格搜索：特征数量、正则化系数、elasticNetParam val paramGrid = new ParamGridBuilder() .addGrid(featureSelector.numTopFeatures, numFeatures) .addGrid(lr.regParam, regParam) .addGrid(lr.elasticNetParam, elasticNetParam) .build() // 交叉验证 val cv = new CrossValidator() .setEstimator(pipeline) .setEvaluator(new BinaryClassificationEvaluator()) .setEstimatorParamMaps(paramGrid) .setNumFolds(properties.getProperty("k.folds").toInt) // K折交叉验证 // 提取最优模型的参数 implicit class BestParamMapCrossValidatorModel(cvModel: CrossValidatorModel) &#123; def bestEstimatorParamMap: ParamMap = &#123; cvModel.getEstimatorParamMaps .zip(cvModel.avgMetrics) .maxBy(_._2) ._1 &#125; &#125; // 训练模型 val cvModel = cv.fit(trainData) val pipelineModel = cvModel.bestModel.asInstanceOf[PipelineModel] val lrModel = pipelineModel.stages(4).asInstanceOf[LogisticRegressionModel] // 数字得根据pipeline stage array中模型的位置来决定 val trainingSummary = lrModel.summary // Obtain the objective per iteration. val objectiveHistory = trainingSummary.objectiveHistory println("############################模型在每个迭代的损失值") objectiveHistory.foreach(loss =&gt; println(loss)) // Obtain the metrics useful to judge performance on test data. // We cast the summary to a BinaryLogisticRegressionSummary since the problem is a // binary classification problem. val binarySummary = trainingSummary.asInstanceOf[BinaryLogisticRegressionSummary] // Obtain the receiver-operating characteristic as a dataframe and areaUnderROC. val roc = binarySummary.roc println("#############################ROC曲线") roc.show() println("ROC面积" + binarySummary.areaUnderROC)// println(binarySummary.predictions.show(20)) // Set the model threshold to maximize F-Measure// val fMeasure = binarySummary.fMeasureByThreshold // (threshold, F-Measure) curve// val maxFMeasure = fMeasure.agg(functions.max("F-Measure")).head().getDouble(0)// val bestThreshold = fMeasure// .where(fMeasure("F-Measure") === maxFMeasure)// .select("threshold").head().getDouble(0)// lrModel.setThreshold(bestThreshold) // 保存模型 pipelineModel.write.overwrite.save(modelSavePath) // 测试模型 println("###############################预测结果") val predictions = pipelineModel.transform(testData) predictions.select("user_id","label","rawPrediction","probability","prediction").show(50) val evaluator = new BinaryClassificationEvaluator().setLabelCol("label") val areaUnderROC = evaluator.setMetricName("areaUnderROC").evaluate(predictions) val areaUnderPR = evaluator.setMetricName("areaUnderPR").evaluate(predictions) // 检查模型在测试集上的表现 val lp = predictions.select( "label", "prediction") val countTotal = predictions.count() val correct = lp.filter(lp("label") === lp("prediction")).count() // 预测正确的样本数量 lp.show(200) val ratioCorrect = correct.toDouble / countTotal.toDouble // 1 流失 0 留存 val truePositive = lp.filter(lp("prediction") === 1.0).filter(lp("label") === lp("prediction")).count() // 真流失用户 val falsePositive = lp.filter(lp("prediction") === 1.0).filter(lp("label") !== lp("prediction")).count() // 假流失用户 val trueNegative = lp.filter(lp("prediction") === 0.0).filter(lp("label") === lp("prediction")).count() // 真留存用户 val falseNegative = lp.filter(lp("prediction") === 0.0).filter(lp("label") !== lp("prediction")).count() // 假留存用户 // 真正例率、假正例率 val tpr = truePositive.toDouble / (truePositive + falseNegative) val fpr = falsePositive.toDouble / (falsePositive + trueNegative) // 流失用户查准率 val positivePrecision = truePositive.toDouble / (truePositive + falsePositive) // 流失用户召回率 val positiveRecall = truePositive.toDouble / (truePositive + falseNegative) // 留存用户查准率 val negativeRecall = trueNegative.toDouble / (trueNegative + falsePositive) // 流失用户召回率 val negativePrecision = trueNegative.toDouble / (trueNegative + falseNegative) println(s"预测样本总数: $countTotal") println(s"正确预测样本数量: $correct") println(s"模型准确率: $ratioCorrect") println(s"模型ROC值：$areaUnderROC") println(s"模型PR值：$areaUnderPR") println(s"预测结果中真流失用户个数：$truePositive") println(s"预测结果中假流失用户个数：$falsePositive") println(s"预测结果中真流失用户比例: $tpr") println(s"预测结果中假流失用户比例: $fpr") println(s"流失用户查准率：$positivePrecision") println(s"流失用户召回率：$positiveRecall") println(s"留存用户查准率：$negativePrecision") println(s"留存用户召回率：$negativeRecall") // 查看最优模型的参数 println("打印模型参数：1") println(cvModel.bestEstimatorParamMap) println("打印模型参数：2") println(s"Coefficients: $&#123;lrModel.coefficients&#125; Intercept: $&#123;lrModel.intercept&#125;") &#125;&#125; 在构建模型中遇到的问题正负样本分配权重样本分布不均匀是再正常不过的了，计算正负样本占比，给不平衡的样本分配更多的权重。 machine learning - Dealing with unbalanced datasets in Spark MLlib - Stack Overflow 模型所有系数都为零Empty Coefficients in Logistic regression in spark - Stack Overflow python - all coefficients turn zero in Logistic regression using scikit learn - Stack Overflow 如何打印模型特征的权重？Matching LR Coefficients With Feature Names - Databricks 如何提取交叉验证模型最佳参数？scala - How to extract best parameters from a CrossValidatorModel - Stack Overflow 逻辑回归多分类问题多分类实现方式介绍和在 Spark 上实现多分类逻辑回归 | 码农网 如何在SQL中将所有特征放置在一行？Pivot rows to columns in Hive - Analyticshut 参考资源 Predicting Breast Cancer Using Apache Spark Machine Learning Logistic Regression | MapR Predicting Loan Credit Risk using Apache Spark Machine Learning Random Forests | MapR 用户增长分析——用户流失预警 - 云+社区 - 腾讯云 机器学习算法系列（3）Logistic Regression | Thinking Realm]]></content>
      <categories>
        <category>Machine-Learning</category>
      </categories>
      <tags>
        <tag>logistic regression</tag>
        <tag>spark</tag>
        <tag>machine learning</tag>
        <tag>scala</tag>
        <tag>逻辑回归</tag>
        <tag>流失预测</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[白话解释统计学中的P值]]></title>
    <url>%2F2019%2Fp-value-by-plain-explanation%2F</url>
    <content type="text"><![CDATA[关于P值，统计学教材中的解释都比较拗口，难以理解，我举一个例子大家就明白了。 但什么样的概率才算小呢？著名的英国统计学家费希尔把小概率的标准定为 0.05 ，虽然费希尔并没有对为什么选择 0.05 给出充分的解释，但人们还是沿用了这个标准，把 0.05 或比 0.05 更小的概率看成小概率。 统计学 (豆瓣) 当我们做假设检验的时候，通常是可以通过z统计量或者P值比较。但是，教程中的解释都比较拗口，我举一个例子大家就明白了。 关于P值的定义，就是我们假定原假设为真的前提下，由实际观察到的数据与原假设不一致的概率。 举个例子，我们假定硬币是均匀的，掷一枚普通硬币5次，如果硬币是均匀的（假定原假设为真），连抛5次得到都是正面的概率就是0.5的5次方，即0.03125（实际观察到的数据），这就是我们所说的P值，即发生这种事件（5次得到都是正面）的概率为0.03125。 使用P值决策的时候，我们会去拿一个观察到的事件发生概率（P值）与0.05做比较，如果这如果这个值比0.05还要小，那么说明，几乎不可能发生的事情，现在居然就发生了，所以我们就有理由拒绝原假设，不相信它是真的。 P值的长处是它反映了观察到的实际数据与原假设之间不一致的概率值，与传统的拒绝域范围相比，P是一个具体的值，这样就提供了更多的信息。如果事先确定了显著性水平，如α＝0.05，则在双侧检验中，P＞0.025（α/2＝0.025）不能拒绝原假设；反之，P＜0.025则拒绝原假设。在单侧检验中，P＞0.05不能拒绝原假设，P＜0.05则拒绝原假设。当然，也可以直接使用P值进行决策，这时P值本身就代表了显著性水平。我们也可以使用P值，按照我们所需要的显著性水平进行判断和决策，具体做法就是用P值和需要的显著性水平进行比较。 统计学 (豆瓣)]]></content>
      <categories>
        <category>教程</category>
      </categories>
      <tags>
        <tag>p值</tag>
        <tag>假设检验</tag>
        <tag>p-value</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[再见，2018]]></title>
    <url>%2F2019%2Fgoodbye-2018%2F</url>
    <content type="text"><![CDATA[其实，年前的时候就计划着写一篇2018年的年终总结和另外一篇技术文章，奈何过年前的一段时间工作太忙，有的事情当时觉得不太方便放在年终总结里，再加上懒癌发作，后来就不了了之。正好清明节有点时间，腾出一个下午来补上之前的年终总结。 到目前为止，2019年到现在也仅仅发布了一则关于SQL优化的博文，输出量有点低，一来上班后就没有太多时间整理，总是想着把事情全部干完，事实上呢事情哪有做得完的，所以老是比较忙；二来总是觉得自己功底不够，零零散散的材料写得不够好，免得误导了别人，倒不如多累积一段时间，发布一些质量比较高的文章。 2018年对于我来说，相当于是人生一个阶段的终点，当年苦下决心二战也要考上研究生，从当时在哈尔滨的失意，到现在到帝都搬砖，一晃就四年了。当时完全出于对计算机感兴趣，并不知道这几年AI、机器学习会如此火爆，到现在来看，那是的坚持应该是正确的，现在仍然喜欢着自己所从事的工作。庆幸有那么段空闲的时间，否则我没有机会思考自己的出路。路还是要靠人走出来的。毕业那会正谋划着写一篇自己一路如何走过来的经历，因为只有自己走过之后才能有所体会，想着也许能给别人带来一些有益的启示，毕竟现在的人都很浮躁。前几天一个学弟用qq来咨询我专业相关的事情，开口就问毕业之后能挣多少钱，工作后要不要加班，我很难回答，有好几个学弟就咨询过我，后来都不了了之。 在我身边，这一年发生了不少事情，有好的也有坏的。梳理今年的几个节点：年初到北京找工作（初次体验北京的冬天，冷）、修改毕业论文（连续熬夜一个星期）、一次小型车祸（还好，人没事）、顺利通过毕业答辩、步入正式工作环境、前所未有的工作压力、自如空气质量房间（运气不好，造成短期身体不适）、转机。不过，起起落落，浮浮沉沉，不过都是过眼云烟，不存在对不起谁，问心无愧就好。 如果要给我的2018年选定一个关键词的话，我大概会选择：怀疑，因为有的事情颠覆了我之前的认知。 出师不利年初的那会到北京找工作，当时未花太多的时间就拿到了几个offer，从中间选择了一个我当时状态下最理想的一个。鉴于我的出身，以及那时的能力水平，当时觉得已经差不多了，做往上可能空间也不是特别大，后来就没有再继续下工夫去找了，开始在学校一心一意修改论文，这段时间也差不多把之前学习的东西都丢了，给后来的事件也造成了一定的影响。这份offer给当时的我带来了蜜汁自信（现在一想真是可笑），自我感觉良好，我想我这个水平放在我们学院同级的毕业生里面应该算很牛的吧，另外又想到我是从文科转向的cs，更加觉得自己了不得，似乎未来尽在我的掌控之中（哈哈），可是，天堂与地狱，谁能知道呢？ 先前，我对自己的心态调整能力还是比较满意的，根据往常的经验，一般来说，再难的槛几天之后就会自然消散，后来才发现那是没有遇到真正的压力，还是太嫩了。我写下这段文字并不是为了去辩诉什么亦或者倒苦水给大家喝，更不会去诋毁他人，因为本来就不是统一路人，就权当做一个记录吧。 还记得从长沙买了一张站票，站了一个晚上才到北京，在第一个早上，陪伴了我三年的iphone6就在火车上宣布报废，一半面积的屏幕损坏，然后便拖着箱子去找房子。我曾经尝试着几次手动让它重新回到原来的状态，但都好景不长，这次彻底的报废正好让我死了折腾的这条心。 作为一个南方人，体验过让你无处躲藏的南方湿热后，心底自然而然地会像萌生像东北人炫耀他们那的天寒地冻的“优越感”，“你这20几度算什么热啊，去南方试试？”。然而，现实是，北京的夏天真的有点热，要不然清政府的统治者也不会花个几十年建造一座行宫用于避暑。这个城市如此傲娇，你爱来不来，爱走不走，她就在那里，我不管你这个社区住着几十万人，每天早上大约有一万人通勤，抱歉，入口只有这么大，等候区的酷热受不了我也没有办法（其实可以弄一个落地扇）。 一直以来，我非常痛恨那些抱怨生活的人，在我的世界观里，没有什么是忍受不了的，如果忍受不了，那就，那就再忍忍吧。偶然一次深夜下班打车回家，我骤然发现自己变成曾经讨厌的那个人了，我每天会向快车司机咒骂目前的处境、抱怨决策者的强权、宣泄工作上的压力，很庆幸再北京遇到的快车司机大多都是比较好的倾听者，他们总安慰我刚走进社会的还需要适应适应。 到现在我还记得某个休息日打车来公司加班的下午，当时北京的温度已经没有七月那般变态，快车司机跟我说他没有什么文化，说话比较朴素，当附近有附近高等学府的学子打车时他明显会感受到隔阂，我说应该不是你想的那样，他说可能是吧。然后，我照例抱怨了一下当前的处境，然后接下来的情景似好莱坞剧情般的演绎，他说起了他的故事，又谈到现在的处境，之后再分析我的处境，最后得出都会好的结论，当时的一番“演讲”让我浑身充满了能量。订单结束后，我给了他五星好评，还写了一段好评。 一切如他所料，之后确实是变好了，不过来得有点迟。 未曾预料到的痛苦正式的工作环境很nice，牛人很多，我所在的那个片区应该只有我一个双非学校的吧，也见识了那种写代码超级厉害的神人，跟我预想中的差不多。 我可以肯定地说，我肯定没有一般心理学上说的“自我归因偏差”，因为一般来说无论我碰到了什么质疑，首先我都会从自身开始寻找原因。这给当时的我带来了很大的痛苦，让我一度怀疑人生，怀疑自己。我不太确定当时的精神状态是否跟房子的空气质量有关，亦或是与工作压力共同作用的结果，睡眠质量差、发烧、脱发、耳鸣困扰了我很长时间，那段时间我甚至发现自己的听力下降了。一开始我一度怀疑自己不是干这行的料，几度濒临崩溃，越做越差。无论我怎么做似乎在别人眼里都是bullshit，都会受到嘲讽，我把原因都归结到了自己身上，我板上钉钉地包揽了所有的过错。后来，我不过才明白那早就是一艘即将沉没的船，这艘还未经历风暴考验的船处女航便被要求远渡重洋，沉没只是迟早的事，船员想要自保是最正当不过的了。也算是年轻时交的学费吧，It’s not my fault!!!从那以后，我便心里时时刻刻告诫自己以后绝对不会成为他们那样的人，出来混，迟早要还的。 不过后来还好，正如快车司机所憧憬的：“一切都会好的”，对，一切都会好的，如果还没好，那就再耐心等等吧。 因祸得福现在想想，庆幸当时没继续在那个地方，否则我会死的。我现在的工作的地方虽然不大，但是至少有成长的空间，领导和同事们都很包容。现在偶尔会写一些Python，目前主要的开发语言是Scala，接触大数据。接触Scala后我发现了一片新天地，原来Python束手无策的海量数据在Scala面前可以随意所欲地处理，Apache下的开源组件种类如此丰富，几乎覆盖了数据业务的方方面面。刚开始接触Scala觉得这门语言很奇怪，特别是箭头和占位符的用法，另外，Scala给人的感觉太灵活了，因为你会发现要实现一个功能，可以有很多种写法，这也可以，那也可以。写了几个SparkSQL任务后，发现上手也挺快的，再到后来就开始结合Spark做一些机器学习相关的项目了（小声说一句：Spark的官方文档真的很糟糕）。 以前我不相信运气一说，后来我相信了，努力不过是入场券，运气才决定了你能否留下来。很多时候你缺少的是那个小小的机会，举个例子，如果没有接触到实际的业务，一般来说练习sql的机会比较少，所以刚开始我觉得SQL很抽象，竟然不区分大小写，对格式也不敏感，所以觉得巨难掌握。由于缺少锻炼的机会，导致SQL都不怎么会写，其实吧，只要连续写上四五天的SQL就能达到熟练的水平了。Don’t ever let somebody tell you… You can’t do something，这话听着可能有些鸡汤，然而事实就是如此，不要因为别人随便一说就轻易地否定自己，说不定换个思路就好了，换个环境就好了，保持大脑极度开放，极度透明。 我在这里写了这么多东西并不是想控诉谁或者发牢骚，而是单纯地想记录下我的一点感受，人与人之间观念的差异是巨大的，你不可能让所有人都如你的意，那些走不到一条路上的人就各自安好吧。 变化可有可无的网络社交圈子 1 不要从朋友圈去猜测别人生活，也不要去羡慕或者获得优越感，因为那都是假的，你看到的是别人愿意让你看到的2 自己也不要去朋友圈发生活照以获得认同，来自朋友圈的认同没有任何意义，冷暖自知3 这是什么狗屎时代 —— 知乎网友 境界是什么，境界就是能看到平时看不到的东西，听到平时听不到的声音。而要达到这种境界，首先需要提高对周围事物的敏感性，用心去感受，用眼睛去观察，用皮肤去体会触感。很可惜，有了社交软件，现在很少有机会能达到这种沉浸式的境界了，就是那种顺其自然的心流。音乐能快速地让人达到这种状态，我想那些花了几十万去买音响的人大概也是出于这个目的吧，他们想听听一些平时听不到的声音。大多数人没有那个条件，我想比较简单的办法是远离焦躁的社交圈子，抛弃一切扰乱你心智的东西，多跟自己对话，多听听别人的声音。 如果要说过去一年多一个比较大的变化的话，脱离无聊的社交圈子算是一个吧，大概是从去年的三月底开始我关闭了微信朋友圈，除了12月份在朋友圈发了一张图片，我大概有一年多时间没有看过朋友圈或者发布过其他消息了。朋友圈已经变成了爱慕虚荣者的竞技场，滋生嫉妒感的温床，满足窥视欲的工具，真没意思。原来自己也没有想象中的那么重要，除了亲人和几个好友，其他人的生活变化对我也没有太大的影响。关闭朋友圈带来的改变可能就是没有先前那么焦虑了吧，毕竟看到别人天南地北地耍而自己在苦苦挣扎，又或者做不到发自心底为别人的现状而高兴，倒不如什么也不看，落个清净，爱咋咋地。 我很高兴地宣布，自从限制了在这两个社交软件上投入的时间，即使工作再忙，我每个星期也能腾出八九个小时额外时间来看些杂七杂八的书，历史、投资、小说。我想，再过一点时间可能我连微信都会放弃，电报就是一个比较好的软件，活跃、友善、注重隐私（至少我加入的几个群组都是）。说到这里，还必须提到前一阵被销号的微博帐号，由于一时手贱，发布了一张调戏审查机制的图片，最终导致陪伴我近8年的帐号彻底死亡。令我愤怒的是之前写过的一些文章也全部被删除了，再也找不到了！后来一想，算了吧，反正也没有会去看。其实我是很高兴的，因为我之前数次戒掉微博失败，我再也不用在这个平台上浪费时间了，我不用为了博得别人关注而斟字酌句，又或者花时间写那些根本不会有人阅读的文章，又或者在这个平台上缺少友好、有帮助的交流气氛。如果说新浪还有那么点作用的话，图床算不算？我将把更多的时间花在Kindle和微信读书上。 远离焦虑早在年初的时候，就有一篇贩卖焦虑的爆款文章《摩拜创始人套现15亿背后，你的同龄人，正在抛弃你》掀起一番波澜，我也写过关于焦虑的文章，不过由于帐号被封就看不到了。其实，多数人焦虑的原因是希望期待得到获取本阶段能力无法匹配的回报，也不是没有办法获得那些回报，自然得要拿出某些东西去换取，比如尊严、欺骗带来的信用破产、超常的付出、常人难以忍受的压力、运气等等，我称之为“献祭”，别无他法。 通常带着丛林法则来看待这个群体的有两类人，一类是与生俱来地拥有着普通人难以企及的资源，比如很多从事 IT 的朋友或许在大学之前都没有正儿八经摸过计算机，或者教育资源等等，这类人群把现阶段的优于平均水平的现状归结于自身的努力而非出生时的运气，他们认为只要努力也能轻而易举地达到他一样的水平，而忽略了运气的成分，他们不知道努力仅仅是步入更高阶段的入场券，开局一条狗和爆神装不是同一个概念。另一类人群则本身没什么资源，通过各种“献祭”手段达到超出平均水平一点点的位置，他们清楚什么才是真正的“努力”，清楚吃得苦中苦，方为人上人到底意味着什么，因为他们真的走过这条路，知道这条路很难走。说到底还是与自己的斗争，突破了，就赢了，没有超过临界值，就老老实实在原来的圈子待着。 考虑到两类人不同的背景，从他们看待问题的角度，或嘲讽、或蔑视都是可以理解的，毕竟立场不同。你所要做的，只有耐心等待，让自己每天进步一点点，哪怕就是一点点，认真地对待手头的每一件事情，该来的，总会来。 人的一生会长大三次。第一次是在发现自己不是世界中心的时候。第二次是在发现即使再怎么努力，终究还是有些事令人无能为力的时候。第三次是在明知道有些事可能会无能为力，但还是会尽力争取的时候。 —— 出处未知 后记此时，2019年已经过去了三分之一了，想要写的文章很多，然而总是觉得无从下笔，想要看得书依然很多，我尽量抽时间在看，想要学的东西更多，一点一点来吧。我写这些东西呢，一方面是记录自己，另外呢就是分析给读者一些经验，可能有点说教的感觉。但是我觉得吧，当时这些道理都没人跟我讲过，我很清楚这条路是怎么走过来的，所以非常能够理解有些人的处境，如果能给他们带去一些有用的东西，那就再好不过了。]]></content>
      <categories>
        <category>思想王国</category>
      </categories>
      <tags>
        <tag>总结</tag>
        <tag>年终总结</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SQL性能优化实践]]></title>
    <url>%2F2019%2Fsql-optimization-practice%2F</url>
    <content type="text"><![CDATA[记录一次优化SQL查询的经历。 需求：统计历史某个时间段内每个新增设备在未来30天中的活跃情况 问题：原SQL执行特别慢，原因在于通过device_id去关联dws_base.device_info_day的方式效率不高，它会为每个device_id去dws_base.device_info_day执行一次查询，每次的查询时间区间为自注册日first_day后的30天，所以导致查询时间特别长 12345678910111213141516171819202122-- 原SQLSET mapreduce.job.queuename=root.hive;select a.dt,count(distinct a.device_id)new_device_nums,count( case when b.device_id is null then a.device_id end)liushi_device_numsfrom (select device_id,from_unixtime(unix_timestamp(first_day),'yyyyMMdd')dt from dws_base.device_info_all where first_day&gt;="2018-12-17 00:00:00"and first_day&lt;="2018-12-23 00:00:00")a left join(select distinct b.device_id,from_unixtime(unix_timestamp(first_day),'yyyyMMdd')dt from dws_base.device_info_all a left join dws_base.device_info_day b on a.device_id = b.device_id where first_day&gt;="2018-12-17 00:00:00"and first_day&lt;="2018-12-23 00:00:00"and b.dt &gt;= from_unixtime(unix_timestamp(a.first_day)+1*24*60*60,'yyyyMMdd')and b.dt &lt;= from_unixtime(unix_timestamp(a.first_day)+30*24*60*60,'yyyyMMdd') )bon a.device_id=b.device_idand a.dt=b.dtgroup by a.dtorder by a.dt; 思路：device_info_all（记为a）表中有设备id和设备首次登陆时间，dws_base.device_info_day（记为b）表中有设备id和只记录其当天的登陆的一条记录。 要统计a表中新增设备的登陆情况，则需要通过关联b表才能出来，但这里有一个问题，a表中设备的登陆记录可能不止一条，所以设备登陆日距离首次登陆日的日期差值date_diff会有多个，date_diff如何计算呢？如果按照之前的SQL，逻辑上是正确的，但是执行起来会消耗大量的资源，因为它会为每个设备id去b表中执行一次子查询，最终会导致内存溢出而执行不成功。所以我想了一个比较笨的办法，举个例子，如果我们要考虑&quot;2018-12-17 00:00:00&quot;至&quot;2018-12-23 00:00:00&quot;之间新注册登录的设备在在未来30天的活跃情况，12(SELECT distinct device_id, from_unixtime(unix_timestamp(first_day),'yyyyMMdd') dt FROM dws_base.device_info_all where first_day&gt;="2018-12-17 00:00:00" and first_day&lt;="2018-12-23 00:00:00" and device_id IS NOT NULL AND device_id &lt;&gt; "") as a 那么我们仅仅需要在b表中把设备登陆日志的时间窗口限制在&quot;2018-12-18 00:00:00&quot;至&quot;2018-01-22 00:00:00&quot;之间即可，123(select * FROM dws_base.device_info_day where dt &gt;= from_unixtime(unix_timestamp("2018-12-17 00:00:00")+1*24*60*60,'yyyyMMdd') and dt &lt;= from_unixtime(unix_timestamp("2018-12-23 00:00:00")+30*24*60*60,'yyyyMMdd')) b 然后计算a表中设备id首次登陆日期与b表中该id在未来30多天登陆记录日期的日期差date_diff。这样，便保证了&quot;2018-12-17 00:00:00&quot;至&quot;2018-12-23 00:00:00&quot;之间的设备完整地被限制在30天的窗口内，虽然除了23号，其他天数的考察窗口都超过了30天，但我们可以通过date_diff参数来区分，留存和流失用户。 最后我们拿到的数据有4列，分别是设备id（device_id）、设备首次登陆日期（first_day）、设备日常登陆日期（login_day）、设备登陆时距离首次登陆的时间（date_diff）到底当设备的date_diff符合什么样的条件才能被判定为活跃呢？总结了一下，大体分为4种情形： 设备自首次登陆之后就再也没有登陆过，那它的date_diff就都为0，所以该设备被判定为流失 设备自首次登陆之后的30天内没有登陆记录，但是30天后有登陆记录，此时的date_diff{31,35,38...}都大于30，但该设备判定为流失 设备自首次登陆之后的30天内有登陆记录，并且30天后也有登陆记录，此时的date_diff{0,5,16,31...}有大于30的有小于30的，该设备判定为留存 设备自首次登陆之后的30天内有登陆记录，并且30天后也有登陆记录，此时的date_diff{5,16,31...}有大于30的有小于30的，该设备判定为留存 现在的任务就是如何写出一个条件准确地筛选出流失用户和留存用户，一开始我的设置的条件是min(date_diff)&lt;=30 and min(date_diff)&gt;0，也就是只要当date_diff的最小值小于等于30且大于0就判定为留存用户，但是SQL执行后的结果却很奇怪，只有时间窗口的第一天（2018-12-17）数据是正常的，其他的日期则留存设备数量明显偏少，很不符合实际情况。后来才发现，如果是2018-12-18，因为设备登陆日志的窗口在&quot;2018-12-18 00:00:00&quot;至&quot;2018-01-22 00:00:00&quot;，那么此时的date_diff会有等于0的情形，而我们的min(date_diff)&gt;0把这部分数据都过滤掉了（对应情形3），所以导致数据不正常。然后我把过滤条件修改为min(date_diff)&lt;=30 and avg(date_diff)&gt;0，这样就完美地把4种情形区分开来了。​1234567891011121314151617181920212223-- 优化后的sqlSET mapreduce.job.queuename=root.hive;select d.first_day dt,count(distinct d.device_id)new_device_nums,sum(label)liucun_device_numsFROM (select c.device_id,c.first_day, case when (min(date_diff)&lt;=30 and avg(date_diff)&gt;0) then 1 -- 防止过滤掉date_diff为0的数据-- else date_diff=0 and min(date_diff)&gt;30 then 0 else 0 end as labelFROM (select a.device_id,a.dt first_day,b.dt login_day,datediff(from_unixtime(unix_timestamp(b.dt,'yyyyMMdd'),'yyyy-MM-dd'),from_unixtime(unix_timestamp(a.dt,'yyyyMMdd'),'yyyy-MM-dd')) as date_diffFROM (SELECT distinct device_id, from_unixtime(unix_timestamp(first_day),'yyyyMMdd') dt FROM dws_base.device_info_all where first_day&gt;="2018-12-17 00:00:00" and first_day&lt;="2018-12-23 00:00:00" and device_id IS NOT NULL AND device_id &lt;&gt; "") as aleft join(select * FROM dws_base.device_info_day where dt &gt;= from_unixtime(unix_timestamp("2018-12-17 00:00:00")+1*24*60*60,'yyyyMMdd') and dt &lt;= from_unixtime(unix_timestamp("2018-12-23 00:00:00")+30*24*60*60,'yyyyMMdd')) b on a.device_id=b.device_id) as c group by c.device_id,c.first_day) as dgroup by d.first_dayorder by d.first_day;]]></content>
      <categories>
        <category>Database</category>
      </categories>
      <tags>
        <tag>sql</tag>
        <tag>hive</tag>
        <tag>大数据</tag>
        <tag>bigdata</tag>
        <tag>性能优化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[查找算法（一）：查找]]></title>
    <url>%2F2018%2Flinear-search%2F</url>
    <content type="text"><![CDATA[基于有序列表的查找。 线性查找12345def search(arr, target): for i in range(len(arr)): if arr[i] == target: return i return -1 二分查找二分查找（也称为折半查找），在给定一个有序数组的前提下，取中间的元素（arr[mid]）作为比较的对象，如果target值大于中间元素，则在中间元素的右边继续查找，反之，则在中间元素的左边继续查找。 1234567891011121314def binary_search(arr, target): lo = 0 hi = len(arr)-1 while lo &lt;= hi: mid = (lo + hi) // 2 if arr[mid] == target: return mid elif arr[mid] &gt; target: hi = mid - 1 else: lo = mid + 1 return -1 斐波那契数列查找斐波那契数列公式： F(n) = F(n-1) + F(n-2)斐波那契查找步骤如下： 第一步：从斐波那契数列中找到第一个大于或等于数组长度（n）的数（fibM） 第二步：当数组中还有未遍历的元素时： 1. 将游标cur定义为fib2覆盖部分的最后一个元素 2. 比较目标值与arr[cur]进行比较，如果匹配，立即返回索引，完成查找 3. 如果目标值大于arr[cur]，则把斐波那契数列下降一个单位 4. 如果目标值小于arr[cur]，则把斐波那契数列下降两个单位 第三步：如果仅余一个元素用于比较，检验fib1是否等于1，如果是，判断该元素是否为target 1234567891011121314151617181920212223242526272829303132333435363738394041424344def fibMonaccianSearch(arr, target, n): # f(n) = f(n-1) + f(n-2) # fib2 + fib1 = fibM # fib2 --&gt; fib1 --&gt; fibM fib2 = 0 fib1 = 1 fibM = fib2 + fib1 while fibM &lt; n: # 从斐波那契数列中找到*第一个*大于或等于数组长度（n）的数（fibM） fib2 = fib1 fib1 = fibM fibM = fib2 + fib1 offset = -1 while fibM &gt; 1: # 当fibM等于1，fib2等于0，后面没有元素了 cur = min(offset+fib2, n-1) # cur相当于二分查找中的mid if arr[cur] &lt; target: # target大于arr[cur]，转向cur的右边 fibM = fib1 # 斐波那契下降一个单位 fib1 = fib2 fib2 = fibM - fib1 offset = cur # cur的右边需要把cur之前的部分加到fib2上 elif arr[cur] &gt; target: # target小于arr[cur]，转向cur的左边 fibM = fib2 # 斐波那契下降两个单位 fib1 = fib1 - fib2 fib2 = fibM - fib1 else: return cur if fib1 == 1 and arr[offset+1] == target: # 如果仅余一个元素用于比较，检验fib1是否等于1，如果是，判断该元素是否为target return offset+1 # element not found. return -1 return -1 # Driver Code arr = [10, 22, 35, 40, 45, 50, 80, 82, 85, 90, 100] n = len(arr) target = 10print("Found at index:", fibMonaccianSearch(arr, target, n))]]></content>
      <categories>
        <category>algorithm</category>
      </categories>
      <tags>
        <tag>面试</tag>
        <tag>数据结构与算法</tag>
        <tag>二分查找</tag>
        <tag>线性查找</tag>
        <tag>斐波那契查找</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[字符串字符统计]]></title>
    <url>%2F2018%2Fcount-the-number-occurrences-of-a-character-in-a-string%2F</url>
    <content type="text"><![CDATA[给定一个字符串，需要统计字符串中每个字符出现的次数，并按照出现频次从高到低排列。 方法一： 123456&gt;&gt;&gt; s = 'hello, world'&gt;&gt;&gt; a = &#123;k:s.count(k) for k in set(s.replace(' ',''))&#125;&gt;&gt;&gt; a&#123;'l': 3, 'd': 1, ',': 1, 'o': 2, 'e': 1, 'w': 1, 'h': 1, 'r': 1&#125;&gt;&gt;&gt; sorted(a.items(),key=lambda d: d[1], reverse=True) # 按照 value 排序[('l', 3), ('o', 2), ('d', 1), ('r', 1), ('w', 1), ('h', 1), ('e', 1)] 方法二： 12345a = &#123;&#125;for i in string: if i not in a and i.isalpha(): a[i] = string.count(i)sorted(a.items(),key=lambda d: d[1], reverse=True) 1[(&apos;l&apos;, 3), (&apos;o&apos;, 2), (&apos;d&apos;, 1), (&apos;r&apos;, 1), (&apos;e&apos;, 1), (&apos;h&apos;, 1), (&apos;w&apos;, 1)]]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>字符串统计</tag>
        <tag>字符串</tag>
        <tag>统计</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Top K 问题]]></title>
    <url>%2F2018%2Ffind-K-largest%2F</url>
    <content type="text"><![CDATA[一步解决数组的Top K有点困难，不妨逐步解决，先从简单的开始，比如只要找到数组中的最大的一个数可以这样实现： 方法一： 12345678910def findLargest(arr, arr_size): if not arr: return -1 largest = -float('inf') for i in range(arr_size): if arr[i] &gt; largest: largest = arr[i] return largest 增大难度，如果需要找到数组中最大的三个数，那又如何实现呢？ 方法二： 123456789101112131415161718192021def find3Largest(arr, arr_size): if arr_size &lt; 3: print('无效的输入') return -1 first = second = third = -float('inf') for i in range(arr_size): if arr[i] &gt; first: # 找出数组中的最大值 third = second # 一旦数组中有新的最大值出现，那么原来*第二大*的就变成*第三大* second = first # *第一大*的变成*第二大* first = arr[i] elif arr[i] &gt; second: third = second second = arr[i] elif arr[i] &gt; third: third = arr[i] return first, second, third 找到最大的3个数也比较简单，那么问题来了，如果要找到最大的K个数，并且K比3要大，比如说10，仍旧用上面的办法就比较不现实。 方法三：冒泡排序变种 还记得冒泡排序吗？每一轮经过两两比较后会产生一个最大的数，如果我们控制外部循环的次数就能找到想要数组中相应最大的K个数，实现如下： 1234567def findKLargest(arr, arr_size, K): for i in range(arr_size, K+1, -1): for j in range(i): if arr[j] &gt; arr[j+1]: arr[j], arr[j+1] = arr[j+1], arr[j] return arr[-K:] 缺点还是有的，时间复杂度太高了，最坏的情况下有$O(nk)$，所以当数组长度很长的时候肯定是不行的。 方法四：用堆排序（最大堆） 回忆堆排序的过程，堆排序在成功构建了最大堆之后，最后一步是从堆中取出最大的元素，然后再构建最大堆，再取出最大元素，所以我们也可以通过这种方法来实现从数组中找出K个最大元素的目标。构建最大堆的时间复杂度为$O(n)$，从堆中k次取出最大元素的复杂度为$O(klog(n)$。 1234567891011121314151617181920212223242526272829303132333435363738394041424344# Python program for implementation of heap Sort # To heapify subtree rooted at index i. # n is size of heap # heapify()函数的作用是对索引i下的子树构建最大堆def heapify(arr, n, i): # n是堆的大小，先假设i是根节点 largest = i # Initialize largest as root l = 2 * i + 1 # left = 2*i + 1 r = 2 * i + 2 # right = 2*i + 2 # See if left child of root exists and is # greater than root if l &lt; n and arr[i] &lt; arr[l]: largest = l # See if right child of root exists and is # greater than root if r &lt; n and arr[largest] &lt; arr[r]: largest = r # Change root, if needed if largest != i: arr[i], arr[largest] = arr[largest], arr[i] # 如果发现左右子节点中有大于根节点的，则互换其位置 # Heapify the root. heapify(arr, n, largest) # 继续对子树构建最大堆 # The main function to sort an array of given size def heapSort(arr): n = len(arr) # Build a maxheap. for i in range(n, -1, -1): # 从完全二叉树最低端的叶子节点往根节点遍历，构建最大堆，时间复杂度为O(n) heapify(arr, n, i) # One by one extract elements for i in range(n-1, n-1-K, -1): # 数组中最大的元素永远在堆的最顶端，下标为0 arr[i], arr[0] = arr[0], arr[i] # 找出树中的最大元素，与最后一个node交换 heapify(arr, i, 0) # 重新构建最大堆，此时i是数组的长度，已经不包含最后一个最大的元素了 # Driver code to test above arr = [ 12, 11, 13, 5, 6, 7] heapSort(arr) print ("Sorted array is", arr[-K:])]]></content>
      <categories>
        <category>algorithm</category>
      </categories>
      <tags>
        <tag>面试</tag>
        <tag>python</tag>
        <tag>冒泡排序</tag>
        <tag>topk</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python网络编程之实现FTP服务器]]></title>
    <url>%2F2018%2Fftp-server-in-python%2F</url>
    <content type="text"><![CDATA[复习《Python核心编程》第三章：因特网客户端编程，本章介绍了文件传输、网络新闻和电子邮件三种客户端的编写方法，基本流程都一样： 连接到服务器 登录 发出服务请求 退出 下面以FTP为例，实现一个简单的服务器、客户端交互，客户端能实现下载、上传等功能。 服务器Python下实现FTP服务器很简单，安装好pyftpdlib这个库，几行代码就能够实现一个简单的服务器。 12345678910111213141516#! /usr/bin/env python# coding: utf-8from pyftpdlib.authorizers import DummyAuthorizerfrom pyftpdlib.handlers import FTPHandlerfrom pyftpdlib.servers import FTPServerauthorizer = DummyAuthorizer()authorizer.add_user("user", "12345", "/home/libin", perm="elradfmwMT")# authorizer.add_anonymous("/home/nobody")handler = FTPHandlerhandler.authorizer = authorizerserver = FTPServer(("localhost", 2121), handler)server.serve_forever() 客户端客户端的代码，CSDN上一位博主写了一个FTP的工具类，我觉得挺不错的，下载、上传的功能都很齐全，直接粘贴过来用了。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263#!/usr/bin/python# -*- coding: UTF-8 -*-from ftplib import FTPimport osimport sysimport timeimport socketclass MyFTP: """ ftp自动下载、自动上传脚本，可以递归目录操作 作者：欧阳鹏 博客地址：http://blog.csdn.net/ouyang_peng/article/details/79271113 """ def __init__(self, host, port=2121): """ 初始化 FTP 客户端 参数: host:ip地址 port:端口号 """ # print("__init__()---&gt; host = %s ,port = %s" % (host, port)) self.host = host self.port = port self.ftp = FTP() # 重新设置下编码方式 self.ftp.encoding = 'gbk' self.log_file = open("log.txt", "a") self.file_list = [] def login(self, username, password): """ 初始化 FTP 客户端 参数: username: 用户名 password: 密码 """ try: timeout = 60 socket.setdefaulttimeout(timeout) # 0主动模式 1 #被动模式 self.ftp.set_pasv(True) # 打开调试级别2，显示详细信息 # self.ftp.set_debuglevel(2) self.debug_print('开始尝试连接到 %s' % self.host) self.ftp.connect(self.host, self.port) self.debug_print('成功连接到 %s' % self.host) self.debug_print('开始尝试登录到 %s' % self.host) self.ftp.login(username, password) self.debug_print('成功登录到 %s' % self.host) self.debug_print(self.ftp.welcome) except Exception as err: self.deal_error("FTP 连接或登录失败 ，错误描述为：%s" % err) pass def is_same_size(self, local_file, remote_file): """判断远程文件和本地文件大小是否一致 参数: local_file: 本地文件 remote_file: 远程文件 """ try: remote_file_size = self.ftp.size(remote_file) except Exception as err: # self.debug_print("is_same_size() 错误描述为：%s" % err) remote_file_size = -1 try: local_file_size = os.path.getsize(local_file) except Exception as err: # self.debug_print("is_same_size() 错误描述为：%s" % err) local_file_size = -1 self.debug_print('local_file_size:%d , remote_file_size:%d' % (local_file_size, remote_file_size)) if remote_file_size == local_file_size: return 1 else: return 0 def download_file(self, local_file, remote_file): """从ftp下载文件 参数: local_file: 本地文件 remote_file: 远程文件 """ self.debug_print("download_file()---&gt; local_path = %s ,remote_path = %s" % (local_file, remote_file)) if self.is_same_size(local_file, remote_file): self.debug_print('%s 文件大小相同，无需下载' % local_file) return else: try: self.debug_print('&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;下载文件 %s ... ...' % local_file) buf_size = 1024 file_handler = open(local_file, 'wb') self.ftp.retrbinary('RETR %s' % remote_file, file_handler.write, buf_size) file_handler.close() except Exception as err: self.debug_print('下载文件出错，出现异常：%s ' % err) return def download_file_tree(self, local_path, remote_path): """从远程目录下载多个文件到本地目录 参数: local_path: 本地路径 remote_path: 远程路径 """ print("download_file_tree()---&gt; local_path = %s ,remote_path = %s" % (local_path, remote_path)) try: self.ftp.cwd(remote_path) except Exception as err: self.debug_print('远程目录%s不存在，继续...' % remote_path + " ,具体错误描述为：%s" % err) return if not os.path.isdir(local_path): self.debug_print('本地目录%s不存在，先创建本地目录' % local_path) os.makedirs(local_path) self.debug_print('切换至目录: %s' % self.ftp.pwd()) self.file_list = [] # 方法回调 self.ftp.dir(self.get_file_list) remote_names = self.file_list self.debug_print('远程目录 列表: %s' % remote_names) for item in remote_names: file_type = item[0] file_name = item[1] local = os.path.join(local_path, file_name) if file_type == 'd': print("download_file_tree()---&gt; 下载目录： %s" % file_name) self.download_file_tree(local, file_name) elif file_type == '-': print("download_file()---&gt; 下载文件： %s" % file_name) self.download_file(local, file_name) self.ftp.cwd("..") self.debug_print('返回上层目录 %s' % self.ftp.pwd()) return True def upload_file(self, local_file, remote_file): """从本地上传文件到ftp 参数: local_path: 本地文件 remote_path: 远程文件 """ if not os.path.isfile(local_file): self.debug_print('%s 不存在' % local_file) return if self.is_same_size(local_file, remote_file): self.debug_print('跳过相等的文件: %s' % local_file) return buf_size = 1024 file_handler = open(local_file, 'rb') self.ftp.storbinary('STOR %s' % remote_file, file_handler, buf_size) file_handler.close() self.debug_print('上传: %s' % local_file + "成功!") def upload_file_tree(self, local_path, remote_path): """从本地上传目录下多个文件到ftp 参数: local_path: 本地路径 remote_path: 远程路径 """ if not os.path.isdir(local_path): self.debug_print('本地目录 %s 不存在' % local_path) return self.ftp.cwd(remote_path) self.debug_print('切换至远程目录: %s' % self.ftp.pwd()) local_name_list = os.listdir(local_path) for local_name in local_name_list: src = os.path.join(local_path, local_name) if os.path.isdir(src): try: self.ftp.mkd(local_name) except Exception as err: self.debug_print("目录已存在 %s ,具体错误描述为：%s" % (local_name, err)) self.debug_print("upload_file_tree()---&gt; 上传目录： %s" % local_name) self.upload_file_tree(src, local_name) else: self.debug_print("upload_file_tree()---&gt; 上传文件： %s" % local_name) self.upload_file(src, local_name) self.ftp.cwd("..") def close(self): """ 退出ftp """ self.debug_print("close()---&gt; FTP退出") self.ftp.quit() self.log_file.close() def debug_print(self, s): """ 打印日志 """ self.write_log(s) def deal_error(self, e): """ 处理错误异常 参数： e：异常 """ log_str = '发生错误: %s' % e self.write_log(log_str) sys.exit() def write_log(self, log_str): """ 记录日志 参数： log_str：日志 """ time_now = time.localtime() date_now = time.strftime('%Y-%m-%d', time_now) format_log_str = "%s ---&gt; %s \n " % (date_now, log_str) print(format_log_str) self.log_file.write(format_log_str) def get_file_list(self, line): """ 获取文件列表 参数： line： """ file_arr = self.get_file_name(line) # 去除 . 和 .. if file_arr[1] not in ['.', '..']: self.file_list.append(file_arr) def get_file_name(self, line): """ 获取文件名 参数： line： """ pos = line.rfind(':') while (line[pos] != ' '): pos += 1 while (line[pos] == ' '): pos += 1 file_arr = [line[0], line[pos:]] return file_arrif __name__ == "__main__": my_ftp = MyFTP("localhost") my_ftp.login("user", "12345")]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>网络编程</tag>
        <tag>ftp</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据结构之线性表（二）]]></title>
    <url>%2F2018%2Flinked-list-2%2F</url>
    <content type="text"><![CDATA[静态链表用数组描述的链表叫做静态链表，它是为高级语言实现单链表而设计的一种方法。数组的每一个元素由data和cur组成，数据域data存放数据元素，cur相当于指针，存放后继元素的下标，又叫做游标。 优点：进行插入和删除时，只需要修改游标，不需要移动元素 缺点：无法随机存储；没有解决连续存储带来难以确定的问题 循环链表将单链表的尾指针由空指针改为指向头节点，就使得单链表形成了一个首尾相连的环，这样的单链表称为循环链表。 双向链表双向链表在单链表的每个节点中，再设置一个指向其前驱节点的指针域。]]></content>
      <categories>
        <category>data-structure</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
        <tag>线性表</tag>
        <tag>链式存储结构</tag>
        <tag>顺序存储结构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[排序算法（四）排序算法时间复杂度统计]]></title>
    <url>%2F2018%2FTime-Complexities-of-all-Sorting-Algorithms%2F</url>
    <content type="text"><![CDATA[汇总8个排序算法的时间复杂度统计与比较 Algorithm Time Complexity 空间复杂度 稳定性 最好 平均 最坏 Selection Sort选择排序 Ω(n^2) θ(n^2) O(n^2) O(1) 不稳定 Bubble Sort冒泡排序 Ω(n) θ(n^2) O(n^2) O(1) 稳定 Insertion Sort Ω(n) θ(n^2) O(n^2) O(1) 稳定 Heap Sort Ω(n log(n)) θ(n log(n)) O(n log(n)) O(1) 不稳定 Quick Sort Ω(n log(n)) θ(n log(n)) O(n^2) O(log(n)) 辅助列表 不稳定 Merge Sort Ω(n log(n)) θ(n log(n)) O(n log(n)) O(n) 稳定 Bucket Sort Ω(n+k) θ(n+k) O(n^2) shell Sort希尔排序 Ω(n log(n)) θ(n(log(n))^2) O(n(log(n))^2) 不稳定 各种排序算法稳定性分析现在分析一下常见的排序算法的稳定性，每个都给出简单的理由。 （1）冒泡排序 冒泡排序就是把小的元素往前调（或者把大的元素往后调）。注意是相邻的两个元素进行比较，而且是否需要交换也发生在这两个元素之间。 所以，如果两个元素相等，我想你是不会再无聊地把它们俩再交换一下。 如果两个相等的元素没有相邻，那么即使通过前面的两两交换把两个元素相邻起来，最终也不会交换它俩的位置，所以相同元素经过排序后顺序并没有改变。 所以冒泡排序是一种稳定排序算法。 （2）选择排序 选择排序即是给每个位置选择待排序元素中当前最小的元素。比如给第一个位置选择最小的，在剩余元素里面给第二个位置选择次小的，依次类推，直到第n-1个元素，第n个元素不用选择了，因为只剩下它一个最大的元素了。 那么，在一趟选择时，如果当前锁定元素比后面一个元素大，而后面较小的那个元素又出现在一个与当前锁定元素相等的元素后面，那么交换后位置顺序显然改变了。 举个例子：序列5 8 5 2 9， 我们知道第一趟选择第1个元素5会与2进行交换，那么原序列中两个5的相对先后顺序也就被破坏了。 所以选择排序不是一个稳定的排序算法。 （3）插入排序 插入排序是在一个已经有序的小序列的基础上，一次插入一个元素。当然，刚开始这个有序的小序列只有1个元素，也就是第一个元素（默认它有序）。 比较是从有序序列的末尾开始，也就是把待插入的元素和已经有序的最大者开始比起，如果比它大则直接插入在其后面。否则一直往前找直到找到它该插入的位置。如果遇见一个与插入元素相等的，那么把待插入的元素放在相等元素的后面。 所以，相等元素的前后顺序没有改变，从原无序序列出去的顺序仍是排好序后的顺序，所以插入排序是稳定的。 （4）快速排序 快速排序有两个方向，左边的i下标一直往右走（当条件a[i] &lt;= a[center_index]时），其中center_index是中枢元素的数组下标，一般取为数组第0个元素。 而右边的j下标一直往左走（当a[j] &gt; a[center_index]时）。 如果i和j都走不动了，i &lt;= j, 交换a[i]和a[j],重复上面的过程，直到i&gt;j。交换a[j]和a[center_index]，完成一趟快速排序。 在中枢元素和a[j]交换的时候，很有可能把前面的元素的稳定性打乱，比如序列为 5 3 3 4 3 8 9 10 11 现在中枢元素5和3(第5个元素，下标从1开始计)交换就会把元素3的稳定性打乱。 所以快速排序是一个不稳定的排序算法，不稳定发生在中枢元素和a[j]交换的时刻。 （5）归并排序 归并排序是把序列递归地分成短序列，递归出口是短序列只有1个元素(认为直接有序)或者2个序列(1次比较和交换)， 然后把各个有序的段序列合并成一个有序的长序列，不断合并直到原序列全部排好序。 可以发现，在1个或2个元素时，1个元素不会交换，2个元素如果大小相等也没有人故意交换，这不会破坏稳定性。 那么，在短的有序序列合并的过程中，稳定是是否受到破坏？ 没有，合并过程中我们可以保证如果两个当前元素相等时，我们把处在前面的序列的元素保存在结果序列的前面，这样就保证了稳定性。 所以，归并排序也是稳定的排序算法。 （6）基数排序 基数排序是按照低位先排序，然后收集；再按照高位排序，然后再收集；依次类推，直到最高位。 有时候有些属性是有优先级顺序的，先按低优先级排序，再按高优先级排序，最后的次序结果就是高优先级高的在前，高优先级相同的情况下低优先级高的在前。 基数排序基于分别排序，分别收集，所以其是稳定的排序算法。 （7）希尔排序 希尔排序是按照不同步长对元素进行插入排序，当刚开始元素很无序的时候，步长最大，所以插入排序的元素个数很少，速度很快； 当元素基本有序时，步长很小，插入排序对于有序的序列效率很高。所以，希尔排序的时间复杂度会比O(N^2)好一些。 由于多次插入排序，我们知道一次插入排序是稳定的，不会改变相同元素的相对顺序， 但在不同的插入排序过程中，相同的元素可能在各自的插入排序中移动，最后其稳定性就会被打乱。 所以shell排序是不稳定的排序算法。 （8）堆排序 我们知道堆的结构是节点i的孩子为2i和2i+1节点，大顶堆要求父节点大于等于其2个子节点，小顶堆要求父节点小于等于其2个子节点。 在一个长为n的序列，堆排序的过程是从第n/2开始和其子节点共3个值选择最大（大顶堆）或者最小（小顶堆），这3个元素之间的选择当然不会破坏稳定性。 但当为n/2-1, n/2-2, …1这些个父节点选择元素时，就会破坏稳定性。 有可能第n/2个父节点交换把后面一个元素交换过去了，而第n/2-1个父节点把后面一个相同的元素没有交换，那么这2个相同的元素之间的稳定性就被破坏了。 所以，堆排序不是稳定的排序算法。]]></content>
      <categories>
        <category>algorithm</category>
      </categories>
      <tags>
        <tag>排序算法</tag>
        <tag>时间复杂度</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据结构之线性表（一）]]></title>
    <url>%2F2018%2Flinked-list-1%2F</url>
    <content type="text"><![CDATA[定义线性表：零个或者多个数据元素的有限序列，注意线性表的第一个元素没有前驱，最后一个元素没有后继，指向为空，并且任意一个元素仅仅对应一个或零个元素。 存储结构 顺序存储结构 顺序存储结构指的是用一段地址连续的存储单元依次存储线性表的元素，顺序存储结构具有三个特性： 存储空间的起始位置 线性表的最大存储量：定义线性表中最多能存储多少数据，在存储分配后一般是不变的 线性表当前长度：随着线性表元素的增加或减少，这个变量是变化的 线性表中每个元素占用的存储单元是固定的，所以知道了线性表中任意一个元素的地址就可以随时计算出其他元素的地址 线性表的元素查找时间复杂度为$O(1)$，而元素的插入和删除的复杂度都为$O(n)$，因为牵一发而动全身，每个数据的地址都会随着元素插入和删除而跟着改变，这也是它最大的缺点。 下面是Python文档中list操作的时间复杂度统计： 可以看出，在Python中，list的就是采用的顺序存储结构，从Python官方的文档说明list最大的开销来自于当前分配空间的变化，因为一旦插入或删除某个元素，其他所有的元素都必须得跟着移动。 Internally, a list is represented as an array; the largest costs come from growing beyond the current allocation size (because everything must move), or from inserting or deleting somewhere near the beginning (because everything after that must move). If you need to add/remove at both ends, consider using a collections.deque instead. 链式存储结构 链式存储结构区别于顺序存储结构的地方在于，它的数据元素除了存储数据元素信息，还需要存储后继元素的存储地址。另外，对于顺序存储结构，它在内存中的存储必须是连续的，而链式存储结构则不受此影响，内存中未被占用的任意位置都能被链式存储结构利用起来，所以它没有顺序存储结构遗留的“碎片”问题。 除了存储元素信息的数据域，还需要存储它后继元素的存储位置——指针域 头指针与头节点：头指针不能为空，它是链表必要元素；头结点不是必要元素，头节点的数据域可以不存储任何信息，也可以存储如线性表长度等类的附加信息，头结点的指针域存储指向第一个结点的指针（即第一个元素结点的存储位置） 线性表的 leetcode 题 Remove Nth Node From End of List Medium Given a linked list, remove the n-th node from the end of list and return its head. Example: 123Given linked list: 1-&gt;2-&gt;3-&gt;4-&gt;5, and n = 2.After removing the second node from the end, the linked list becomes 1-&gt;2-&gt;3-&gt;5. Note: Given n will always be valid. Follow up: Could you do this in one pass? 给定一个线性表，要求删除从尾部开始数第n个节点，并返回，题目要求一次遍历就完成，解法： 这一题用到了快慢指针，快指针比慢指针快n步，所以当快指针到达链表的末尾时，慢指针刚好距离末尾还有n个距离。 1234567891011121314151617181920212223242526# Definition for singly-linked list.# class ListNode(object):# def __init__(self, x):# self.val = x# self.next = Noneclass Solution(object): def removeNthFromEnd(self, head, n): """ :type head: ListNode :type n: int :rtype: ListNode """ dummy = ListNode(0) # 初始化一个dummy dummy.next = head slow = fast = dummy for _ in range(n): # 快指针先走n步 fast = fast.next while fast.next: # 当快指针到达链表的末尾，慢指针距离末尾n fast = fast.next slow = slow.next slow.next = slow.next.next # 慢指针直接越过下一个Node return dummy.next Reverse Linked List Easy Reverse a singly linked list. Example: 12Input: 1-&gt;2-&gt;3-&gt;4-&gt;5-&gt;NULLOutput: 5-&gt;4-&gt;3-&gt;2-&gt;1-&gt;NULL Follow up: A linked list can be reversed either iteratively or recursively. Could you implement both? 这是一道翻转链表的题目，难度程度为easy。 12345678910111213141516171819# Definition for singly-linked list.# class ListNode(object):# def __init__(self, x):# self.val = x# self.next = Noneclass Solution(object): def reverseList(self, head): """ :type head: ListNode :rtype: ListNode """ prev = None # 初始化一个为null的节点 while head != None: tmp = head.next # 将head.next临时存放在tmp中 head.next = prev # head.next指向先前的null prev = head # prev往后移动一个节点 head = tmp # head往后移动一个节点 return prev]]></content>
      <categories>
        <category>data-structure</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
        <tag>线性表</tag>
        <tag>链式存储结构</tag>
        <tag>顺序存储结构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[用Python为你念一句诗]]></title>
    <url>%2F2018%2Fread-poem-for-you%2F</url>
    <content type="text"><![CDATA[看完网络编程的知识，觉得客户端和服务器之间消息传递太无聊了，于是调用了一个外部API和语音合成服务，只要在客户端输入任意内容，服务器返回一句诗并读给你听。 知识回顾回顾《Python核心编程》第二章：网络编程的基础知识。 c/s架构客户端/服务器架构，也称为c/s架构，即客户端发出请求，服务器接受请求并提供服务。举个例子，银行的柜员可以看作是服务器，而排队的客户则可以视为一个一个的客户端，柜员不断接受并处理来自客户的请求，然后将结果回复给客户。 c/s架构通常分为软件和硬件两种： 硬件：打印机、文件服务器 软件：web服务器 套接字 一种计算机网络数据结构，开始任何通信之前必须创建套接字 套接字家族（address family）：AF_UNIX, AF_NETLINK, AF_TIPC, AF_INET（最常用） 主机-端口对：“区号-电话号码”，有效端口范围为0~65535 套接字类型： 面向连接：TCP（传输控制协议），SOCK_STREAM 无连接套接字：UDP（用户数据报协议），SOCK_DGRAM 实现步骤实现的步骤较为简单，代码见github：https://github.com/swordspoet/ReadPoem 创建TCP服务器12345678910111213141516171819202122232425262728293031323334#!/usr/bin/env python# coding: utf-8from socket import *from poetry_request import poetry_queryhost = 'localhost'port = 21765buff_size = 1024address = (host, port) # 主机-地址对server_sock = socket(AF_INET, SOCK_STREAM) # 创建服务器套接字server_sock.bind(address) # 套接字与主机-地址对绑定server_sock.listen(5) # 监听连接while 1: # 服务器无线循环 cli_sock, addr = server_sock.accept() # 接受客户端连接，开启单线程服务器 while 1: # 通信无限循环 query = cli_sock.recv(buff_size).decode() # 接受客户端消息 if not query: break query = poetry_query() print(query.get('result').get('content')) # 从API返回结果中提取诗句内容并发送至客户端 content = query.get('result').get('content') cli_sock.send(bytes(content.encode('utf8'))) # auth.save_voice(content.encode('utf8'), 'test') # player.play_voice_by('test') cli_sock.close()server_sock.close() 创建TCP客户端123456789101112131415161718192021222324252627282930313233343536#!/usr/bin/bin python# coding: utf-8from socket import *from voice import Voicefrom player import Playerhost = 'localhost'port = 21765buff_size = 1024address = (host, port)cli_sock = socket(AF_INET, SOCK_STREAM)cli_sock.connect(address)# 阿里云语音合成配置（秘钥可以找我索取）secret_id = ''secret_key = ''auth = Voice(secret_id, secret_key)player = Player()while 1: query = input("念诗一首: ") if not query: break cli_sock.send(query.encode()) data = cli_sock.recv(buff_size).decode() # 客户端接收来自服务器的消息 print(data) auth.save_voice(data.encode('utf8'), 'test') # 将服务器返回的诗句合成为MP3文件 player.play_voice_by('test') # 调用系统播放器播放MP3文件 if not data: breakcli_sock.close() 调用诗词API接口接口的地址是：https://api.apiopen.top/singlePoetry，直接访问便可返回json字段的诗词信息，如： 1&#123;"code":200,"message":"成功!","result":&#123;"author":"白居易","origin":"空闺怨","category":"古诗文-抒情-闺怨","content":"秋霜欲下手先知，灯底裁缝剪刀冷。"&#125;&#125; 12345678910111213141516#!/usr/bin/env python# coding: utf-8import certifiimport urllib3import jsonhttp = urllib3.PoolManager(cert_reqs='CERT_REQUIRED', ca_certs=certifi.where())def poetry_query(): request_url = 'https://api.apiopen.top/singlePoetry' response_text = http.request('GET', request_url) response = json.loads(response_text.data.decode('utf-8')) return response 诗词转换为MP3文件文本转为语音，即TTS，bat公司都有开放的语音合成服务，因为之前用过阿里的，所以直接调用（引用了https://pypi.org/project/aliyun-voice/1.0.2/）。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778#!/usr/bin/env python# coding: utf-8import requestsimport timeimport urllibimport hmac, hashlib, base64class Voice(object): def __init__(self, access_id, access_key): ''' 用阿里云access_id,access_key初始化 ''' self.access_id = access_id self.access_key = access_key self.tts_params = &#123; "encode_type": "mp3", "voice_name": "xiaoyun", "volume": 40, "sample_rate": 16000, "speech_rate": 0, "pitch_rate": 0, "tss_nus": 1, "background_music_id": -1, "background_music_offset": 0, "background_music_volume": 50, &#125; self.__API = "https://nlsapi.aliyun.com/speak?%s" def __get_tts_auth(self, text, date): md5 = hashlib.md5() md5.update(text) body_md5 = base64.b64encode(md5.digest()) feature = "%s\n%s\n%s\n%s\n%s" % ( "POST", "audio/%s,application/json" % self.tts_params["encode_type"], body_md5.decode('utf-8'), "text/plain", date) return base64.b64encode(hmac.new(bytes(self.access_key.encode('utf-8')), bytes(feature.encode('utf-8')), hashlib.sha1).digest()).decode('utf-8') def __get_tts_params(self): return urllib.parse.urlencode(self.tts_params) def get_voice(self, text, **kw): ''' 获取文本的声音文件 @params&lt;string&gt; 文本 @params&lt;dict&gt; 设置参数，参照aliyun tts的配置。 【注意，文件类型只需传入 [mp3, wav, ...]即可】 @return&lt;raw&gt; 返回声音的二进制 @throw error 当参数不符合阿里云要求时会抛出错误 ''' for key, value in kw.items(): self.tts_params[key] = value date = time.strftime("%a, %d %b %Y %H:%M:%S GMT", time.gmtime()) headers = &#123; "Authorization": "Dataplus %s:%s" % (self.access_id, self.__get_tts_auth(text, date)), "Content-Type": "text/plain", "accept": "audio/%s,application/json" % self.tts_params["encode_type"], "date": date &#125; urlstr = self.__API % self.__get_tts_params() r = requests.post(urlstr, headers=headers, data=text, stream=True) content_type = r.headers['Content-Type'] if content_type.find('json') != -1: raise StandardError(r.json()) else: return r.content # return r.content def save_voice(self, text, dist, **kw): ''' 存储文本的声音文件 @params&lt;string&gt; 文本 @params&lt;string&gt; 声音文件存储路径 @params&lt;dict&gt; 设置参数，参照aliyun tts的配置。 【注意，文件类型只需传入 [mp3, wav, ...]即可】 @throw error 当参数不符合阿里云要求时会抛出错误 ''' content = self.get_voice(text, **kw) with open(dist, 'wb') as fd: fd.write(content) 调用播放器播放MP3文件经过文字转语音，还需要调用播放器来播放MP3文件。 123456789import subprocessclass Player(object): def play_voice_by(self, voice_file_path): '''- 调用 mac 系统播放器 afplay 播放 MP3 文件 - linux 下安装 sudo apt install mplayer，调用方法为：subprocess(['mplayer', voice_file_path]) ''' subprocess.call(['mplayer', voice_file_path])]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>网络编程</tag>
        <tag>tcp</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[排序算法（三）：希尔排序]]></title>
    <url>%2F2018%2Fshell-sort%2F</url>
    <content type="text"><![CDATA[希尔排序是插入排序的升级版，回忆插入排序，当拿到一张新的扑克牌，我们需要拿着它逐一与手中的牌比较。当这张新的扑克牌恰好是最小的，那意味着手里有多少张牌就要移动多少次，所以插入排序的时间复杂度跟原数组的有序程度紧密相连。 为了提高插入排序的效率，算法专家考虑是否能先把数组变得相对有序一些，尽量先把数组中较大的和较小的分别放置在数组的末尾和开头，这样比较的次数就会下降很多。 1234567891011121314151617181920# Python program for implementation of Shell Sort def shellSort(arr): n = len(arr) gap = n // 2 # 初始化间隔，间隔的取舍没有确切的规定 while gap &gt; 0: for i in range(gap, n): # 索引从间隔（gap）--&gt;数组的末尾（n） new_card = arr[i] # 手中的扑克牌 j = i - gap # 索引间隔gap左侧的元素 while j &gt;= 0 and arr[j] &gt; new_card: # 当左侧的元素大于手中的扑克牌 arr[j + gap] = arr[j] # 将较大的元素放在i=j+gap的位置 j -= gap arr[j + gap] = new_card # gap左侧的元素遍历完了，将手中的扑克牌放在j+gap的位置，同插入排序 gap //= 2 对比插入排序，其实代码的差别不大，只不过把代码中的1换成了gap： 123456789def insert_sort(array): for i in range(1, len(array)): new_card = array[i] # 手中的牌 j = i - 1 while j &gt;= 0 and new_card &lt; array[j]: # 新摸到的牌如果大于手中的牌则往后移动 array[j+1] = array[j] j -= 1 array[j+1] = new_card return array]]></content>
      <categories>
        <category>coding</category>
        <category>algorithm</category>
      </categories>
      <tags>
        <tag>数据结构与算法</tag>
        <tag>排序算法</tag>
        <tag>插入排序</tag>
        <tag>希尔排序</tag>
        <tag>shell sort</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[排序算法（二）：堆排序]]></title>
    <url>%2F2018%2Fheap-sort%2F</url>
    <content type="text"><![CDATA[完全二叉树的特点（百度百科）： 一棵二叉树至多只有最下面的两层上的结点的度数可以小于2，并且最下层上的结点都集中在该层最左边的若干位置上，则此二叉树成为完全二叉树，并且最下层上的结点都集中在该层最左边的若干位置上，而在最后一层上，右边的若干结点缺失的二叉树，则此二叉树成为完全二叉树。 完全二叉树具备以下特点： 叶子节点只能出现在最下两层 最下一层的的节点可能没有右子节点，如果只有一个节点，那么它必须是左子节点 除了最下面两层，每个节点必须有且仅有两个子节点 从完全二叉树回到堆排序，堆排序分为三个步骤： 首先，我们需要从已有的数组构建最大堆，然后，已有数据的最大值会位于堆的最顶端（root node） 最大堆一旦建立，将最大值（root node）与数组最后一个元素交换，并从堆中删除最后一个node 只要堆的大小大于1，重复1-2步骤 堆排序的时间复杂度： 堆排序是一种原地排序（inplace）算法，不需要分配额外的内存空间，它的运行时间主要消耗在了构建堆和重建堆时的反复筛选上了，它对原始数据的排序状态并不敏感，总体来说，无论是最好、最坏还是平均，堆排序的时间复杂度都为$O(nlogn)$。 下面是堆排序的Python实现： 123456789101112131415161718192021222324252627282930313233343536373839404142434445# Python program for implementation of heap Sort # To heapify subtree rooted at index i. # n is size of heap # heapify()函数的作用是对索引i下的子树构建最大堆def heapify(arr, n, i): # n是堆的大小，先假设i是根节点 largest = i # Initialize largest as root l = 2 * i + 1 # left = 2*i + 1 r = 2 * i + 2 # right = 2*i + 2 # See if left child of root exists and is # greater than root if l &lt; n and arr[i] &lt; arr[l]: largest = l # See if right child of root exists and is # greater than root if r &lt; n and arr[largest] &lt; arr[r]: largest = r # Change root, if needed if largest != i: arr[i], arr[largest] = arr[largest], arr[i] # 如果发现左右子节点中有大于根节点的，则互换其位置 # Heapify the root. heapify(arr, n, largest) # 继续对子树构建最大堆 # The main function to sort an array of given size def heapSort(arr): n = len(arr) # Build a maxheap. for i in range(n, -1, -1): # 从完全二叉树最低端的叶子节点往根节点遍历，构建最大堆，时间复杂度为O(n) heapify(arr, n, i) # One by one extract elements for i in range(n-1, 0, -1): # 数组中最大的元素永远在堆的最顶端，下标为0 arr[i], arr[0] = arr[0], arr[i] # 找出树中的最大元素，与最后一个node交换 heapify(arr, i, 0) # 重新构建最大堆，此时i是数组的长度，已经不包含最后一个最大的元素了 # Driver code to test above arr = [ 12, 11, 13, 5, 6, 7] heapSort(arr) print ("Sorted array is", arr) # This code is contributed by Mohit Kumra]]></content>
      <categories>
        <category>coding</category>
        <category>algorithm</category>
      </categories>
      <tags>
        <tag>数据结构与算法</tag>
        <tag>排序算法</tag>
        <tag>快速排序</tag>
        <tag>归并排序</tag>
        <tag>插入排序</tag>
        <tag>选择排序</tag>
        <tag>冒泡排序</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python 工具箱系列 （6）：为jupyter notebook中添加虚拟环境]]></title>
    <url>%2F2018%2Fadd-virtualenv-in-jupyter-notebook%2F</url>
    <content type="text"><![CDATA[123456789# 建立虚拟环境scrapymkvirtualenv scrapy# 进入虚拟环境scrapyworkon scrapy# 安装jupyter，具体操作略过(scrapy) pip install jupyter# 安装ipykernel，添加kernel(scrapy) pip install ipykernel(scrapy) python -m ipykernel install --user --name scrapy --display-name "Python2(scrapy)"]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>虚拟环境</tag>
        <tag>jupyter notebook</tag>
        <tag>virtualenv</tag>
        <tag>python工具箱系列</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[详解RNN文本分类模型的架构]]></title>
    <url>%2F2018%2FRNN-structure-in-text-classification%2F</url>
    <content type="text"><![CDATA[典型RNN模型下图是典型的RNN模型，$X$是输入，对文本经过嵌入层嵌入处理，再进入RNN，RNN的后面是全连接层，输出各个类别的概率。 下面来描述一次数据从输入到输出的完整过程： 在自然语言处理中，不管是中文还是英文，首先第一步的任务是如何将文本数据数值化。对于中文，可以先建立词汇表，给词汇表中的没歌词建立唯一的id标识（数字），这样每段文本都可以用一串数字id来表示，然后就能进行词嵌入操作，英文的处理方法也类似。 对文本预处理时，由于每条文本的长度不一，需要给输入统一规定长度（seq_length），超过的截断，不足的填充。假设有$N​$段文本（text），统一长度后就变成了[N, 1, seq_length]​大小的矩阵，矩阵的每一行代表一段文本，大小为​[1, seq_length]​。 通常，传统的做法是将one-hot编码，但是劣势是矩阵会很稀疏，并且上下文之间的语义关系非常模糊，难以捕捉。现在一般的做法是将文本词向量化，将每个词（字）“嵌入”为大小为[1, embedding_size]大小的向量矩阵。到了这一步，输入基本就确定下来了，它是大小为[seq_length, embedding_size]大小的矩阵。 接下来是训练，训练中可以一条一条给RNN“喂”数据，当有一条数据进来，等待它的是seq_length个cell，LSTM的输出整个过程如下： 但是这样的训练太慢了，并且优化的参数也难以顾及到全局的信息，通常是批次训练，一次喂给模型batch_size条数据。 批次训练全过程详解 原帖地址：understanding-pytorch-batching-lstm/Understanding Pytorch Batching.ipynb at master · ngarneau/understanding-pytorch-batching-lstm 1. one-hot 编码 2. 零填充 3. 批量输入将每一个batch设置为4 4. 词嵌入（word embedding）矩阵中的每一行代表一个字母的向量 5. Packed Embeddings 注意，这里将词向量矩阵打包过了一次，因为我们并不需要零填充后的词向量。在这种情况下，batch_sizes为[4, 4, 4, 3, 2, 1]，即第一步到第三步每次会给LSTM喂4个字母，到第四步，San已经用光了，所以给LSTM的只有3个字母，以此类推，到最后只有1个字母v。 6. LSTM out数据进入RNN之后输出的结果 7. padded LSTM out填充LSTM的输出 8. reshape output 9. maxpooling最大池化 10. predict]]></content>
      <categories>
        <category>Machine-Learning</category>
      </categories>
      <tags>
        <tag>架构</tag>
        <tag>RNN</tag>
        <tag>循环神经网络</tag>
        <tag>分类模型</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于 sklearn 的传统机器学习在搜狗新闻数据的文本分类实践]]></title>
    <url>%2F2018%2Ftext-classification-classic-ml-by-sklearn%2F</url>
    <content type="text"><![CDATA[sklearn is good tool. 1. 背景在sklearn下比较各个传统机器学习模型在文本分类任务上的表现。 2. 数据准备数据源依旧来自搜狗的新闻数据，由于机器的性能，本文只用到了一个星期的数据。 数据清洗从搜狗下载的新闻数据格式为xml，从HTML标签中提取标签和正文的工作在基于 Tensorflow 的 TextCNN 在搜狗新闻数据的文本分类实践一文中已经完成，不再赘述。因为在本次实验中不再使用基于字符级的思想，而是基于词，所以还要对数据做分词、去噪清洗操作。 分词：使用jieba分词工具，因为在分词时数据是单条单条处理的，所以速度会比较慢，对原始数据采用多进程分词，能节省不少的时间。你可以把多进程理解为把任务列表扔给进程池，“这是交给你的任务，你们赶紧给我办好！”，然后进程池的工人们就迅速过来认领任务，并同时开始工作。 12345678910111213141516171819202122232425262728293031# coding: utf-8import jiebastopwords_path = 'stopwords.txt'class CutWord(object): def __init__(self, stopwords_path=stopwords_path): self.stopwords_path = stopwords_path def add_dictionary(self, dict_list): map(lambda x: jieba.load_userdict(x), dict_list) def seg_sentence(self, sentence, stopwords_path=None): if not stopwords_path: stopwords_path = self.stopwords_path def stopwords_list(file_path): stopwords = [line.strip() for line in open(file_path, 'r').readlines()] return stopwords segmented_sentence = jieba.cut(sentence.strip()) stopwords_list = stopwords_list(stopwords_path) out_str = '' for word in segmented_sentence: if word not in stopwords_list: out_str += word out_str += ' ' return out_str 多进程分词： 123456789from multiprocessing import Poolpool = Pool(processes=6)for file_path, write_path in zip(file_list, write_list): paths = [file_path, write_path] pool.apply_async(seg_file, (paths,))pool.close()pool.join()print('Word segment finished...') 去噪：原始数据中有标点符号、停用词等噪声，通过加载外部停用词文件过滤掉这一部分的噪声。为了方便，把这两个功能整合成为一个工具类CutWord，在其他程序中直接调用使用即可。 分词、去噪清洗工作完毕，看看各个类别下的数据分布，占比最多的前四个是体育、新闻、商业和娱乐，其余8个类别的新闻占比均不足5%，数据量比较小。因为在这之前我用该数据做了二分类的任务，发现分类效果实在太好了，1万多的测试数据只有十几个分错了，好得有点不真实，所以，我决定拿占比排名前4类的数据来做实验。 数据切分每个类别新闻按照5000条训练集、500条验证集、1000条测试集提取，所以总共有训练集：验证集：测试集为20000：2000：4000。 特征提取sklearn中常用的文本特征提取方法有tfidf，将文本转换为文档-词项矩阵，矩阵中的元素，可以使用词频也可以使用tfidf值。为了获得更好的效果，本次试验的文本特征提取用到了全量的数据。 123456def tf_idf(contents): """提取文本特征tf-idf""" vectorizer = CountVectorizer(min_df=1e-5) transformer = TfidfTransformer() tfidf = transformer.fit_transform(vectorizer.fit_transform(contents)) return tfidf 3. 模型训练3.1 logistic regression logitstic regression的背景知识参考我的笔记：机器学习算法系列（3）Logistic Regression | Thinking Realm 为了有更好地优化参数，选用了更有效的交叉验证的LogisticRegressionCV模型，cv设置为10，训练的时间略长；n_jobs设置为-1，利用起所有的CPU核心。 123456lr_model = LogisticRegressionCV(solver='newton-cg', multi_class='multinomial', cv=10, n_jobs=-1)lr_model.fit(x_train[:20000], y_train[:20000])y_pred = lr_model.predict(x_train[20000:24000])print(metrics.confusion_matrix(y_train[20000:24000], y_pred))print(classification_report(y_train[20000:24000], y_pred, target_names=['business', 'sports', 'news', 'yule'])) 训练结果如下，模型在测试集上的表现还不错，除了体育的准确率为89%，其余的类别的指标都在90%以上。读不懂报告，参考：读懂sklearn的classification_report 1234567891011121314[[924 84 0 0] [ 51 941 16 16] [ 1 14 963 5] [ 3 16 5 961]] precision recall f1-score support business 0.94 0.92 0.93 1008 sports 0.89 0.92 0.91 1024 news 0.98 0.98 0.98 983 yule 0.98 0.98 0.98 985 micro avg 0.95 0.95 0.95 4000 macro avg 0.95 0.95 0.95 4000weighted avg 0.95 0.95 0.95 4000 3.2 线性支持向量机 支持向量机的背景知识参考我的系列笔记： 机器学习算法系列（4）支持向量机（一） | Thinking Realm 机器学习算法系列（5）支持向量机（二） | Thinking Realm 机器学习算法系列（6）支持向量机（三） | Thinking Realm 123456svm_model = SGDClassifier(n_jobs=-1)svm_model.fit(x_train[:20000], y_train[:20000])y_pred = svm_model.predict(x_train[20000:24000])print(metrics.confusion_matrix(y_train[20000:24000], y_pred))print(classification_report(y_train[20000:24000], y_pred, target_names=['business', 'sports', 'news', 'yule'])) 强大如支持向量机，30秒不到就完成了训练，按照性价比，完全可以把LogisticRegressionCV摁在地上摩擦。 1234567891011121314[[889 118 0 1] [ 67 922 17 18] [ 2 12 962 7] [ 1 14 4 966]] precision recall f1-score support business 0.93 0.88 0.90 1008 sports 0.86 0.90 0.88 1024 news 0.98 0.98 0.98 983 yule 0.97 0.98 0.98 985 micro avg 0.93 0.93 0.93 4000 macro avg 0.94 0.94 0.94 4000weighted avg 0.94 0.93 0.93 4000 3.3 朴素贝叶斯 朴素贝叶斯的背景知识参考我的系列笔记：机器学习算法系列（7）朴素贝叶斯法 | Thinking Realm 速度飞快，效果还行。 1234567891011121314[[908 99 0 1] [129 842 15 38] [ 2 14 948 19] [ 6 17 4 958]] precision recall f1-score support business 0.87 0.90 0.88 1008 sports 0.87 0.82 0.84 1024 news 0.98 0.96 0.97 983 yule 0.94 0.97 0.96 985 micro avg 0.91 0.91 0.91 4000 macro avg 0.91 0.92 0.91 4000weighted avg 0.91 0.91 0.91 4000 4. 模型对比 文本分类任务，支持向量机的表现可以吊打其他模型 一般的数据集的分类任务，各个模型的表现差距不是特别明显，用法大同小异 在结巴分词下逐条分词速度很慢，考虑多进程来解决 该实验用到的数据量不大，当数据量上升，场景复杂化，参数优化必不可少]]></content>
      <categories>
        <category>Machine-Learning</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>支持向量机</tag>
        <tag>搜狗新闻数据</tag>
        <tag>文本分类</tag>
        <tag>传统机器学习方法</tag>
        <tag>sklearn</tag>
        <tag>logistic regression</tag>
        <tag>svm</tag>
        <tag>朴素贝叶斯</tag>
        <tag>text classification</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[读懂sklearn的classification_report]]></title>
    <url>%2F2018%2Funderstanding-sklearn-classification-report%2F</url>
    <content type="text"><![CDATA[本文介绍如何读懂classification_report的各个指标。 y_true为样本真实标记，y_pred为样本预测标记 support：某类别在测试数据中的样本个数，测试数据中有1个样本的真实标签为class 0 precision：模型预测的结果中有多少是预测正确的 micro avg：计算所有数据下的指标值，比如全部数据5个样本中有3个预测正确，故micro avg为0.6 macro avg：每个类别评估指标未加权的平均值，比如准确率的macro avg，(0.50+0.00+1.00)/3=0.5 weighted avg：加权平均，比如第一个值的计算方法，(0.50*1 + 0.0*1 + 1.0*3)/5 = 0.70 1234567891011121314&gt;&gt;&gt; from sklearn.metrics import classification_report&gt;&gt;&gt; y_true = [0, 1, 2, 2, 2]&gt;&gt;&gt; y_pred = [0, 0, 2, 2, 1]&gt;&gt;&gt; target_names = ['class 0', 'class 1', 'class 2']&gt;&gt;&gt; print(classification_report(y_true, y_pred, target_names=target_names)) precision recall f1-score support class 0 0.50 1.00 0.67 1 class 1 0.00 0.00 0.00 1 class 2 1.00 0.67 0.80 3 micro avg 0.60 0.60 0.60 5 macro avg 0.50 0.56 0.49 5weighted avg 0.70 0.60 0.61 5]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>sklearn</tag>
        <tag>classification_report</tag>
        <tag>分类报告</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于 Tensorflow 的 TextRNN 在搜狗新闻数据的文本分类实践]]></title>
    <url>%2F2018%2Ftext-classification-rnn-by-tensorflow%2F</url>
    <content type="text"><![CDATA[先前一篇文章基于 Tensorflow 的 TextCNN 在搜狗新闻数据的文本分类实践是CNN在文本分类中的一次尝试，在接触了LSTM之后了解到它自然语言处理中有非常广泛的应用，比如情感分析、信息提取（命名体识别）、机器翻译等领域大放异彩，本文就来看看它在文本分类上究竟会有什么样的表现。 因为原始数据的处理在上一次尝试中已经完成了，所以在本次实验中只需要完成RNN模型的参数配置和模型运行的脚本。 RNN配置参数1234567891011121314151617181920class TRNNConfig(object): """RNN配置参数""" embedding_dim = 64 seq_length = 1000 num_classes = 11 vocab_size = 5000 num_layers = 2 # 隐藏层层数 hidden_dim = 128 rnn = 'gru' # 记忆单元是lstm或gru dropout_keep_prob = 0.8 learning_rate = 1e-3 batch_size = 128 num_epochs = 10 print_per_batch = 100 save_per_batch = 10 RNN训练与验证RNN的训练验证部分代码与CNN的差别很小，修改模型存储路径等几个小地方即可。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195# coding: utf-8from __future__ import print_functionimport osimport sysimport timefrom datetime import timedeltaimport numpy as npimport tensorflow as tffrom sklearn import metricsfrom rnn_model import TRNNConfig, TextRNNfrom data.sougounews_loader import read_vocab, read_category, batch_iter, process_file, build_vocabbase_dir = 'data/'train_dir = os.path.join(base_dir, 'news_train.txt')test_dir = os.path.join(base_dir, 'news_test.txt')val_dir = os.path.join(base_dir, 'news_val.txt')vocab_dir = os.path.join(base_dir, 'vocab.txt')save_dir = 'checkpoints/text_rnn'save_path = os.path.join(save_dir, 'best_validation') # 最佳验证结果的保存路径def get_time_dif(start_time): end_time = time.time() time_dif = end_time - start_time return timedelta(seconds=int(round(time_dif)))def feed_data(x_batch, y_batch, keep_prob): feed_dict = &#123; model.input_x: x_batch, model.input_y: y_batch, model.keep_prob: keep_prob &#125; return feed_dictdef evaluate(sess, x_, y_): data_len = len(x_) batch_eval = batch_iter(x_, y_, 128) total_loss = 0.0 total_acc = 0.0 for x_batch, y_batch in batch_eval: batch_len = len(x_batch) feed_dict = feed_data(x_batch, y_batch, 1.0) loss, acc = sess.run([model.loss, model.acc], feed_dict=feed_dict) total_loss += loss * batch_len total_acc += acc * batch_len return total_loss / data_len, total_acc / data_lendef train(): print('Configuring TensorBoard and Saver...') tensorboard_dir = 'tensorboard/text_rnn' if not os.path.exists(tensorboard_dir): os.makedirs(tensorboard_dir) tf.summary.scalar('loss', model.loss) tf.summary.scalar('accuracy', model.acc) merged_summary = tf.summary.merge_all() writer = tf.summary.FileWriter(tensorboard_dir) # Saver 配置 saver = tf.train.Saver() if not os.path.exists(save_dir): os.makedirs(save_dir) print('Loading training and validation data...') # 载入训练集和验证集数据 start_time = time.time() x_train, y_train = process_file(train_dir, word_to_id, cat_to_id, config.seq_length) x_val, y_val = process_file(val_dir, word_to_id, cat_to_id, config.seq_length) time_dif = get_time_dif(start_time) print("Time usage:", time_dif) # 创建 session session = tf.Session() session.run(tf.global_variables_initializer()) writer.add_graph(session.graph) print('Training and evaluating...') start_time = time.time() total_batch = 0 best_acc_val = 0.0 last_improved = 0 require_improvement = 1000 flag = False for epoch in range(config.num_epochs): print('Epoch:', epoch + 1) batch_train = batch_iter(x_train, y_train, config.batch_size) for x_batch, y_batch in batch_train: feed_dict = feed_data(x_batch, y_batch, config.dropout_keep_prob) if total_batch % config.save_per_batch == 0: # 每多少批将训练结果写入 tensorboard s = session.run(merged_summary, feed_dict=feed_dict) writer.add_summary(s, total_batch) if total_batch % config.print_per_batch == 0: # 每多少批次输出训练集和验证集结果 feed_dict[model.keep_prob] = 1.0 loss_train, acc_train = session.run([model.loss, model.acc], feed_dict=feed_dict) loss_val, acc_val = evaluate(session, x_val, y_val) if acc_val &gt; best_acc_val: best_acc_val = acc_val last_improved = total_batch saver.save(sess=session, save_path=save_path) improved_str = '*' else: improved_str = '' time_dif = get_time_dif(start_time) msg = 'Iter: &#123;0:&gt;6&#125;, Train Loss: &#123;1:&gt;6.2&#125;, Train Acc: &#123;2:&gt;7.2%&#125;,' \ + ' Val Loss: &#123;3:&gt;6.2&#125;, Val Acc: &#123;4:&gt;7.2%&#125;, Time: &#123;5&#125; &#123;6&#125;' print(msg.format(total_batch, loss_train, acc_train, loss_val, acc_val, time_dif, improved_str)) session.run(model.optim, feed_dict=feed_dict) total_batch += 1 if total_batch - last_improved &gt; require_improvement: # 验证集准确率不提升，提前结束训练 print("No optimization for a long time, auto-stopping...") flag = True break if flag: breakdef test(): print('Loading test data...') start_time = time.time() x_test, y_test = process_file(test_dir, word_to_id, cat_to_id, config.seq_length) session = tf.Session() session.run(tf.global_variables_initializer()) saver = tf.train.Saver() saver.restore(sess=session, save_path=save_path) print('Testing...') loss_test, acc_test = evaluate(session, x_test, y_test) msg = 'Test Loss: &#123;0:&gt;6.2&#125;, Test Acc: &#123;1:&gt;7.2%&#125;' print(msg.format(loss_test, acc_test)) batch_size = 128 data_len = len(x_test) num_batch = int((data_len - 1) / batch_size) + 1 y_test_cls = np.argmax(y_test, 1) y_pred_cls = np.zeros(shape=len(x_test), dtype=np.int32) for i in range(num_batch): start_id = i * batch_size end_id = min((i + 1) * batch_size, data_len) feed_dict = &#123; model.input_x: x_test[start_id:end_id], model.keep_prob: 1.0 &#125; y_pred_cls[start_id:end_id] = session.run(model.y_pred_cls, feed_dict=feed_dict) # 评估 print('Precision, Recall and F1-score...') print(metrics.classification_report(y_test_cls, y_pred_cls, target_names=categories)) # 混淆矩阵 print('Confusion Matrix...') cm = metrics.confusion_matrix(y_test_cls, y_pred_cls) print(cm) time_dif = get_time_dif(start_time) print('Time usage:', time_dif)if __name__ == '__main__': if len(sys.argv) != 2 or sys.argv[1] not in ['train', 'test']: raise ValueError("""please use: python run_rnn.py [train / test]""") print('Configuring RNN model...') config = TRNNConfig() if not os.path.exists(vocab_dir): build_vocab(train_dir, vocab_dir, config.vocab_size) categories, cat_to_id = read_category() words, word_to_id = read_vocab(vocab_dir) config.vocab_size = len(words) model = TextRNN(config) if sys.argv[1] == 'train': train() else: test() 模型训练和测试结果从运行的结果来看，RNN的训练时间明显要长很多，大约是CNN的两倍，并且内存占用也明显要高。 GRU经过10轮的训练，模型在训练集上的准确率达到96.09%，验证集达到86.59%，相比CNN来说表现还是要差一点点，原因嘛，暂时也不知道（==）。因为训练数据集比较小，所以有的epoch训练结果没有打印在屏幕上。 再看看测试集上的表现，总体准确率为83.96%，除了文化（cul）、商业和社会（news），其他类别的准确率都在80%以上。社会类别的新闻表现最差，才65%，但是我觉得跟搜狗的新闻数据质量也有关系，因为就文章内容来说，文不对题和内容属性模糊的频率很高，特别是社会类别，似乎分在文化类也可以。 LSTM把RNN的cell换成LSTM，看看效果如何。训练的时间依然很长，花了半个小时，10个epoch中，在第8轮验证集数据达到最佳效果，最佳的训练集准确率为95.31%，验证集准确率为85.66%。 测试集数据，准确率为81.79%，这次轮到社会类和汽车的类别预测效果最差，需要进一步优化参数。 思考RNN模型在文本分类任务上表现不佳的原因可能有： RNN训练本身需要大量的数据，而在本次实践中拿到的数据每个类别仅仅只有550条 本次新闻分类尚未引进预训练好的词向量，所以出现了过拟合 基于字符级的文本分类比基于词的表现要差？]]></content>
      <categories>
        <category>Machine-Learning</category>
      </categories>
      <tags>
        <tag>TensorFlow</tag>
        <tag>搜狗新闻数据</tag>
        <tag>文本分类</tag>
        <tag>循环神经网络</tag>
        <tag>textrnn</tag>
        <tag>lstm</tag>
        <tag>gru</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习算法系列（18）理解 LSTM 网络]]></title>
    <url>%2F2018%2Fmachine-learning-algorithm-series-understanding-lstm%2F</url>
    <content type="text"><![CDATA[先前仅仅是听RNN这个名词比较多一点，但没有深入去了解，在前一篇文章中我用CNN做了一次文本分类的实践，然后发现RNN也可以完成该任务，所以便开始寻找相关资料学习RNN。我比较推荐台大李宏毅老师的机器学习课程，算是比较清晰易懂的了，没有公式堆砌，完全“人肉手撕”。课程里面有两节专门讲解RNN，第一节讲原理（认真看），第二节讲应用的场景（快速过）。 学完课程，对照PDF课件差不多知道了RNN相比传统的神经网络具有“记忆”的特性，更加注重数据的上下文关系，就不难理解现有语音识别、机器翻译、自动生成等等技术背后的知识，以及RNN最成功的扩展长短时记忆网络（简称LSTM，Long Short-term Memory）。现在，我基本理解了RNN的一些知识，以笔记的形式记录下来。 神经网络比较回想先前接触的神经网络，前馈神经网络、DNN、卷积神经网络，它们都可以比较好地胜任器学习任务，比如文本分类，这些任务的有一个最大的特点：在建立训练模型的时候不用考虑各个输入之间的关系（上下文），输入与输入、输出与输出都是相互独立的。 现有一个新的场景，如下图，给定一段文字，需要从文字中提取地点和时间信息，用传统的神经网络可以解决，但是如果我们想知道地点到底是“出发”还是“目的地”就不可以了，因为输入“arrive”和“Taipei”之间是独立的，一般神经网络没有办法学习到它们之间的联系，这个时候就需要神经网络拥有“记忆”能力。 在RNN中会有一个记忆单元，它储存着隐藏层的输出，并把它当做下一次的输入之一。你有可能不明白，为什么是“输入之一”的表述，难道还有多个输入？是的，这就是RNN区别于普通神经网络的地方，上一次的输入对下一次的输入会有影响，正是如此它才拥有记忆力，假如你随意改变输入的序列顺序将会对RNN有很大的影响，一个说话颠三倒四的人不太好理解吧哈哈。 如果你不太理解，建议打开原来的视频看看李老师的教学视频，从7:21开始的地方有一个例子，看完就会理解RNN记忆单元的工作原理，本文就不再复述了。 LSTM结构上述是RNN最简单的一种形式，当然它还有很多扩展，其中最有名的便是长短时记忆网络，简称LSTM。 长短时记忆网络，正确的断句应该是“长/短时记忆/网络”，字面意思是在LSTM中RNN会拥有比较长的短时记忆，由遗忘门控制。 相比最简单的RNN，控制LSTM记忆单元（memory cell）有3个gate，分别是输入门（input gate）、输出门（output gate）和遗忘门（forget gate），所以LSTM这种特殊的神经元有四个输入、一个输出： 输入：对应下图指向红色框框的四个箭头 输出：红色框框一个对外指向的箭头 所以，LSTM的参数数量会比较多，是一般神经网络的4倍，LSTM的训练会很难，并且loss的波动会很大，看起来会有点“异常”；还有因为在LSTM中网络是有记忆的，参数的扰动会带来很大的影响，有点类似蝴蝶效应，这也是造成loss波动大的原因之一。 计算过程下图是LSTM的计算过程，输入一共有四个：$Z$、输入门$Z_i$、输出门$Z_o$、遗忘门$Z_f$，一个输出$a$。三个门各司其职，每个门通常使用sigmoid函数作为激活函数，激活后的值处在0和1之间，故方便控制“门”的开启和关闭，输入门决定$Z$能走多远，遗忘门决定记忆单元的值是否刷新或者重置，输出门则决定最后的能否被输出。 一个LSTM单元的计算过程如下： 下图解释了在LSTM中上下文之间是如何关联起来的，每个输入$x^{t+1}$都会接受来自上一个记忆单元的值$c^t$ 到此，LSTM的基本知识学习完毕，总的来说，李宏毅老师的教学视频是目前为止最通俗易懂的，他的课程值得反复观看理解。当然，LSTM的扩展千变万化，有多层、双向，LSTM的参数多，实现起来比较困难，据说当时只有mikolov一个人的代码能work。还好现在很多框架已经实现了这部分的工作，比如keras、tensorflow等等，直接拿过来用即可，这是我学完课程后，找了一些网上的代码做的一次LSTM文本分类实践：基于 Tensorflow 的 TextRNN 在搜狗新闻数据的文本分类实践 | Thinking Realm。 最后引用一段话作为结尾： 不少搞工程的人认为，要理解什么东西，搞明白其底层数学描述是必要和充分的，你需要“了解背后的数学原理”。其实，在所有场景下，这几乎都不是充分的，也不是必要的——远远不是。以PCA为例，知道怎么做5x5矩阵对角化，算是“知道PCA背后的数学原理”。但这对你了解PCA是什么、能做什么，以及为何有用没太大帮助。你需要更高级的心智模式。 这几乎是普遍的事实：要理解某项事物，你需要正确的心智模式，抓住那些真正关键的方面，而不仅仅是最最底层的数学描述。大多数情况下，两种模式完全正交。深度学习反向传播也是如此——知道怎么写反向传播的程序，并不会让你了解深度学习的实用知识，相反，深入深度学习的心智模式，一定不是以了解反向传播算法细节为中心的。此外，有了正确的心智模式，在需要时可以很容易地自行得出算法细节，至少有效实现是没问题的。 via:François Chollet 参考资源 李宏毅老师的课件：Recurrent Neural Network (RNN).pdf) 李宏毅机器学习(2017)之RNN]]></content>
      <categories>
        <category>Machine-Learning</category>
      </categories>
      <tags>
        <tag>机器学习算法系列</tag>
        <tag>RNN</tag>
        <tag>循环神经网络</tag>
        <tag>长短时记忆网络</tag>
        <tag>LSTM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux 工具箱系列（2）：创建交换空间]]></title>
    <url>%2F2018%2Fcreate-swap-partition%2F</url>
    <content type="text"><![CDATA[安装Ubuntu的时候没有创建交换空间，最近跑代码的时候提示内存不足，有必要创建交换空间了，下面记录如何为Linux创建交换空间。 一、快速创建方式 你可能已经注意到了，上述方式创建Swap交换文件的过程比较慢，如果想快速创建交换分区可以使用fallocate程序来实现。该命令会立即创建一个预分配的交换文件，而产是按实际大小写入分配的大小。 如果同样是要创建4G大小的交换创建文件，快捷创建方式的命令如下： 1sudo fallocate -l 4G /swapfile 二、启用Swap分区文件 我们的swapfile交换文件已经创建好了，但Ubuntu还不知道它是被用作交换分区，所以我们还需要将此文件格式化为Swap分区交启用它。 1、首先，我们需要使用如下命令更改下swapfile文件的权限： 1sudo chmod 600 /swapfile 2、然后，我们需要用如下将swapfile初始化为交换文件： 1sudo mkswap /swapfile 3、最后，还需要使用如下命令来启用交换文件： 1sudo swapon /swapfile 5、查看结果 1free -m 6、配置启用时挂载Swap分区文件 Swap交换文件虽然已经配置好并已经启用了，但Ubuntu在重启后是不会挂载我们刚创建的交换分区文件的，因此我们还需要对/etc/fstab文件进行更改，让其在系统启动时就自动挂载我们创建的/swapfile文件。 1sudo vim /etc/fstab 添加如下内容： 1/swapfile none swap sw 0 0]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>swap</tag>
        <tag>交换空间</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于 Tensorflow 的 TextCNN 在搜狗新闻数据的文本分类实践]]></title>
    <url>%2F2018%2Ftext-classification-cnn-by-tensorflow%2F</url>
    <content type="text"><![CDATA[这篇文章记录了从零开始用 tensorflow 构建卷积神经网络模型，并对搜狗的新闻数据做的文本分类实践。众所周知，tensorflow 是一个开源的机器学习框架，它的出现大大降低了机器学习的门槛，即使你没有太多的数学知识，它也可以允许你用“搭积木”的方式快速实现一个神经网络，即使没有调节太多的参数，模型的表现一般还不错。目前，tensorflow 的安装已经变得非常简单，一个简单的pip install tensorflow即可，然后import tensorflow as tf就能愉快玩耍了。 1. 背景卷积神经网络，即CNN，它的核心思想是捕捉数据的局部特征（感兴趣的同学可以阅读我先前写的一篇关于CNN的笔记：机器学习算法系列（13）理解卷积神经网络 | Thinking Realm），不仅仅在图像领域大放异彩，CNN在文本分类领域也有很强的表现。在Yoon Kim的这篇论文中，比较清楚地解释了CNN用于文本分类的原理，关键在于如何将文本向量化，如下图，即把每个词都表示为一个 1×k的向量，对长度为N的文本则表示为N×K的矩阵，经过这一步处理，那么我们就可以把图像上的分类经验应用到文本上来了。 现在有预训练好的中文词向量，但这不是本文的重点，因为这里不需要用到预训练好的词向量，本文的文本分类是基于字符级的CNN实现，也就是并没有对文本数据做分词处理，而是从原始文本中建立词汇表，然后把文本中的每个字符都对应编码。比如，“我爱北京天安门。”，我们就会把这段文本全部打散成为“我”、“爱”、“北”、“京”、“天”、“安”、“门”、“。”，甚至标点符号、特殊字符都会有对应的编码，一开始还有怀疑，不过从模型的表现来看，真香。 本文用到的数据集来自搜狗实验室（Sogou Labs）提供的新闻数据，涵盖了国内，国际，体育，社会，娱乐等18个频道的新闻数据，不过数据集的质量不是特别高，存在大量的分类不清晰、文不对题数据。限于单机的性能，又没有 GPU，所以我只下载了精简版的一个月数据，大约 347M。用 sublime 打开原始数据是乱码的，解决的方案见：如何解决Sublime Text 3不能正确显示中文的问题 - 冷编程 - SegmentFault 思否，然后，打开长这样： xml的格式，gbk编码，每一个&lt;doc&gt;与&lt;/doc&gt;之间是一篇单独的新闻，包含URL、文档编号、标题和正文，其中新闻的分类类别在URL的子域名中，如sports代表体育，house代表房产等等，所以本文只需要拿到URL和content之间的内容就行。 2. 代码解析OK，背景介绍差不多到这里就结束了，下面是代码实现的解读： 2.1 数据清洗下载下来的原始数据分为129个TXT文件，每个文件中包含有不同类别的新闻数据，我要做的是遍历每个文件，然后把相同类别的新闻提取出来并写入新的文件中。读取中文txt文档乱码依旧是个头疼的问题，话说我至今仍然弄不是很清楚Python的编码模式。读取完成后，提取出URL和content，我们还需要用正则表达式把子域名拿出来，最后得到15个按类别分好的文件，最终留下了11个类别。 123456789101112131415161718192021222324252627282930313233343536373839404142#!/usr/bin/python# coding: utf-8import osimport redef _read_file(txt_file): """读取txt文件""" return open(txt_file, 'rb').read().decode("gbk", 'ignore')def extract_class_content(doc): """提取分类和内容""" url = doc.split('&lt;url&gt;')[1].split('&lt;/url&gt;')[0] content = doc.split('&lt;content&gt;')[1].split('&lt;/content&gt;')[0] category = re.findall(r"http://(.*?).sohu.com/", url) return category[0], contentdef file_writer(category, content): dir_name = '/home/libin/data/' path = os.path.join(dir_name, category) f = open(path, 'a', encoding='utf-8') f.write(category + '\t' + content + '\n') f.close()def category_data(txt_file): """将每个文件中不同类别的新闻分别存储""" f = _read_file(txt_file) docs_xmls = f.split('&lt;doc&gt;\n') for doc in docs_xmls: if doc: category, content = extract_class_content(doc) file_writer(category, content)if __name__ == '__main__': for file in os.listdir('/home/libin/data/SogouCS.reduced/'): file_path = os.path.join('/home/libin/data/SogouCS.reduced', file) category_data(file_path) 然后，看看各个类别下数据量的分布，发现体育、商业、新闻的数量较多，文化类的比较少，数据分布不太平衡，但这并不影响，因为我们并不会用到全部的数据，而是从每个类别中抽取一部分来训练模型。 1234567891011 7241 auto61843 business 3291 cul 5482 health12353 it10673 learning 2930 mil.news82740 news85984 sports 8957 travel33091 yule 2.2 准备数据数据清洗完成后，下一步就是为模型准备训练集、验证集和测试集数据， 训练集和测试集按照4:1的比例分配。 1234567891011121314151617181920212223242526def save_file(dir_name): f_train = open('../data/news_train.txt', 'w', encoding='utf-8') f_test = open('../data/news_test.txt', 'w', encoding='utf-8') f_val = open('../data/news_val.txt', 'w', encoding='utf-8') for category in os.listdir(dir_name): cat_file = os.path.join(dir_name, category) fp = _read_file(cat_file) count = 0 for line in fp: category, content = _unpack_line(line) if category and content: if count &lt; 400: f_train.write(category + '\t' + content + '\n') elif count &lt; 500: f_test.write(category + '\t' + content + '\n') elif count &lt; 550: f_val.write(category + '\t' + content + '\n') else: break count += 1 print('Finished', category) f_train.close() f_test.close() f_val.close() 考虑到单机的计算能力，一开始我没有抽取太多的数据，仅仅从每个类别抽取400条作为训练集，100条作为测试集，50条作为验证集。第一组数据测试训练模型时，物理内存占用率并不高，CPU才是最占用计算资源的。下图中显示CPU已经在超负荷工作，而内存无动于衷，在这个基础上开始试着增大数据量。 构建词汇表：vocab上述的准备工作做完了之后，数据的准备并没有结束，因为我们还没有为字符-&gt;向量做好铺垫，通常的做法是加入已经训练好的词向量（比如，这个链接归纳总结的预训练好的词向量就比较全）。在本文呢我没有用它们，而是筛选出的训练集语料中出现频次较高的5000个字符作为词汇表，我比较好奇的是我并没有对原始语料做任何的清洗、去噪，却丝毫不影响分类器的表现。添加一个 &lt;PAD&gt;来将所有文本pad为同一长度 12345678910111213def build_vocab(train_path, vocab_path, vocab_size=5000): """构建词汇表""" data_train, _ = read_file(train_path) all_data = [] for content in data_train: all_data.extend(content) counter = Counter(all_data) counter_pairs = counter.most_common(vocab_size-1) words, _ = list(zip(*counter_pairs)) words = ['&lt;PAD&gt;'] + list(words) open_file(vocab_path, mode='w').write('\n'.join(words) + '\n') 提取出来的词汇表长这样，停用词、标点符号居多。 词汇表建立好了，txt文件并不适合查询，所以这里用字符在文件的顺序作为其标识的id，存储到字典word_to_id中，这样以来就方便查找了。 12345def read_vocab(vocab_path): with open(vocab_path) as f: words = [_.strip() for _ in f.readlines()] word_to_id = dict(zip(words, range(len(words)))) return words, word_to_id 类别编码（因变量）1234def read_category(): categories = ['mil.news', 'cul', 'health', 'travel', 'auto', 'learning', 'it', 'yule', 'sports', 'business', 'news'] cat_to_id = dict(zip(categories, range(len(categories)))) return categories, cat_to_id 处理数据做完构建词汇表、类别转换为one-hot编码的准备工作，终于要进入正题了，数据进入模型训练、验证、测试前的准备工作还没有做。下面，process_file()函数首先读取数据文件，将正文和标签分别对应存储在contents和labels两个列表中，然后再处理contents中的每一段文本，把文本中每一个字符在词汇表中找到其对应的id，完成文本数值化操作。类别转换为one-hot表示：y_pad = kr.utils.to_categorical(label_id, num_classes=len(cat_to_id))。 12345678910111213def process_file(file_name, word_to_id, cat_to_id, max_length=600): contents, labels = read_file(file_name) data_id, label_id = [], [] for i in range(len(contents)): data_id.append([word_to_id[x] for x in contents[i] if x in word_to_id]) label_id.append(cat_to_id[labels[i]]) # 使用keras提供的pad_sequences来将文本pad为固定长度 x_pad = kr.preprocessing.sequence.pad_sequences(data_id, max_length) y_pad = kr.utils.to_categorical(label_id, num_classes=len(cat_to_id)) # 将标签转换为one-hot表示 return x_pad, y_pad 2.3 CNN模型设置CNN参数设置 区别于传统的机器学习，现有任务下，一般的深度学习即使没有经过参数调节也可以达到不错的效果，可见其强大之处。由于上述的原因，往往深度学习也被诟病为“黑箱操作”，因为它比较难以理解，比如对于不太了解深度学习的人，从字符-&gt;向量转化过程的理解就比较困难，字符怎么就可以转化成为可以计算的数值呢？就算字符的向量化过程完成了，当有新的数据进入训练模型，它们又是如何从已有的词汇表中匹配到对应的向量？这些都是需要考虑的问题…… 解释一下CNN常见的配置参数： seq_length是输入矩阵的宽度，由输入数据的长度决定，考虑到新闻长度会很长，所以我把矩阵的宽度设置为1000 embedding_dim，词向量的宽度，即由现有语料训练得到字向量的宽度，默认设置为64 num_classes则根据你实际的类别来设定，设置为11 dropout_keep_prob是dropout的比例，一般设置为0.5 num_epochs全部数据通过神经网络的次数，决定经过多少轮后停止训练，我在模型中设置为10，实际中有可能没有到10轮就停止了 1234567891011121314151617181920class TCNNConfig(object): """CNN配置参数""" embedding_dim = 64 # 词向量宽度 seq_length = 1000 # 输入矩阵的宽度 num_classes = 11 # 类别数 num_filters = 256 # 卷积核数目 kernel_size = 5 # 卷积核尺寸，即卷积核覆盖的词汇数量 vocab_size = 6000 # 词汇表大小 hidden_dim = 128 # 全连接层神经元 dropout_keep_prob = 0.5 # dropout保留比例 learning_rate = 1e-3 # 学习率 batch_size = 64 # 每批训练大小 num_epochs = 10 # 总迭代轮次 print_per_batch = 100 # 每多少轮输出一次结果 save_per_batch = 10 # 每多少轮存入tensorboard 文本分类模型 tf.placeholder()是创建占位符，给输入数据腾出空间，第二个参数是占位符的形状，设置为None是为了使模型可以接受任意数量的数据。self.input_x = tf.placeholder(tf.int32, [None, self.config.seq_length], name=&#39;input_x&#39;)代表创建大小为[None, seq_length]的空间，其中这个空间中的每一行代表了一条输入数据，在CNN模型中我们将其设置为1000，表示只取文本的前1000个字符，之后会用字符在词汇表中的id来对self.input_x填充。 网络的第一层是嵌入层，将词汇映射到低维向量，设置为64，嵌入操作tf.nn.embedding_lookup(embedding, self.input_x)完成后，输出结果是3D张量，形如[None, sequence_length, embedding_dim]，对应了下图： 嵌入操作完成后，紧接着便是卷积和池化层，卷积核大小设置为5，表示卷积核每次扫过5个字符，一共有256个卷积核，然后对卷积核生成的feature map做最大池化，池化之后便是第一个全连接层，计算之后dropout掉一些元素，接着是修正线性单元激活函数和softmax层，最后返回softmax层最大值的索引，即预测类别的id。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950class TextCNN(object): """文本分类，CNN模型""" def __init__(self, config): self.config = config # 三个待输入的数据，腾出占位符 # input_x 为 n * seq_length 的矩阵，n 大小不固定 # input_y 同 self.input_x = tf.placeholder(tf.int32, [None, self.config.seq_length], name='input_x') self.input_y = tf.placeholder(tf.float32, [None, self.config.num_classes], name='input_y') self.keep_prob = tf.placeholder(tf.float32, name='keep_prob') self.cnn() def cnn(self): """CNN模型""" # 词向量映射 with tf.device('/cpu:0'): # 强制使用CPU embedding = tf.get_variable('embedding', [self.config.vocab_size, self.config.embedding_dim]) embedding_inputs = tf.nn.embedding_lookup(embedding, self.input_x) with tf.name_scope("cnn"): # CNN layer conv = tf.layers.conv1d(embedding_inputs, self.config.num_filters, self.config.kernel_size, name='conv') # global max pooling layer gmp = tf.reduce_max(conv, reduction_indices=[1], name='gmp') with tf.name_scope("score"): # 全连接层，后面接dropout以及relu激活 # 激活函数后得到第二个全连接层 fc = tf.layers.dense(gmp, self.config.hidden_dim, name='fc1') fc = tf.contrib.layers.dropout(fc, self.keep_prob) fc = tf.nn.relu(fc) # 修正线性单元激活函数，大于零才被激活 # 分类器 self.logits = tf.layers.dense(fc, self.config.num_classes, name='fc2') self.y_pred_cls = tf.argmax(tf.nn.softmax(self.logits), 1) # 预测类别，返回最大值的索引 with tf.name_scope("optimize"): # 损失函数，交叉熵 cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(logits=self.logits, labels=self.input_y) self.loss = tf.reduce_mean(cross_entropy) # 优化器 self.optim = tf.train.AdamOptimizer(learning_rate=self.config.learning_rate).minimize(self.loss) with tf.name_scope("accuracy"): # 准确率 correct_pred = tf.equal(tf.argmax(self.input_y, 1), self.y_pred_cls) self.acc = tf.reduce_mean(tf.cast(correct_pred, tf.float32)) 2.4 训练和测试123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122def train(): print("Configuring TensorBoard and Saver...") # 配置 Tensorboard，重新训练时，请将tensorboard文件夹删除，不然图会覆盖 tensorboard_dir = 'tensorboard/textcnn' if not os.path.exists(tensorboard_dir): os.makedirs(tensorboard_dir) tf.summary.scalar("loss", model.loss) tf.summary.scalar("accuracy", model.acc) merged_summary = tf.summary.merge_all() writer = tf.summary.FileWriter(tensorboard_dir) # 配置 Saver saver = tf.train.Saver() if not os.path.exists(save_dir): os.makedirs(save_dir) print("Loading training and validation data...") # 载入训练集与验证集 start_time = time.time() x_train, y_train = process_file(train_dir, word_to_id, cat_to_id, config.seq_length) x_val, y_val = process_file(val_dir, word_to_id, cat_to_id, config.seq_length) time_dif = get_time_dif(start_time) print("Time usage:", time_dif) # 创建session session = tf.Session() session.run(tf.global_variables_initializer()) writer.add_graph(session.graph) print('Training and evaluating...') start_time = time.time() total_batch = 0 # 总批次 best_acc_val = 0.0 # 最佳验证集准确率 last_improved = 0 # 记录上一次提升批次 require_improvement = 1000 # 如果超过1000轮未提升，提前结束训练 flag = False for epoch in range(config.num_epochs): print('Epoch:', epoch + 1) batch_train = batch_iter(x_train, y_train, config.batch_size) for x_batch, y_batch in batch_train: feed_dict = feed_data(x_batch, y_batch, config.dropout_keep_prob) if total_batch % config.save_per_batch == 0: # 每多少轮次将训练结果写入tensorboard scalar s = session.run(merged_summary, feed_dict=feed_dict) writer.add_summary(s, total_batch) if total_batch % config.print_per_batch == 0: # 每多少轮次输出在训练集和验证集上的性能 feed_dict[model.keep_prob] = 1.0 loss_train, acc_train = session.run([model.loss, model.acc], feed_dict=feed_dict) loss_val, acc_val = evaluate(session, x_val, y_val) # todo if acc_val &gt; best_acc_val: # 保存最好结果 best_acc_val = acc_val last_improved = total_batch saver.save(sess=session, save_path=save_path) improved_str = '*' else: improved_str = '' time_dif = get_time_dif(start_time) msg = 'Iter: &#123;0:&gt;6&#125;, Train Loss: &#123;1:&gt;6.2&#125;, Train Acc: &#123;2:&gt;7.2%&#125;,' \ + ' Val Loss: &#123;3:&gt;6.2&#125;, Val Acc: &#123;4:&gt;7.2%&#125;, Time: &#123;5&#125; &#123;6&#125;' print(msg.format(total_batch, loss_train, acc_train, loss_val, acc_val, time_dif, improved_str)) session.run(model.optim, feed_dict=feed_dict) # 运行优化 total_batch += 1 if total_batch - last_improved &gt; require_improvement: # 验证集正确率长期不提升，提前结束训练 print("No optimization for a long time, auto-stopping...") flag = True break # 跳出循环 if flag: # 同上 breakdef test(): print("Loading test data...") start_time = time.time() x_test, y_test = process_file(test_dir, word_to_id, cat_to_id, config.seq_length) session = tf.Session() session.run(tf.global_variables_initializer()) saver = tf.train.Saver() saver.restore(sess=session, save_path=save_path) # 读取保存的模型 print('Testing...') loss_test, acc_test = evaluate(session, x_test, y_test) msg = 'Test Loss: &#123;0:&gt;6.2&#125;, Test Acc: &#123;1:&gt;7.2%&#125;' print(msg.format(loss_test, acc_test)) batch_size = 128 data_len = len(x_test) num_batch = int((data_len - 1) / batch_size) + 1 y_test_cls = np.argmax(y_test, 1) y_pred_cls = np.zeros(shape=len(x_test), dtype=np.int32) # 保存预测结果 for i in range(num_batch): # 逐批次处理 start_id = i * batch_size end_id = min((i + 1) * batch_size, data_len) feed_dict = &#123; model.input_x: x_test[start_id:end_id], model.keep_prob: 1.0 &#125; y_pred_cls[start_id:end_id] = session.run(model.y_pred_cls, feed_dict=feed_dict) # 评估 print("Precision, Recall and F1-Score...") print(metrics.classification_report(y_test_cls, y_pred_cls, target_names=categories)) # 混淆矩阵 print("Confusion Matrix...") cm = metrics.confusion_matrix(y_test_cls, y_pred_cls) print(cm) time_dif = get_time_dif(start_time) print("Time usage:", time_dif) 3. 模型训练测试结果经过10轮的训练，训练集的准确率为98.4%，验证集的最佳准确率94.23%，可能跟数据集较小的缘故，训练收敛得比较快，并且仅仅用了11分钟。 模型在测试集的表现也尚可，达到了94.15%，除了文化类，其他类别的新闻预测准确率都达到了90%以上，召回率也表现不错。 训练过程准确率和损失的可视化结果可以在tensorboard中查看，命令行输入：tensorboard --logdir path/to/eventfile（是文件夹目录）。 参考资源 Chinese Word Vectors：目前最全的中文预训练词向量集合 | 机器之心 新闻上的文本分类：机器学习大乱斗 - 知乎 gaussic/text-classification-cnn-rnn: CNN-RNN中文文本分类，基于TensorFlow]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>卷积神经网络</tag>
        <tag>TensorFlow</tag>
        <tag>textcnn</tag>
        <tag>搜狗新闻数据</tag>
        <tag>文本分类</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flask 入门（7）：添加文章编辑器]]></title>
    <url>%2F2018%2Flearn-flask-7%2F</url>
    <content type="text"><![CDATA[添加富文本控件CKeditor 是一款“所见即所得”的富文本编辑器，在 Flask 项目中添加 CKeditor 很方便，在 layout.html文件的&lt;script&gt;标签中加入 cdn 的地址即可（这部分涉及到 bootstrap 方面的知识，暂时不太了解）。 12&lt;script src="//cdn.ckeditor.com/4.4.6/standard/ckeditor.js"&gt;&lt;/script&gt;&lt;script type="text/javascript"&gt;CKEDITOR.replace('editor')&lt;/script&gt; Flask项目集成富文本编辑器CKeditor - digwtx - SegmentFault 思否 新功能开发流程在学习教程的过程中，我总结了一套 Flask 的开发流程，假设我们需要给网站添加新功能，遵循以下流程： 编写视图函数，添加路由，在后端处理与数据库的交互，比如增加、删除、修改、删除数据 编写 HTML 文件，我们在后端把数据处理好了，需要把它们传递到前端，此时需要为该新功能新建 HTML 页面，此时会涉及到一些前端的知识 总的来说，添加新功能就是前后端交互的过程，后端负责数据处理，前端负责数据展示。下面，以给网站添加“新建博文”功能为例： 编写视图函数（后端）首先，新建路由，新建\add_blog页面，但此时访问是 404，因为add_blog.html还没写好。我们先把数据处理的逻辑写好，Flask 的 request 会从浏览器那里接受到用户输入的数据 Form，验证通过之后，我们将它写入数据库。 123456789101112131415161718@app.route('/add_blog', methods=['GET', 'POST'])@is_logged_indef add_blog(): form = ArticleForm(request.form) if request.method == 'POST' and form.validate(): title = form.title.data body = form.title.data cur = mysql.connection.cursor() cur.execute("insert into articles(title, body, author) values(%s, %s, %s)", (title, body, session['username'])) mysql.connection.commit() cur.close() flash('文章创建成功', 'success') return redirect(url_for('dashboard')) return render_template('add_blog.html', form=form) 编写 HTML 文件（前端）然后，是 HTML 文件的编写，它主要负责接受用户从浏览器输入的数据 Form 123456789101112131415&#123;% extends 'layout.html' %&#125;&#123;% block body %&#125; &lt;h1&gt;添加文章&lt;/h1&gt; &#123;% from 'include/_formhelpers.html' import render_field %&#125; &lt;form method="POST" action=""&gt; &lt;div class="form-group"&gt; &#123;&#123; render_field(form.title, class_='form-control') &#125;&#125; &lt;/div&gt; &lt;div class="form-group"&gt; &#123;&#123; render_field(form.body, class_='form-control', id='editor') &#125;&#125; &lt;/div&gt; &lt;p&gt;&lt;input class="btn btn-primary" type="submit" value="新建文章"&gt;&lt;/p&gt; &lt;/form&gt;&#123;% endblock %&#125; 效果如下： 这里实现的是新建文章的功能，剩下的删除、修改就不说了，步骤都差不太多。至此，Flask 入门学习基本上完成了，demo 能实现网站的基本功能，登陆、注册、数据增删改功能。 （完）]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>flask</tag>
        <tag>网络框架</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flask 入门（6）：登陆注销机制]]></title>
    <url>%2F2018%2Flearn-flask-6%2F</url>
    <content type="text"><![CDATA[本文主要内容为登陆和注销两种状态下的权限，一般的，我们不希望授予未登陆的用户查看网站内容的权限。这里用到了 Flask 中一种极为简单的认证方法Simple Authorization | Flask (A Python Microframework)，snippets 是一个装饰器，使用方法超级简单，只需要把下面这段代码中的get_current_user_role()替换成你想要的判断逻辑，然后在加入到对应有权限限制的视图函数即可。 1234567891011from functools import wrapsdef requires_roles(*roles): def wrapper(f): @wraps(f) def wrapped(*args, **kwargs): if get_current_user_role() not in roles: return error_response() return f(*args, **kwargs) return wrapped return wrapper 下面是修改后的例子，如果登陆成功则该干嘛干嘛，否则跳转到登陆页面，提示需要登陆。 123456789def is_logged_in(f): @wraps(f) def wrapped(*args, **kwargs): if 'logged_in' in session: return f(*args, **kwargs) else: flash('尚未登陆，请登陆', 'danger') return redirect(url_for('login')) return wrapped]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>flask</tag>
        <tag>网络框架</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flask 入门（5）：添加登陆页面]]></title>
    <url>%2F2018%2Flearn-flask-5%2F</url>
    <content type="text"><![CDATA[在上一篇文章中，我们完成了注册页面的设计，在这篇笔记中将记录登陆页面的设计过程。登陆页面主要涉及到密码验证、信息提示。 登陆过程的逻辑设计，拿到用户的登陆名和密码，从数据库中查找到对应的信息进行比对，分为三种情形：账号不存在、密码不正确和密码正确。登陆逻辑处理完毕后，再着手对登陆页面login.html的设计。 12345678910111213141516171819202122232425262728293031@app.route('/login', methods=['GET', 'POST'])def login(): if request.method == 'POST': username = request.form['username'] password_candidate = request.form['password'] cur = mysql.connection.cursor() result = cur.execute("select * from users where username = %s ", [username]) if result: data = cur.fetchone() password = data['password'] if sha256_crypt.verify(password_candidate, password): app.logger.info('密码正确') session['logged_in'] = True session['username'] = username flash('你已经成功登陆', 'success') else: app.logger.info('密码不正确') error = '登陆失败，密码不正确' return render_template('login.html', error=error) else: app.logger.info('账号不存在') error = '账号不存在' return render_template('login.html', error=error) cur.close() return render_template('login.html') 设计登陆页面，一般有现成的模板，直接拿过来用即可。 12345678910111213141516&#123;% extends 'layout.html' %&#125;&#123;% block body %&#125; &lt;h1&gt;登陆&lt;/h1&gt; &lt;form action="" method="POST"&gt; &lt;div class="form-group"&gt; &lt;label&gt;用户名&lt;/label&gt; &lt;input type="text" name="username" class="form-control" value=&#123;&#123; request.form.username &#125;&#125;&gt; &lt;/div&gt; &lt;div class="form-group"&gt; &lt;label&gt;密码&lt;/label&gt; &lt;input type="password" name="password" class="form-control" value=&#123;&#123; request.form.password &#125;&#125;&gt; &lt;/div&gt; &lt;button type="submit" class="btn btn-primary"&gt;提交信息&lt;/button&gt; &lt;/form&gt;&#123;% endblock %&#125; 然后，我们还要对登陆的结果给予反馈，这里还新建了_messager.html 1234567891011121314151617&#123;% with messages = get_flashed_messages(with_categories=true) %&#125; &#123;% if messages %&#125; &lt;ul class=flashes&gt; &#123;% for category, message in messages %&#125; &lt;div class="alert alert-&#123;&#123; category &#125;&#125;"&gt;&#123;&#123; message &#125;&#125;&lt;/div&gt; &#123;% endfor %&#125; &lt;/ul&gt; &#123;% endif %&#125;&#123;% endwith %&#125;&#123;% if error %&#125; &lt;div class="alert alert-danger"&gt;&#123;&#123; error &#125;&#125;&lt;/div&gt;&#123;% endif %&#125;&#123;% if msg %&#125; &lt;div class="alert alert-success"&gt;&#123;&#123; msg &#125;&#125;&lt;/div&gt;&#123;% endif %&#125;]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>flask</tag>
        <tag>网络框架</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flask 入门（4）：添加注册页面]]></title>
    <url>%2F2018%2Flearn-flask-4%2F</url>
    <content type="text"><![CDATA[1. 前期准备创建用户表123456create table users (id INT(11) AUTO_INCREMENT PRIMARY KEY, name VARCHAR(100), email VARCHAR(100), username VARCHAR(100), password VARCHAR(100), register_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP); 安装 Flask 的依赖包123pip install flask-mysqldb # flask 的 MySQL 依赖包pip install flask-wtf # 验证框依赖包pip install passlib # 密码加密 2. 设计注册页面123456789101112131415161718192021222324&#123;% extends 'layout.html' %&#125;&#123;% block body %&#125; &lt;h1&gt;注册&lt;/h1&gt; &#123;% from 'include/_formhelpers.html' import render_field %&#125; &lt;form method="POST" action=""&gt; &lt;div class="form-group"&gt; &#123;&#123; render_field(form.name, class_='form-control') &#125;&#125; &lt;/div&gt; &lt;div class="form-group"&gt; &#123;&#123; render_field(form.email, class_='form-control') &#125;&#125; &lt;/div&gt; &lt;div class="form-group"&gt; &#123;&#123; render_field(form.username, class_='form-control') &#125;&#125; &lt;/div&gt; &lt;div class="form-group"&gt; &#123;&#123; render_field(form.password, class_='form-control') &#125;&#125; &lt;/div&gt; &lt;div class="form-group"&gt; &#123;&#123; render_field(form.confirm, class_='form-control') &#125;&#125; &lt;/div&gt; &lt;p&gt;&lt;input type="submit", class="btn btn-primary" value="提交信息"&gt;&lt;/p&gt; &lt;/form&gt;&#123;% endblock %&#125; 3. 注册页面视图函数初始化 MySQL 配置信息 123456app.config['MYSQL_HOST'] = 'localhost'app.config['MYSQL_USER'] = 'root'app.config['MYSQL_PASSWORD'] = ''app.config['MYSQL_DB'] = 'myflaskapp'app.config['MYSQL_CURSORCLASS'] = 'DictCursor'mysql = MySQL(app) 首先，定义登录信息格式的类，如姓名、用户名、密码长度，密码确认以及确认密码，然后，创建视图函数，接入从页面通过 POST 方法传递过来的参数，并写入数据库中。 1234567891011121314151617181920212223class RegisterForm(Form): name = StringField('姓名', [validators.Length(min=1,max=50)]) username = StringField('用户名', [validators.Length(min=4,max=25)]) email = StringField('电子邮箱', [validators.Length(min=6,max=50)]) password = PasswordField('登录密码', [validators.DataRequired(), validators.EqualTo('confirm', message='Password not match!')]) confirm = PasswordField('确认登录密码')@app.route('/register', methods=['GET', 'POST'])def register(): form = RegisterForm(request.form) if request.method == 'POST' and form.validate(): name = form.name.data email = form.email.data username = form.username.data password = sha256_crypt.encrypt(str(form.password.data)) cur = mysql.connection.cursor() cur.execute("insert into users(name, email, username, password) values (%s, %s, %s, %s)", (name, email, username, password)) mysql.connection.commit() cur.close() return render_template('register.html', form=form) 完成上述步骤后，刷新页面，填写注册信息，点击“提交信息”，数据成功写入表中。]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>flask</tag>
        <tag>网络框架</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ubuntu 下安装卸载 MySQL]]></title>
    <url>%2F2018%2Finstall-uninstall-mysql-ubuntu%2F</url>
    <content type="text"><![CDATA[Ubuntu 下如何安装、卸载 MySQL，以及添加 Python3 支持。 安装12sudo apt-get updatesudo apt-get install mysql-server 然后，启动 MySQL 服务：systemctl start mysql 进入 MySQL ：mysql -uroot -p 为 MySQL 设置密码：UPDATE mysql.user SET Password = PASSWORD(&#39;password&#39;) WHERE User = &#39;root&#39;; 参考链接：Install MySQL Server on Ubuntu 卸载1234sudo apt-get purge mysql-server mysql-client mysql-common mysql-server-core-* mysql-client-core-*sudo rm -rf /etc/mysql /var/lib/mysqlsudo apt-get autoremovesudo apt-get autoclean 参考：Uninstall or Completely remove mysql from ubuntu 16-04 | Linux Scripts Hub 在 Python 3 中支持 MySQL在 Python3 中通过 import MySQLdb连接 MySQL，然而安装 MySQLdb 却不是 pip install MySQLdb，正确的做法是： 123sudo apt-get updatesudo apt-get install python3-dev libmysqlclient-devsudo pip3 install mysqlclient 然后， 123456pi@raspberrypi:~ $ python3.5Python 3.5.2 (default, Dec 15 2017, 15:32:37)[GCC 4.9.2] on linuxType &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.&gt;&gt;&gt; import MySQLdb&gt;&gt;&gt; 参考：pi 3 - How to connect MySQLdb in Python 3? - Raspberry Pi Stack Exchange]]></content>
      <categories>
        <category>教程</category>
      </categories>
      <tags>
        <tag>Ubuntu</tag>
        <tag>MySQL</tag>
        <tag>MySQLdb</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flask 入门（3）：动态路由]]></title>
    <url>%2F2018%2Flearn-flask-3%2F</url>
    <content type="text"><![CDATA[假如现在你有很多文章，每一篇文章都有编号，那么为每一篇文章单独写一个函数不太现实，Flask 中的动态路由允许你通过参数动态生成页面。 123@app.route('/blog/&lt;string:id&gt;')def article(id): return render_template('article.html', id=id, blogs=blogs) 123456789101112131415&#123;% extends 'layout.html' %&#125;&#123;% block body %&#125; &lt;h1&gt;这是第&#123;&#123; id &#125;&#125;篇文章。&lt;/h1&gt; &lt;ul class="list-group"&gt; &#123;% for blog in blogs %&#125; &#123;% set aid = blog.id | string %&#125; &#123;% if id == aid %&#125; &lt;li class="list-group-item"&gt;&#123;&#123; blog.title &#125;&#125;&lt;/li&gt; &lt;li class="list-group-item"&gt;&#123;&#123; blog.author &#125;&#125;&lt;/li&gt; &lt;li class="list-group-item"&gt;&#123;&#123; blog.body &#125;&#125;&lt;/li&gt; &#123;% endif %&#125; &#123;% endfor %&#125; &lt;/ul&gt;&#123;% endblock %&#125;]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>flask</tag>
        <tag>网络框架</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flask 入门（2）：添加导航条]]></title>
    <url>%2F2018%2Flearn-flask-2%2F</url>
    <content type="text"><![CDATA[给网站添加导航条。 Bootstrap 官网有很多现成的例子：Examples · Bootstrap，在这个例子中，我的目的要到达在网站中加入一个导航条，并修改导航条的链接文字。 首先，新建空白页面，这再简单不过了，运行代码，这个页面非常简单，首页只有一行“Hello, World” 12345678910from flask import Flask, render_templateapp = Flask(__name__)@app.route('/')def hello_world(): return 'Hello, World!'if __name__ == '__main__': app.run(debug=True) 在搭建网站中，现有的框架已经很成熟，我们不需要为了实现一个功能而从头开始造轮子，直接调用即可，节省了大量的开发时间。比如，Bootstrap 是一个强大的前端框架，里面有很多组件即拿即用。 然而，仅仅返回“Hello, World!”字符串远远不能满足需求，网页中实现各种特效还得依靠 HTML 和 CSS 这些标记语言，Flask 的 render_template() 方法，字面上即“提交模板”，我们在模板中设计好样式后，Flask 再将完整的 HTML 文件提交给浏览器展示。 在 templates 文件夹下新建 index.html 作为主页， 12345678&#123;% extends 'layout.html' %&#125;&#123;% block body %&#125; &lt;div class="jumbotron text-center"&gt; &lt;h1&gt;这是一个 Flask 示例网站！&lt;/h1&gt; &lt;p class="lead"&gt;Flask 是一个快速的网络框架！&lt;/p&gt; &lt;/div&gt;&#123;% endblock %&#125; layout.html 作为主页的样式模板，还要在 templates 文件夹下新建 includes文件夹，把导航条的代码（按住 F12 查看网页源代码）复制到includes/_navibar.html文件中。 123456789101112131415&lt;!DOCTYPE html&gt;&lt;html lang="en"&gt;&lt;link&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;My Flask APP&lt;/title&gt; &lt;link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.1.3/css/bootstrap.min.css"&gt;&lt;/link&gt;&lt;/head&gt;&lt;body&gt; &#123;% include 'includes/_navibar.html' %&#125; &lt;div class="container"&gt; &#123;% block body %&#125;&#123;% endblock %&#125; &lt;/div&gt; &lt;script src="https://stackpath.bootstrapcdn.com/bootstrap/4.1.3/css/bootstrap.min.css"&gt;&lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 最后，还需要修改 app 的代码： 12345678910from flask import Flask, render_templateapp = Flask(__name__)@app.route('/')def hello_world(): return render_template('index.html')if __name__ == '__main__': app.run(debug=True) 整个执行的过程是，Flask 先访问 index.html 文件，然后该文件的样式由 layout.html 决定，layout.html中再引入导航条的源代码，最后实现的效果。]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>flask</tag>
        <tag>网络框架</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flask 入门（1）：概念]]></title>
    <url>%2F2018%2Flearn-flask-1%2F</url>
    <content type="text"><![CDATA[1. 最小的应用123456from flask import Flaskapp = Flask(__name__)@app.route('/')def hello_world(): return 'Hello, World!' 这段代码发生了什么？ 首先，导入 Flask 类 创建名为 app 的 Flask 实例 将构造函数的 name 参数传给 Flask 程序,这一点可能会让 Flask 开发新手心生迷惑。Flask 用这个参数决定程序的根目录,以便稍后能够找到相对于程序根目录的资源文件位置。 route，即路由，处理 URL 和函数之间关系的程序称为路由。@app.route(&#39;/&#39;)是一个装饰器，它告诉它所装饰的函数hello_world()要触发的 URL hello_world()函数用于生成页面，并返回我们要展示在浏览器上的信息 浏览器向服务器发送请求，服务器再把请求发送给 flask，flask 把返回的值再传到浏览器。 2. 启动程序并进入调试启动程序 123$ export FLASK_APP=hello.py$ flask run * Running on http://127.0.0.1:5000/ 在命令行进入调试模式，确保每次修改代码后不必重启服务器 12$ export FLASK_ENV=development$ flask run 3. URL 构建函数：url_for()什么，写死 URL？这辈子是不可能把 URL 写死的。 123456789101112131415161718192021222324from flask import Flask, url_forapp = Flask(__name__)@app.route('/')def index(): return 'index'@app.route('/login')def login(): return 'login'@app.route('/user/&lt;username&gt;')def profile(username): return '&#123;&#125;\'s profile'.format(username)with app.test_request_context(): print(url_for('index')) print(url_for('login')) print(url_for('profile', username='John Doe'))//login/user/John%20Doe 3. http 方法路由默认只有 get 方法，可以使用路由装饰器的 methods 参数来处理不同的 HTTP 方法。 4. 静态文件比如说 CSS、js、背景图片这些都是固定的，存储在 static文件夹中，我可以用url_for()函数来访问这些静态文件，比如访问static/style.css就可以通过url_for(&#39;static&#39;, filename=&#39;style.css&#39;)。 5. 渲染模板另外一个非常重要的文件夹是templates，好比就是人的衣柜，它可以让人有不同的装饰风格，flask 的模板引擎是 Jinja。 其他如会话、重定向、响应，目前还用不到，后续补充。]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>flask</tag>
        <tag>网络框架</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[软技能——代码之外的生存指南]]></title>
    <url>%2F2018%2Fsoft-skills-the-software-developer-life-manual%2F</url>
    <content type="text"><![CDATA[这是一位在 33 岁实现财务自由的程序员的个人奋斗史，也是他对于生活和事业的全面总结以及留给后辈的指导。 转变心态转变心态，从签了“卖身契”的打工者变为把自己当做品牌经营的生意人。因此，你需要经常注意： 我能提供什么服务？ 如何提升服务的质量？ 绝大多数人的时间精力有限，专注与某一方面的服务 如何在团队中脱颖而出承担责任！承担责任！承担责任！ 直接：想方设法寻找别人没有注意到的、不愿涉及的领域，比如简化流程、维护文档 间接：帮助其他人解决问题，加速成长 让自己极度透明化，不要怕暴露自己的缺点（《原则》一书中也提到） 自学能力 像专业人士一样思考专业人士有自己的判断，从而他们有敢于说“不”的底气，不然很容易陷入无理要求的陷阱。书中列出了专业人士的几个特点，括号内为对应外行的特点： 遵守自己的原则（让干什么就干什么） 专注于正确完成工作（专注于完成工作） 不惧怕承认自己错了，不会文过饰非（不懂装懂）注：文过饰非，即用漂亮的文字掩盖自己的错误 持续稳定（无法预测，不可靠） 勇于承担责任（回避责任） 看完后我心里一惊，自己多少会有几个括号内的特点，无非就是不敢直面真正的问题。如何成为专业人士？ 良好的习惯 强大的时间管理能力，确定优先级 dare to say no （未完待续…）]]></content>
      <categories>
        <category>思想王国</category>
      </categories>
      <tags>
        <tag>读书笔记</tag>
        <tag>软技能</tag>
        <tag>程序员</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[学习 Flask——尝试用用十步学习法]]></title>
    <url>%2F2018%2Flearn-flask-by-10-steps%2F</url>
    <content type="text"><![CDATA[尝试用十步学习法学习 Flask。 1. 了解全局 阅读背景知识：博客、视频、书籍（目录、大纲） 目的是了解自己对学习主题的了解程度 Flask 是 Python 下一个基于 Werkzeug（WSGI 套件，Web 服务器网关接口）、Jinja 2（模板引擎） 和良好扩展特性的微框架。为什么是“微”？我特地查了一些文档，“Flask 永远不会包含数据库，也不会有表单库或是这方面的其他东西”、“Flask 的理念是为所有应用建立良好的基础，其余的一切都取决于你自己或者扩展”，它很小，能快速应对上线需求，不需要复杂的数据库设计，也就是说 Flask 仅仅充当一个桥梁的架子，至于外观设计（模板）、桥梁流量控制（WSGI）则取决与桥梁设计师。 Flask 是 Python 下的，然后我之前有在老师的指导下用 Flask 写过两个简单的应用，时间久远，印象有点模糊，需查看笔记。另外，实践 Flask 还需要有一些 HTML 和 CSS 的知识，虽然没有系统学过，但问题应该不大，查找文档和在浏览器里打开 F12 调试即可，我在调试自己的博客过程中有一些调节样式的经验。 Flask 的教程有很多，国内国外的都有，国内的培训机构有的会把教学视频发布到腾讯视频，水平参差不齐，很多废话，观看时开两倍速快速过一遍，国外的质量相对较高，但是我还没有确定下看哪个TODO。 2. 确定范围（边界） 拆解：可控、聚焦 时间：在特定的时间能学会什么 在大约一周的时间内，学习 Flask 的基本概念：路由、部署、修改模板引擎、虚拟环境安装、常见问题调试技巧，掌握文档的快速上手部分，掌握如何开发一个简单的网站并上线（登录）。 3. 设定目标 清晰，拒绝含糊不清的描述 目标是具体的，可以实现的 按照教学视频复现一个可以登录注册的导航工具网站（奥森导航），该网站提供为程序员提高生产力的工具。 学完快速上手Quickstart — Flask 1.0.2 documentation 导航页面：把关于 | Thinking Realm里提到的几个链接作为导航，有多简单就多简单，能用就行 博客：能够实现登录注册，新建、修改删除文章功能 将以上的学习、开发过程整理为博客笔记 4. 寻找资源 书籍 博客、在线文档 教学视频 我收集到的资源： 下载电子书：Flask Web开发：基于Python的Web应用开发实战 安装了 pycharm 文档 Tutorial — Flask 1.0.2 documentation：如何用 Flask 搭建简单博客的技术文档；有个培训班老师制作了对应的教学视频，Mars 老师，共 17 讲 Quickstart — Flask 1.0.2 documentation：官方 Flask 快速入门文档，涉及到 Flask 中的一些常见概念，如路由、静态文件、模板等等。有两个中文翻译版本 欢迎来到 Flask 的世界 — Flask 1.0.2 documentation 快速入门 — Flask 0.10.1 文档 Deployment Options — Flask 1.0.2 documentation：如何部署 Flask？ 知乎问答：怎样才能彻底掌握Flask？怎样的学习顺序比较合理？ - 知乎 5. 创建学习计划通过阅读书籍和教学视频，观察别人的学习计划，定制自己的学习计划。 学完快速上手Quickstart — Flask 1.0.2 documentation：预计 1 hour，整理成学习笔记发布到博客 观看 Mars 老师的教学视频 6. 筛选资源由第 5 步的计划筛选第 4 步收集的资源 （未完待续……）]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>软技能</tag>
        <tag>程序员</tag>
        <tag>flask</tag>
        <tag>网络框架</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux 工具箱系列（1）使用当前日期创建目录]]></title>
    <url>%2F2018%2Fcreate-directory-by-local-time%2F</url>
    <content type="text"><![CDATA[使用当前日期创建目录 注意是`不是’ 1mkdir `date --iso`]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>创建目录</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习算法系列（17）k 近邻]]></title>
    <url>%2F2018%2Fmachine-learning-algorithm-series-knn%2F</url>
    <content type="text"><![CDATA[k 近邻（knn）是一种很简单的分类算法，它的基本原理是：给定一个实例点，然后在所有的数据中找到与该实例点距离最近的 k 个样本，最后选择 k 个样本中出现最多的分类标记作为实例点的分类预测结果。 下面这个视频比较清晰地说明了 knn 是如何工作的：How kNN algorithm works - YouTube 基本的原理很简单，所以它很“懒”，是“懒惰学习”代表，它遗留了两个问题给我们思考： k 值如何选择？ k 值的选择不能过小，否则实例点会对自己周边的点异常敏感，容易过拟合 k 值的选取也不能过大，容易产生较大的误差 一般选择一个合适的 k 值，用交叉验证法择优选取 如何定义实例点与样本点之间的距离？ 欧式距离 $L_p$距离，p=1时为曼哈顿距离 闵可夫斯基（Minkowski） 距离 如何快速找到实例点距离最近的 k 个样本？ 关于问题 3 的回答，kd 树是 knn 的实现算法之一，参考【数学】kd 树算法之详细篇]]></content>
      <categories>
        <category>Machine-Learning</category>
      </categories>
      <tags>
        <tag>机器学习算法系列</tag>
        <tag>分类</tag>
        <tag>knn</tag>
        <tag>k近邻</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习算法系列（16）随机森林]]></title>
    <url>%2F2018%2Fmachine-learning-algorithm-series-random-forests%2F</url>
    <content type="text"><![CDATA[为了得到更强的泛化性能，在集成学习中我们期望个体学习器尽可能不相同，因为只有这样才能够全面地概括数据，所以有了 Bagging 这个方法。Bagging是并行式集成学习方法的代表，它基于自助采样法（Bootstrap sampling），给定m个样本的数据集，有放回地抽取一个数据作为数据集，重复m次随机采样得到m个样本的数据集。然后，我们可以将以上的步骤重复T次，得到T个大小为m的随机样本数据集，基于每个样本数据集训练学习器。最后，针对是分类或者回归任务使用投票或者平均来确定最终学习器学习的结果。 随机森林在 Bagging 的基础上做了改进，随机森林会再次对特征做了一次随机选择，比如对于自助采样后的每一个子数据集（总共 m 个子数据集），我们并不会像决策数那样用到所有的特征，随机森林会从所有的特征中随机选择一个包含 k（k&lt;n） 个特征的子集（通常k取log2(n)）。当有一条新数据进来，在随机森林的 m 棵树会各自给出一个答案，如果是分类任务，我们就选择投票法，如果是回归任务则一般选择平均值作为输出。不像决策树，越靠近根节点的特征重要性越高，在随机森林中，在每个特征都是有可能成为“主角”的，也不容易出现过拟合的问题，可以说泛化的优点很明显。 看了很多资料，似乎大家都把随机森林理解得有点复杂，推荐大家观看一个印度人的讲解视频：What is Random Forest Algorithm? A graphical tutorial on how Random Forest algorithm works? - YouTube，很清晰明了（需要翻墙）。]]></content>
      <categories>
        <category>Machine-Learning</category>
      </categories>
      <tags>
        <tag>机器学习算法系列</tag>
        <tag>集成学习</tag>
        <tag>随机森林</tag>
        <tag>bagging</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习算法系列（15）特征工程]]></title>
    <url>%2F2018%2Fmachine-learning-algorithm-series-feature-engineering%2F</url>
    <content type="text"><![CDATA[1. 数据预处理特征缩放（Feature scaling）的定义参见Feature scaling - Wikipedia 无量纲化：不相同规格的特征无法放在一起比较，通常线性转换的手段有： 标准化（Standardization）：转换成标准正态分布 归一化（Normalization）：将特征缩放到相同的区间 最小最大归一化（Rescaling（min-max normalization））：将数据压缩到[0, 1]区间） 均值归一化 优势： 向量单位化：针对特征向量单位化（Scaling to unit length），使其长度为1 定量特征：二值化 定性特征：one-hot编码 分类特征编码 特征组合：基于多项式的、基于指数函数的、基于对数函数 归一化与标准化的比较，参见特征工程中的「归一化」有什么作用？ - 微调的回答 - 知乎 。 2. 特征选择为什么要做特征选择？ 简化模型便于解释 缩短训练时间 避免维度灾难 提高模型的泛化性能，比如通过减少过拟合 一般，特征选择有三个思路： 基于过滤（Filter） 方差检验：去掉那些方差不符合阈值的特征，方差很小代表它对区分样本的作用不大，见sklearn.feature_selection.VarianceThreshold — scikit-learn 0.20.0 documentation 相关性检验：优先选择与目标变量相关性高的特征 相关系数 卡方检验 互信息检验：经验熵与条件熵的差称为互信息（信息增益），信息增益大的特征具有更强的分类能力 基于嵌入（Embedded） 正则化：L1、L2正则化 基于树：随机森林、GBDT 基于包裹（Wrapper）：典型的是递归特征消除算法（recursive feature elimination algorithm） 学习器返回的 coef_ 属性获得每个特征的重要程度 从当前的特征集合中移除最不重要的特征 重复递归这个步骤，直到最终达到所需要的特征数量为止 sklearn.feature_selection.RFE — scikit-learn 0.20.0 documentation 特征选择（5）-递归消除法 | 算法之道]]></content>
      <categories>
        <category>Machine-Learning</category>
      </categories>
      <tags>
        <tag>机器学习算法系列</tag>
        <tag>数据预处理</tag>
        <tag>特征工程</tag>
        <tag>特征选择</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python 工具箱系列 （4）：获取用户名和传入外部参数]]></title>
    <url>%2F2018%2Fget-user-name-and-transfer-parameter%2F</url>
    <content type="text"><![CDATA[1. 获取当前用户信息1234In [1]: import getpassIn [2]: getpass.getuser()Out[2]: &apos;lb&apos; 2. syssys.modulessys.modules是一个全局字典，该字典是python启动后就加载在内存中。每当程序员导入新的模块，sys.modules将自动记录该模块。当第二次再导入该模块时，python会直接到字典中查找，从而加快了程序运行的速度。它拥有字典所拥有的一切方法。 sys.argv有的时候在我们运行不同任务时需要设置不同的参数或者文件路径，这个时候返回脚本修改往往就很麻烦。在 Python 中，sys.argv[] 能帮助从命令行向程序内部传递参数，比如 python task.py file_path，你只需要在task.py脚本中加入以下代码即可，当你执行命令时，file_path 就能传进脚本。 123import sysfile_path = sys.argv[1]]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python工具箱系列</tag>
        <tag>用户名</tag>
        <tag>外部参数</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python 工具箱系列（5）：多个版本 Python 共存]]></title>
    <url>%2F2018%2Fpython2-python3-coexistence%2F</url>
    <content type="text"><![CDATA[查看现有 Python 的版本 12ubuntu@user~$:python --versionPython 2.7 需要删除原有的 Python 连接文件 1$ ubuntu@user~:rm /user/bin/python 然后建立指向 Python3.5 的软连接，然后把 Python2.7 指向 Python2 12$ ubuntu@user~:ln -s /usr/bin/python3.5 /usr/bin/python$ ubuntu@user~:ln -s /usr/bin/python2.7 /usr/bin/python2 现在 Python 的默认版本就是 Python3 了，然后，你还要安装 pip 1sudo apt-get install python3-pip 问题 pip: no module named _internal 解决办法，重新安装： 123wget https://bootstrap.pypa.io/get-pip.py --no-check-certificate# 上面已经将 Python2.7 指向了 Python2sudo python2 get-pip.py 升级 pip 后出错：ImportError: cannot import name &#39;main&#39; 解决办法： 12sudo python -m pip uninstall pipsudo apt-get install python3-pip]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python工具箱系列</tag>
        <tag>Python版本</tag>
        <tag>Python2</tag>
        <tag>Python3</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[从扑克牌的角度理解排序算法（一）]]></title>
    <url>%2F2018%2Funderstanding-sort-algorithm-from-poker-1%2F</url>
    <content type="text"><![CDATA[快速排序快速排序用到了“分而治之”的思想，即先解决小问题，然后再将小问题挨个合并，这种思想在归并排序中也有体现。 想象我们手中有一手未排序的扑克牌，如何用快速排序的方法对它排序呢？首先，需要选择一张牌作为基准（pivot），选择的方式有多种，为了方便起见，就拿第一张作为基准牌。基准牌的作用是用来切割扑克牌，我们约定比它小的全部放在它左边（less），比它大的全部放在它右边（more）。经过第一次的切割之后，基准牌的左边都是比它小的，右边都是比它大的，但是左右两边依然是无序的。 然后仍旧递归地对左边和右边重复以上的操作，直到左边和右边的长度都小于等于1，此时排序结束，返回 left + pivot_list + right。 常见的快速排序递归版本，这个版本的快速排序比较容易理解，但是要求分配额外的内存空间： 12345678910111213141516171819202122def quick_sort(list): less = [] more = [] pivot_list = [] if len(list) &lt;= 1: return list else: pivot = list[0] for item in list: if item &lt; pivot: less.append(item) elif item &gt; pivot: more.append(item) else: pivot_list.append(pivot) # 对左边和右边分别递归 less = quick_sort(less) more = quick_sort(more) return less + pivot_list + more 另外一个版本，这个是不需要额外分配空间的原地排序。原地排序版本的快速排序的关键在于切分（partition）的操作，它在数组中找到一个哨兵值（pivot），使得在数组中哨兵值的左边都小于哨兵值，而数组的右边都大圩哨兵值。 1234567891011121314151617181920212223242526272829# in-placedef partition(array, lo, hi): i = lo + 1 # 左指针 j = hi # 右指针 pivot = array[lo] # 哨兵值 while i &lt;= j: while array[i] &lt; pivot and i &lt; j: # 直到找到一个*大于等于*哨兵值的元素 i += 1 # i&lt;j是为了防止当哨兵值是最大值时左指针越界 while array[j] &gt; pivot: # 直到找到一个*小于等于*哨兵值的元素 j -= 1 # 切分元素不可能会比自己小，右指针不会存在越界情况 if i &lt; j: # 左右指针未相遇，交换位置 array[i], array[j] = array[j], array[i] i += 1 j -= 1 else: # 左右指针相遇 i += 1 array[lo], array[j] = array[j], array[lo] # 将哨兵值和左子数组最右侧元素array[j]交换，返回j return j def qsort_inplace(array, lo, hi): if hi &lt;= lo: return pivot = partition(array, lo, hi) qsort_inplace(array, lo, pivot-1) qsort_inplace(array, pivot+1, hi) 有人可能会问，这两个版本的快速排序有什么区别吗？第一个版本的看起来更加容易理解啊，但是有一个很大的缺陷，你会发现这种实现的方法额外地分配了空间，it’s not worth the cost。 冒泡排序想象你手中有一副牌，从最左边第一张牌开始，将第一张牌与第二张牌比较大小，如果第一张牌大，那么交换两者的位置，否则，保持两者不动。然后到第二张牌与第三张牌比较，第三张与第四张比较，以此类推。经过第一轮相邻比较，我们找出了手中最大的一张牌，然后继续对剩下的牌重复上面的步骤。 之所以称之为冒泡排序，经过一轮一轮的比较，越小的数字会逐渐“浮现”到数组的顶端。下面的冒泡排序的实现： 123456def bubble_sort(array): for i in range(len(array)-1, 0, -1): for j in range(i): if array[j] &gt; array[j+1]: array[j], array[j+1] = array[j+1], array[j] return array 插入排序想象自己打扑克牌时理牌的过程，开始手上没有一张牌，摸到一张牌array[0]放在手上（假定是有序的，并且保证手中的牌始终有序的），再摸一张牌，与手上的牌array[0]比较，小的放后面，大的放前面。在打扑克时使用插入排序的人看起来比较小心谨慎还有追求稳定，他们生怕自己手中的扑克牌没有按照顺序排序，使用这种排序算法的老人居多。 123456789def insert_sort(array): for i in range(1, len(array)): new_card = array[i] # 手中的牌 j = i - 1 while j &gt;= 0 and new_card &lt; array[j]: # 新摸到的牌如果大于手中的牌则往后移动 array[j+1] = array[j] j -= 1 array[j+1] = new_card return array Python-算法]python实现冒泡，插入，选择排序 - CSDN博客 选择排序我在观察大人打扑克牌的时候发现，我小舅舅在摸牌时并不会像其他人用“插入排序”，他的策略是在所有牌摸完之前不给牌排序。等到所有的牌都到手后，他开始使用“选择排序”。 首先从第一张牌开始，假定第一张牌是最小的，然后将所有的牌与第一张牌比较，找到真正最小的牌并跟第一张牌交换，每次循环找出一个最小值。由此看来，冒泡排序和选择排序差不多，但是两者的复杂度还是有区别的，在最坏的情况，冒泡排序需要 $O(n^{2})$次交换，而插入排序只要最多$O(n)$交换。 下面是选择排序的实现： 12345678def select_sort(array): for i in range(0, len(array)-1): min = i for j in range(i+1, len(array)): if array[j] &lt; array[min]: min = j array[min], array[i] = array[i], array[min] return array 归并排序同快速排序，归并排序也用到了“分而治之”的思想，归并排序的过程是一分一合，“分”指的是先把子序列变成有序，“治”指的是合并各对有序的子序列，使其变成一个整体，它是将两个排好序的数组合并为一个数组的过程。 下面是归并排序的实现： 1234567891011121314151617181920212223242526272829303132# Recursively implementation of Merge Sortdef merge(left, right): result = [] while left and right: if left[0] &lt;= right[0]: result.append(left.pop(0)) else: result.append(right.pop(0)) if left: result += left if right: result += right return resultdef merge_sort(L): if len(L) &lt;= 1: # When D&amp;C to 1 element, just return it return L mid = len(L) // 2 left = L[:mid] right = L[mid:] left = merge_sort(left) right = merge_sort(right) # conquer sub-problem recursively return merge(left, right) # return the answer of sub-problemif __name__ == "__main__": test = [1, 4, 2, 3.6, -1, 0, 25, -34, 8, 9, 1, 0] print("original:", test) print("Sorted:", merge_sort(test)) 图解排序算法(四)之归并排序 - dreamcatcher-cx - 博客园 下面来看另外一个版本的归并排序，不过加入了指针的概念： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546def merge(left, right): result = [None] * (len(left) + len(right)) # 创建临时数组，长度为左右两个数组长度之和 i, j, k = 0, 0, 0 while i &lt; len(left) and j &lt; len(right): if left[i] &lt; right[j]: # 当左数组元素*小于*右数组元素，则将该元素放入result数组，同时左指针右移 result[k] = left[i] i += 1 else: # 当左数组元素*大于*右数组元素，则将该元素放入result数组，同时右指针右移 result[k] = right[j] j += 1 k += 1 while i &lt; len(left): # 左右两个子数组长度不相等，则将剩余的部分全部放入result result[k] = left[i] i += 1 k += 1 while j &lt; len(right): result[k] = right[j] j += 1 k += 1 return resultdef merge_sort(array): if len(array) &lt;= 1: # When D&amp;C to 1 element, just return it return array mid = len(array) // 2 left = array[:mid] right = array[mid:] left = merge_sort(left) right = merge_sort(right) return merge(left, right) # return the answer of sub-problemif __name__ == "__main__": test = [1, -1, 3, 2] print("original:", test) print("Sorted:", merge_sort(test))]]></content>
      <categories>
        <category>coding</category>
      </categories>
      <tags>
        <tag>数据结构与算法</tag>
        <tag>排序算法</tag>
        <tag>快速排序</tag>
        <tag>归并排序</tag>
        <tag>插入排序</tag>
        <tag>选择排序</tag>
        <tag>冒泡排序</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[白话解释 SSH 端口转发原理]]></title>
    <url>%2F2018%2Fplain-explain-ssh-port-transform%2F</url>
    <content type="text"><![CDATA[SSH 为 Secure Shell 的缩写，是目前较可靠，专为远程登录会话和其他网络服务提供安全性的协议。 在实际场景中，由于防火墙的限制，A 与 B 之间不能直接通讯，我们通过 SSH 在两者之间建立隧道（tunneling）来实现通讯。 建立隧道的作用主要有两个：加密 client 和 server 之间的数据传输、突破防火墙的限制。 SSH 命令参数1234567-C 压缩数据传输-f 后台登录用户名密码-N 不执行shell[与 -g 合用]-g 允许打开的端口让远程主机访问 -L 本地端口转发-R 远程端口转发-p ssh 端口 SSH 本地端口转发实例本地端口转发的命令格式： 1ssh -L &lt;local port&gt;:&lt;remote host&gt;:&lt;remote port&gt; &lt;SSH hostname&gt; 设想一个场景，现有两台机器，本地主机 host_1 和 远程主机 host_2，但是由于防火墙的原因两台机器无法互相通信，而另外一台机器 host_3 却可以同时访问这两台机器，那么我们就可以设法通过 host_3 在 host_1 和 host_2 之间搭一座桥梁，让他们彼此能通信。 比如说，ssh -p 34185 -CNfL 4000:0.0.0.0:4000 lb@x.x.x.x 这条命令，我本地的机器跟 0.0.0.0 的机器网络不通，而 lb@x.x.x.x 是跳板机，它能够连接所有的机器。所以我可以用跳板机作为中间人，在我本机和 0.0.0.0 之间开一个“隧道”，并将端口号设置为 4000，这样 0.0.0.0 4000 端口的所有数据都会被转发到我本机的 4000 端口。 xshell 端口转发设置也可以通过在 xshell 手动设置端口转发 参考资料 实战 SSH 端口转发 Linux 下 SSH 命令实例指南 ssh 端口转发]]></content>
      <categories>
        <category>architecture</category>
      </categories>
      <tags>
        <tag>ssh</tag>
        <tag>端口转发</tag>
        <tag>ssh隧道</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kudu 部署指南与 Kudu Python 客户端使用]]></title>
    <url>%2F2018%2Fhow-to-deploy-kudu-and-use-kudu-python-client%2F</url>
    <content type="text"><![CDATA[1. kudu 部署指南（centos）1. 安装 lsb 依赖先安装 lsb 依赖：sudo rpm redhat-lsb 2. 安装 kudu推荐手动安装，因为 cloudera 的源非常不稳定，在 kudu 下载页面 下载以下四个安装包： 12345678910kudu-client0-1.4.0+cdh5.12.2+0-1.cdh5.12.2.p0.8.el6.x86_64.rpmkudu-client-devel-1.4.0+cdh5.12.2+0-1.cdh5.12.2.p0.8.el6.x86_64.rpmkudu-master-1.4.0+cdh5.12.2+0-1.cdh5.12.2.p0.8.el6.x86_64.rpmkudu-1.4.0+cdh5.12.2+0-1.cdh5.12.2.p0.8.el6.x86_64.rpm 在集群中的一台机器安装 master 和 t-server，其余只安装 t-server。 3. 创建 kudu 文件夹根据空间考虑 kudu 位置：sudo mkdir -p /data8/kudu &amp;&amp; sudo chown kudu:kudu /data8/kudu 4. 配置文件设置master 和 server 的配置文件设置。 master 设置123--fs_wal_dir=/data8/kudu/master--fs_data_dirs=/data8/kudu/master--default_num_replicas=1 t-server 设置12345--fs_wal_dir=/data8/kudu/tserver--fs_data_dirs=/data8/kudu/tserver--tserver_master_addrs=xxx.xxx.xxx.xxx:7051--default_num_replicas=1 5. 启动 kudu12sudo service kudu-master startsudo service kudu-tserver start 6. 安装 kudu-python 模块一定要确保 pip 是最新版本!!!，并且 Cython 已经安装好，安装 kudu-python 的 1.2.0 版本。 123sudo pip install --upgrade pipsudo pip install -i https://pypi.douban.com/simple Cythonsudo pip install -i https://pypi.douban.com/simple kudu-python==1.2.0 7. kudu 部署参考资料 http://kudu.apache.org/docs/installation.html#install_packages http://www.cnblogs.com/zlslch/p/7607700.html kudu python 使用教程（最新） kudu python 教程 程序园：Kudu Configuration Reference hadoop生态圈列式存储系统—kudu介绍及安装配置 Install kudu on Ubuntu 2. kudu 的 Python 客户端使用在 Python 下连接使用 Kudu 的方法少得可怜，并且也并非官方宣传的那般快速。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859import kudufrom kudu.client import Partitioningfrom datetime import datetime# Connect to Kudu master server# 连接 kudu master 服务client = kudu.connect(host=&apos;kudu.master&apos;, port=7051)# Define a schema for a new table# 为表定义一个模式builder = kudu.schema_builder()builder.add_column(&apos;key&apos;).type(kudu.int64).nullable(False).primary_key()builder.add_column(&apos;ts_val&apos;, type_=kudu.unixtime_micros, nullable=False, compression=&apos;lz4&apos;)schema = builder.build()# Define partitioning schemapartitioning = Partitioning().add_hash_partitions(column_names=[&apos;key&apos;], num_buckets=3)# Create new tableclient.create_table(&apos;python-example&apos;, schema, partitioning)# Open a tabletable = client.table(&apos;python-example&apos;)# Create a new session so that we can apply write operationssession = client.new_session()# Insert a row# 往表里面插入一行数据op = table.new_insert(&#123;&apos;key&apos;: 1, &apos;ts_val&apos;: datetime.utcnow()&#125;)session.apply(op)# Upsert a rowop = table.new_upsert(&#123;&apos;key&apos;: 2, &apos;ts_val&apos;: &quot;2016-01-01T00:00:00.000000&quot;&#125;)session.apply(op)# Updating a rowop = table.new_update(&#123;&apos;key&apos;: 1, &apos;ts_val&apos;: (&quot;2017-01-01&quot;, &quot;%Y-%m-%d&quot;)&#125;)session.apply(op)# Delete a rowop = table.new_delete(&#123;&apos;key&apos;: 2&#125;)session.apply(op)# Flush write operations, if failures occur, capture print them.try: session.flush()except kudu.KuduBadStatus as e: print(session.get_pending_errors())# Create a scanner and add a predicate# 先创建一个 scanner，然后再读取表中的数据# 表中的数据太多，你还需要添加一个所谓的 predicate 只读取规定区间内的数据scanner = table.scanner()scanner.add_predicate(table[&apos;ts_val&apos;] == datetime(2017, 1, 1))# Open Scanner and read all tuples# Note: This doesn&apos;t scale for large scansresult = scanner.open().read_all_tuples() 参考资源 python中使用kudu：https://kudu.apache.org/docs/developing.html#_kudu_python_client理解kudu与impala之间的联系：https://blog.csdn.net/cdxxx5708/article/details/79074489 kudu踩坑：https://www.2cto.com/kf/201707/653572.html https://blog.cloudera.com/blog/2016/01/interactive-analytics-on-dynamic-big-data-in-python-using-kudu-impala-and-ibis/]]></content>
      <categories>
        <category>architecture</category>
      </categories>
      <tags>
        <tag>大数据平台部署</tag>
        <tag>架构</tag>
        <tag>Apache</tag>
        <tag>Kudu</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python 工具箱系列（3）：三种方法遍历文件的所有行]]></title>
    <url>%2F2018%2Fiter-all-lines-of-files%2F</url>
    <content type="text"><![CDATA[当文件比较小，用 readlines 读取文件开头到结尾的所有行，它会返回一个包含所有行的列表，然后你只需要循环遍历列表的所有元素即可。 1234lines = open(file, &apos;rU&apos;).readlines()for line in lines: do something 当文件比较大，你不可能一次性将整个文件加载到内存，此时只能逐行读取。 12345f = open(file)while 1: line = f.readline() if not line: break 从文件的某一行开始读取，该实现需要借助 itertools 模块， 12345import itertoolswith open(&apos;/file/path&apos;) as f: for line in itertools.islice(f, 2, None): # 遍历文件从第二行到最后一行 print line]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>IO</tag>
        <tag>Python工具箱系列</tag>
        <tag>PythonToolkitSeries</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[理解 avro 及其解析模块：fastavro、pyavroc]]></title>
    <url>%2F2018%2Funderstanding-avro-file-and-its-decode-modules-fastavro-pyavroc%2F</url>
    <content type="text"><![CDATA[在我看来，avro 文件最大的特点无非是：便于实现并发。 avro 是一种快速可压缩的二进制数据形式，对数据二进制序列化后可以节约数据存储空间和网络传输带宽使用，avro 文件总体上由文件头(Header)和数据块(Data Block)及同步标识(Synchronization marker)三部分组成。 关于 avro 文件，Wikipedia 给出的定义是： Avro是一种远程过程调用和数据序列化框架，是在Apache的Hadoop项目之内开发的。它使用JSON来定义数据类型和通讯协议，使用压缩二进制格式来序列化数据。它主要用于Hadoop，它可以为持久化数据提供一种序列化格式，并为Hadoop节点间及从客户端程序到Hadoop服务的通讯提供一种电报格式。 还有一点，avro 定义了一个对象容器存储的文件格式，一个文件有一个模式（schema），这样便可以高效、精确地对文件进行切割划分。Avro文件的文件头包含以下元信息（metadata），每个avro 文件都包含这些信息： 下面详细解释这三个部分： 文件头为标识为Header的青色大框部分，包括文件的 schema、codec、sync marker 等信息。在 fastavro 的命令行工具中，使用fastavro --schema weather.avro可以获取文件的 schema 信息，schema 提供了文件类型、变量数据格式等等细节 12345678910111213141516171819202122$ fastavro --schema weather.avro&#123; &quot;type&quot;: &quot;record&quot;, &quot;namespace&quot;: &quot;test&quot;, &quot;doc&quot;: &quot;A weather reading.&quot;, &quot;fields&quot;: [ &#123; &quot;type&quot;: &quot;string&quot;, &quot;name&quot;: &quot;station&quot; &#125;, &#123; &quot;type&quot;: &quot;long&quot;, &quot;name&quot;: &quot;time&quot; &#125;, &#123; &quot;type&quot;: &quot;int&quot;, &quot;name&quot;: &quot;temp&quot; &#125; ], &quot;name&quot;: &quot;Weather&quot;&#125; 数据块为文件头下方紧邻的灰色的 Data Block 部分，包括 block 内数据（record）个数、序列化对象大小、16 byte 大小的同步标识（sync marker）。在这些信息的基础上，每个 block 在文件中都有唯一确定的标识，这样就可以有效地提取或跳过某个 block 的二进制数据， 块大小，对象计数和同步标记的组合可以帮助检测损坏的块并确保数据完整性。 同步标识 sync marker 数据块下方紧接着的橘色的 Synchronization marker 部分 1. avro 文件解析模块：fastavro 和 pyavroc目前，Python 环境下使用较多的模块有三种，解析速度最快的是底层基于 C 实现的 pyavroc， 它的遍历速度大约是 fastavro 的 4 倍，每遍历 1 W 条 record 只需要 0.5s。只不过 pyavroc 的安装稍微麻烦一点，需要先安装 Cmake 编译，其他的两个安装都比较简单，一个 pip 命令就可以搞定。 Name Description Relative speed (bigger is better) python-avro Avro’s implementation (pure Python) 1 fastavro python-avro improved, using Cython 10 pyavroc Python/C API on upstream Avro-C 40 1.1 安装 pyavroc1.1.1 安装步骤作者在 github 上给出的安装步骤，按照步骤一步一步来就好了，安装完成之后import pyavroc就可以直接使用。 https://github.com/Byhiras/pyavroc#installating-the-module Download and run cmake installation file from https://cmake.org/ (e.g. https://cmake.org/files/v3.12/cmake-3.12.1-Linux-x86_64.sh) Once extracted just add its bin folder to the PATH so that cmake command is available Clone this repo - git clone https://github.com/Byhiras/pyavroc Run ./clone_avro_and_build.sh in the cloned folder This generates a build folder python setup.py bdist_wheel -d build makes an executable wheel of the project and saves in the build/ folder cd build pip install pyavroc-0.7.2-cp36-cp36m-linux_x86_64.whl 1.1.2 安装 cmakepyavroc 是使用 cmake 进行编译的，先安装 cmake。 https://blog.csdn.net/qing666888/article/details/79090622 1234567sudo wget https://cmake.org/files/v3.12/cmake-3.12.1-Linux-x86_64.tar.gz --no-check-certificatetar -xzf cmake-3.12.1-Linux-x86_64.tar.gzexport PATH=$PATH:/file/path/cmake-3.10.1-Linux-x86_64/bincmake --version # 如果返回版本信息则表示安装成功 1.2.3 安装 pyavroc前面的准备工作做好之后，在目录下有一个脚本，直接执行脚本会创建一个 build 的文件夹，进入文件夹，里面有一个 whl 的安装文件（可能跟文档说的不一样）。 ./clone_avro_and_build.sh python setup.py bdist_wheel -d build cd build pip install pyavroc-0.7.2-cp36-cp36m-linux_x86_64.whl 1.2 安装 fastavropip install fastavro 2. pyavro 与 fastavro 读取 avro 文件fastavro 读取 avro 文件时有两种方法：逐条遍历和块读取。 123456789101112# 逐条遍历from fastavro import readerwith open(&apos;some-file.avro&apos;, &apos;rb&apos;) as fo: avro_reader = reader(fo) for record in avro_reader: process_record(record)# 块 block_reader 读取from fastavro import block_readerwith open(&apos;some-file.avro&apos;, &apos;rb&apos;) as fo: avro_reader = block_reader(fo) for block in avro_reader: process_block(block) pyavroc 的速度虽说比 fastavro 要快，但是它的方法少得可怜，比如读取文件，pyavroc 只能逐条遍历，没有类似 fastavro 的块读取： 12345&gt;&gt;&gt; import pyavroc&gt;&gt;&gt; with open(&apos;myfile.avro&apos;) as fp:&gt;&gt;&gt; reader = pyavroc.AvroFileReader(fp, types=True)&gt;&gt;&gt; for record in reader:&gt;&gt;&gt; print record 写到这里，本文还没有提及到 avro 快速的原因，下一篇文章将会继续提到。]]></content>
      <categories>
        <category>教程</category>
      </categories>
      <tags>
        <tag>avro</tag>
        <tag>apache</tag>
        <tag>fastavro</tag>
        <tag>pyavroc</tag>
        <tag>同步标示</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux | 定时任务、文件切割]]></title>
    <url>%2F2018%2Flinux-split-files-and-timed-task%2F</url>
    <content type="text"><![CDATA[1. mkdir主要注意有两个参数，p 和 m 123456在目录/usr/meng下建立子目录test，并且只有文件主有读、写和执行权限，其他人无权访问mkdir -m 700 /usr/meng/test在当前目录中建立bin和bin下的os_1目录，权限设置为文件主可读、写、执行，同组用户可读和执行，其他用户无权访问mkdir -p-m 750 bin/os_1 2. crontab 设置定时任务12crontab -l # 编辑该用户的计时器设置crontab -e # 列出该用户的计时器设置 设置命令执行的频率，在 crontab 中设置的格式是： 12* * * * * commandminute hour day month week command 顺序：分 时 日 月 周 星号（*）：代表所有可能的值，例如month字段如果是星号，则表示在满足其它字段的制约条件后每月都执行该命令操作。 逗号（,）：可以用逗号隔开的值指定一个列表范围，例如，“1,2,5,7,8,9” 中杠（-）：可以用整数之间的中杠表示一个整数范围，例如“2-6”表示“2,3,4,5,6” 正斜线（/）：可以用正斜线指定时间的间隔频率，例如“0-23/2”表示每两小时执行一次。同时正斜线可以和星号一起使用，例如*/10，如果用在minute字段，表示每十分钟执行一次。 例子：每隔两天的上午8点到11点的第3和第15分钟执行 command 3,15 8-11 */2 * * command 3. 搜索命令行历史纪录Ctrl + r，如果不是这条命令的话，可以再按下 Ctrl + r，Bash 会向前搜索有 hi 字符的命令。 4. find 在制定目录下查找文件见linux find 命令 12# 当前目录搜索所有文件，文件内容 包含 “140.206.111.111” 的内容find . -type f -name &quot;*&quot; | xargs grep &quot;140.206.111.111&quot; 5. alias 设置别名alias命令用来设置指令的别名。我们可以使用该命令可以将一些较长的命令进行简化。使用alias时，用户必须使用单引号’’将原来的命令引起来，防止特殊字符导致错误。 1alias py=&apos;python&apos; alias命令的作用只局限于该次登入的操作。若要每次登入都能够使用这些命令别名，则可将相应的alias命令存放到bash的初始化文件 /etc/bashrc 中。 6. 文件切割split 命令 可以将一个大文件分割成很多个小文件。 12345-b：值为每一输出档案的大小，单位为 byte。-C：每一输出档中，单行的最大 byte 数。-d：使用数字作为后缀。-l：值为每一输出档的列数大小。-a：指定后缀长度(默认为2)。 比如，split -l 10 test.txt -d -a 3 file 将 test.txt 文件每 10 行切分一次，使用 file 作为前缀，数字作为后缀，后缀长度为 3。 1234split -l 5 test.csv for i in * ; do mv $i $i&quot;.csv&quot; ; donesed -i &apos;1i &quot;uid&quot;,&quot;phone_cc&quot;,&quot;phone&quot;,&quot;screen_name&quot;,&quot;avatar&quot;,&quot;country&quot;,&quot;province&quot;,&quot;city&quot;,&quot;gender&quot;,&quot;birthday&quot;,&quot;description&quot;,&quot;status&quot;,&quot;created_at&quot;,&quot;email&quot;,&quot;email_verified&quot;&apos; *.csvhead -n 2 xaa.csv 7. 列出文件的绝对路径就是在每行记录的开头加上当前路径 1ls | sed &quot;s:^:`pwd`/:&quot;]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>文件切割</tag>
        <tag>定时任务</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python 工具箱系列（1）：常用文件操作]]></title>
    <url>%2F2018%2Fpython-file-operations-in-common-use%2F</url>
    <content type="text"><![CDATA[1. 新建文件夹12if not os.path.isdir(path_out): os.makedirs(path_out) 2. 遍历所有文件和子文件夹12for a, b, filenames in os.walk(path_data): for filename in filenames: 3. 只遍历当前文件，不包含子文件夹123for a, b, filenames in os.walk(path_data): for filename in filenames: if a == path_data: 4. 读取文件的前几行或中间某些行得到一个由迭代器生成的切片对象，标准切片不能做到。 12345import itertoolswith open(&apos;/file/path&apos;) as f: for line in itertools.islice(f, 2, None): # 遍历文件从第二行到最后一行 print line 5. 跳过拥有某些元素的行在逐行读取文件时，我们可能要忽略掉拥有某些元素的行，通常一般的写法是： 1234with open(&apos;file/path&apos;) as f: for line in f: if line.startswith(&apos;#&apos;): print line 6. 判断是文件还是目录判断路径是文件还是目录，不依赖于所处环境 1234567891011121314&gt;&gt;&gt; os.path.exists(&quot;te&quot;)True&gt;&gt;&gt; os.path.exists(&quot;nothing&quot;)False&gt;&gt;&gt; os.path.isfile(&quot;nothing&quot;)False&gt;&gt;&gt; os.path.isdir(&quot;nothing&quot;)False&gt;&gt;&gt;&gt;&gt;&gt; os.path.isdir(&quot;te&quot;)False&gt;&gt;&gt; os.path.isfile(&quot;te&quot;)True&gt;&gt;&gt; 7. 遍历文件中的所有行1234567with open(&apos;foo.txt&apos;) as fp: line = fp.readline() n = 0 while line: n += 1 print line print n]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>IO</tag>
        <tag>Python工具箱系列</tag>
        <tag>PythonToolkitSeries</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python | 对比两段计数器后想到的]]></title>
    <url>%2F2018%2Fcomparison-between-two-counters%2F</url>
    <content type="text"><![CDATA[任务背景：有 n 个 txt 文件，每个文件都有 i 行，每行的内容就是对应的行号 j。现在需要按照一定的间隔（slice_size），连续切分这 n 个文件，并返回切分点对应位置的总数、切分点在文件中对应行号、从该切分点开始能读取多少数据。 三个示例文件如下，三个文件的长度分别为 12，4 和 5，总共 21 行数据。 返回：[(1, 1, 5), (6, 6, 5), (11, 11, 2), (13, 1, 4), (17, 1, 5)] 文件一： 123456789101112 1 1 2 2 3 3 4 4 5 5 6 6 7 7 8 8 9 910 1011 1112 12 文件二： 12341 12 23 34 4 文件三： 123451 12 23 34 45 5 我认为，问题处理的难点在于最后一个返回值，我们需要在遍历完文件之后才能知道切分点后又多少数据。比如第一个文件，按照每 5 行切分一次，最后一次切分点在第 11 行，而此时，指针到文件末尾时我才知道从 11 这个切分点开始只剩下了两个数据，我无法提前知道剩下多少个数据。在这个地方我纠结了很久，到底怎样才能保存好这个值呢？我能确定的是，只有当指针到达文件末尾时，最后一次切分才会结束，才会知道在该次切分中剩余多少数据，所以，在 counter_01 中，我加入了一个 if not line_pos: 的判断条件。 接下来就是计算剩下多少数据的问题了，我通过 res = idx % slice_size 得到剩余的数据，比如总数 12 的文件，以 5 来切分，12 % 5 = 2，这样就得到了最后一次需要读取的数据。但是，这种做法存在一个很严重的问题：每次切分时我都是默认切分点后面会有 slice_size 个数据，所以当指针到达 11 行时，我就提前假设后面有了 5 条数据，然而事实上并没有，需要当指针到达最后一行时来修正。为了掩饰那个错误，我用了比较暴力的办法：直接干掉列表的倒数第二个自以为正确的数据。 对于第一个返回值，切分点在三个文件累计位置，只需要在文件遍历的最外层设置 total 计数变量即可，每读完一行数据再加一。这里也有一个需要处理的坑，比如在 if not line_pos: 的判断中，切分点累计位置如果还等于 total，那最后一个切分点的位置永远会在文件的最后一行，所以需要在 if idx % slice_size == 0: 中先把 total 赋值给 start_num 存下来给接下来的判断用。第二个返回值处理的逻辑也类似，把 line_pos 赋值给 tmp_line_pos，留给接下来的判断语句中使用。 1234567891011121314151617181920212223242526272829303132333435363738#!/usr/bin/python# -*- coding=utf8 -*-"""# @Author : Li Bin# @Created Time : 2018-08-15 23:32:55# @Description : 返回切分列表"""slice_size = 5file_paths = ['/home/libin/cook_book/dataset/test_01.txt', '/home/libin/cook_book/dataset/test_02.txt', '/home/libin/cook_book/dataset/test_03.txt']def counter_01(): total = 0 slice_list = [] for file_path in file_paths: with open(file_path) as fp: line_pos = fp.readline() idx = 0 while line_pos: total += 1 if idx % slice_size == 0: start_num = total slice_list.append((start_num, int(line_pos.strip()), slice_size)) tmp_line_pos = line_pos line_pos = fp.readline() if not line_pos: last_idx = (idx / slice_size) * slice_size res = idx % slice_size slice_list.append((start_num, int(tmp_line_pos.strip()), res+1)) del slice_list[-2] idx += 1 print slice_list 第一个计数器勉强实现了计数的功能，但是不够漂亮，特别是处理剩余数据的逻辑上有说不出的混乱和复杂，很蹩脚。来看第二个计数器，初始参数有 total 和 slice_counter 两个，对比计数器一，计数器二的思路为每次只切一份小片数据，slice_counter 等于 1，记下 slice_start_pos、slice_start_num，当 slice_counter 等于 5，将上一个 slice_start_pos、slice_start_num 取出来，此时该小片有 5 个数据。 那么问题来了，要是最后一个分片没有 5 行数据咋办？这里的处理逻辑很漂亮，当切完一个片，slice_counter 归零，当指针到达了最后一行，如果最后一个分片不足 5，那么 slice_counter 是不会归零的！while 判断跳出来后，后面加个 if slice_counter &gt; 0: 把剩余的 slice_start_pos、slice_start_num、slice_counter 取出来，任务完成。 123456789101112131415161718192021222324def counter_02(): total = 0 slice_list = [] for file_path in file_paths: with open(file_path) as fp: line_pos = fp.readline() slice_counter = 0 while line_pos: total += 1 slice_counter += 1 if slice_counter == 1: slice_start_pos = int(line_pos.strip()) slice_start_num = total if slice_counter == slice_size: slice_list.append((slice_start_num , slice_start_pos, slice_counter)) slice_counter = 0 line_pos = fp.readline() if slice_counter &gt; 0: slice_list.append((slice_start_num, slice_start_pos, slice_counter)) print slice_list 对比以上两个计数器，显然第二个更优，处理的逻辑干净、思路清晰，每个变量负责的任务很明确，特别是最后的 if slice_counter &gt; 0: 很漂亮。第一个计数器在一开始想干的事情太多了，一开始就想着假设后面有 5 条数据，然后靠后面的暴力删除来掩饰这个错误。对比完这两个计数器，我觉得，在处理的问题的时候，拆解问题的能力很重要，一个逻辑处理一个小问题，别想着存在一个万能逻辑解决多个问题，这是我以上想到的。 继续加强代码能力。。。]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>计数器</tag>
        <tag>复盘</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python 工具箱系列（2）：生成大数据结构]]></title>
    <url>%2F2018%2Fgenerate-big-data-structure%2F</url>
    <content type="text"><![CDATA[测试数据库性能用 1. 批量生成大字典生成一个大字典，大字典中再嵌套 10 个小字典，第一个循环控制字典的个数，第二个循环控制字典的宽度。 1234for i in range(10): dict[str(i)] = &#123;&#125; for j in range(10): dict[str(i)][str(j)] = j 2. 批量生成大列表列表嵌套列表，第一个循环控制列表个数，np 控制列表的宽度 12345data = []for i in range(10): arr = np.random.randint(1, 10, size=250) arr_2_list = arr.tolist() arr_2_list[0] = i 3. 批量生成字符串批量生成固定前缀的字符串列表。 12345batch_string_list = []for i in range(10)： str_prefix = &apos;test_&apos; tmp = str_prefix + str(i) batch_string_list.append(tmp)]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>Python工具箱系列</tag>
        <tag>PythonToolkitSeries</tag>
        <tag>list</tag>
        <tag>大字典</tag>
        <tag>大字符串</tag>
        <tag>大数组</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[方法论]]></title>
    <url>%2F2018%2Fmind-method%2F</url>
    <content type="text"><![CDATA[拆解 优先级 计划]]></content>
      <categories>
        <category>思想王国</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[常用的 Linux 命令：文件和系统操作]]></title>
    <url>%2F2018%2Ffiles-system-operations-command-in-linux%2F</url>
    <content type="text"><![CDATA[1. 文件操作相关解压 .gz 文件gzip -d 其他解压命令linux下 tar解压 gz解压 bz2等各种解压文件使用方法 移动、重命名文件重命名、移动文件夹： mv file_a file_b：将 file_a 更名为 file_b mv /a /b/c：将文件夹 a 转移到文件夹 /b/c 下 查看某个文件或目录占用磁盘空间的大小常用选项组合为：du -sh，查看当前文件夹的大小。 下载/上传文件sz 下载，rz 上传 tar包解压缩tar -zxvf /opt/soft/test/log.tar.gz 2. 系统相关查看物理CPU个数、核数、逻辑CPU个数1234567891011# 总核数 = 物理CPU个数 X 每颗物理CPU的核数 # 总逻辑CPU数 = 物理CPU个数 X 每颗物理CPU的核数 X 超线程数# 查看物理CPU个数cat /proc/cpuinfo| grep &quot;physical id&quot;| sort| uniq| wc -l# 查看每个物理CPU中core的个数(即核数)cat /proc/cpuinfo| grep &quot;cpu cores&quot;| uniq# 查看逻辑CPU的个数cat /proc/cpuinfo| grep &quot;processor&quot;| wc -l Linux查看物理CPU个数、核数、逻辑CPU个数 htop 与 top 命令的区别为什么 Linux 的 htop 命令完胜 top 命令 如何使用htop命令 pstree -p 36545 查看 pid 为36545主进程的进程树结构，当然，你也可以将 htop 换成进程的树形结构。 在后台挂起任务nohup [command] &amp; 后台挂起任务并且执行信息不会打印出来，输出的信息存储在 nohup.txt 文件中。当然，也可以将命令打印的信息存放到指定的路径，如 nohup [command] &gt; output.file 2&gt;&amp;1 &amp;，2&gt;&amp;1 这个意思是把标准错误（2）重定向到标准输出中（1），而标准输出又导入文件 output 里面，所以结果是标准错误和标准输出都导入文件output里面了。 操作系统中有三个常用的流： 0：标准输入流 stdin1：标准输出流 stdout2：标准错误流 stderr 一般当我们用 &gt; console.txt，实际是 1&gt;console.txt的省略用法；&lt; console.txt ，实际是 0 &lt; console.txt的省略用法。 杀掉进程ps -ef | grep [search_name] 搜索包含特定字符串的进程，找到进程唯一标识 pid，输入命令kill -s 9 pid，杀掉进程。 查看当前操作系统版本信息cat /proc/version chmod 更改文件权限1234r 读取权限，数字代号为“4”;w 写入权限，数字代号为“2”；x 执行或切换权限，数字代号为“1”；- 不具任何权限，数字代号为“0”； Linux 文件的权限分布图，前面那一长串 10 个字符可以分为 1\3\3\3 四个部分，第一个部分占一个字符长度，表示 .gitmodules 是文件还是目录，第二、三、四 3 个部分分别占三个字符长度，表示 .gitmodules 在操作对象手中的权限。下面的例子说明，.gitmodules 在拥有者手中有读和写的权限，在群组（group）和其他人（others）中只有读的权限（Linux用 户分为：拥有者、组群(Group)、其他（other））。 123456 -rw-r--r-- 1 user staff 651 Oct 12 12:53 .gitmodules# ↑╰┬╯╰┬╯╰┬╯# ┆ ┆ ┆ ╰┈ 0 其他人# ┆ ┆ ╰┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈ g 属组# ┆ ╰┈┈┈┈ u 属组# ╰┈┈ 第一个字母 `d` 代表目录，`-` 代表普通文件 那么如何改变文件或者目录的权限呢？chmod 命令可以解决，一般的用法是 chmod 后面加上一个三位数，三个数组分别对应三个用户对该文件或目录所拥有的权限，如 chmod 600 file.txt 表示自己对 file.txt 文件有读写权限（4+2），而群组和其他人没有读写和执行的权限。详见chmod。 3. 常用学习地址 Linux 命令搜索引擎 Linux基础]]></content>
      <categories>
        <category>教程</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>chmod</tag>
        <tag>gzip</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[三步教你轻松使用文献管理神器——Endnote，毕业必备！]]></title>
    <url>%2F2018%2Fhow-to-use-Endnote-by-three-steps%2F</url>
    <content type="text"><![CDATA[不用一条条修改参考文献序号，不用担心插入一条新文献后序号全部需要改变，不用记忆历史插入的文献列表，Endnote 全部帮你搞定！ 声明：本文仅作为使用教程，请勿传播盗版软件，后果自负！！！ 1. 安装 Endnote安装之前关闭 Word！Endnote 安装好后插件会自动安装到 Word。 1.1 EndNote X7 下载地址链接：https://pan.baidu.com/s/1FpW67FT6ngawLF6vyFESCA 密码：vdf5 1.2 解压，大约需要 30s1.3 安装第一步：点击，然后再点击“安装”，第二步：进入 Endnote。 1.4 进入软件主界面新建一个库，重命名，记得存储位置。 2. 如何在 Word 中导入文献？一般从知网或者 Google 学术导出参考文献文件 2.1 cnki 2.2 Google 学术 3. 在 Word 中插入参考文献3.1 设置插入格式首先，需要在 Word 设置参考文献格式，中文毕业论文一般是 Chinese GBT7714(numeric) 格式 3.2 分三步插入 Word首先将从知网或 Google 学术下载的文献文件导入 Endnote， 然后，在 Word 文档中适当的位置插入文献 常见问题 Endnote插入Word时出现大括号时的解决办法 Endnote官网上的国标输出格式少半个括号！ 记得在 Endnote 官网注册一个账号，随时同步文献，防止意外，祝大家顺利毕业！]]></content>
      <categories>
        <category>教程</category>
      </categories>
      <tags>
        <tag>Endnote</tag>
        <tag>文献管理</tag>
        <tag>毕业论文</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[谈谈一点关于阅读方面的事]]></title>
    <url>%2F2018%2Ftalk-something-about-reading%2F</url>
    <content type="text"><![CDATA[15 年，也就是研究生入学前几个月，有过一段非常密集的阅读体验。 在家闲着反正也是闲着，正好京东、亚马逊还有当当这三个书城那会促销力度特别大。我记得满 400 减 200 、满 300 减 100 的活动就有好几次。 那会也是买书舍得出血，几个订单就花了 1400 多块，到现在我自己也没明白为什么这么痛快。 我特别喜欢我家里那辆豪爵摩托车，150cc 的动力劲相当足，载几个人爬坡哼哼几下就上去了。读高中那会一放假就跟着同学骑个摩托车到处飚，均速 60 码以上（放现在不敢了，太危险）。而且它如 Linux 一般经济稳定，一点小问题它不会过来招你惹你。不过现在我那些同学都耻于骑两个轮子的。 等到收到快递过来的电话后，我就骑着摩托车顶着太阳一箱一箱往家里运。 我看的书比较杂而且比较贪，小说、人文社科、历史、传记、畅销书、科普什么都看，比如枯燥如《合作的进化》、快餐如《大数据时代》几个小时就能看完。以往我看书必一字一句去抠，这样做坏处可大，后来我觉得不行，世界这么多好作品，照这么下去可不太亏了。于是我开始有意锻炼阅读的速度，最后差不多一两天就可以读完一本书，然后花一天时间做读书笔记。 为什么要做读书笔记？要回答这个问题那么得先回到为什么要阅读，其实密集的阅读体验也是有一点功利的性质在里边。我的阅读速度自认为还不算太差，可是有一个毛病，忘得特别快。我打心底敬佩那些在人前能对一部作品侃侃而谈的人，现在我也没弄明白他们是怎么做到的，难道是将那些经典部分事先背诵下来？所以，我觉得做读书笔记是个不错的办法，一来温故而知新，二来发到网上可以满足自己的虚荣心。也就是在那个时候，我逐渐形成了阅读的习惯（虽然读的并不多）。 后来呢，我到了学校，专业的书籍就买得比较频繁了，不过大多也是只用来作为工具书。技术的书籍比较贵，统计了一下，读研买了 2000 多块的书了，虽然有的书可能买过来根本没看完或者没看，但是我也觉得不惭愧，看不完大不了留给我儿子看。 比如学习编程的书籍，大多看了前面几个基础章节，后面的章节就很少涉猎了。所以，我建议，以后的类似专业的技术书籍可以分为上下两册，入门的和精通的，不过这样出版社肯定不干。 电子书和纸质书的争端从来没有停息过，纸质书挺好，拿到手里有感觉，但是太笨重，换地方可就遭殃了。电子版方便，有电子设备就行，资源也丰富，哪里都可以看。依我看，电子书和纸质书的区别就好比 VR 和实地旅游。 去年，我买了一个 kindle，日本亚马逊刚好有一个会员日，半价淘了一个白色的 kpw 3，加上运费也不过 600 块，比国行的便宜多了。kindle 是去年买得最值的电子产品，这里我又得提起我家的那辆摩托车（可见我有多喜欢），kindle 跟摩托车一样，不用怎么去管它，它就干一件事，就是阅读（载人）。除非你想起它来了，打开翻翻看看，否则它就一声也不吭。 我比较支持正版的，虽然说盗版的电子书在很多网站触手可及，相比花个十几块去 kindle 商店买回来，盗版还是挺有诱惑的。我始终认为，读完书得给别人分享，通过把书讲解给别人听，看看听众会有什么反应。令人沮丧的是，我始终还没有遇到能认认真真听我讲讲感受的书友，也许我得学学如何生动表达这门技术。 kindle 有一个缺点，它的社交功能太弱了，无法满足读者的虚荣心，看到个好句子还必须得用手机拍下来分享。不过，极大可能也是亚马逊有意为之。 我始终认为阅读不是一件能够让你有优越感的活动，跟打球、看电影一样，它就是一种消遣娱乐的方式。 阅读顶多能帮助你解开心中的疑惑，能够在遇到烦心事后不至于没有一丝头绪，能够装点一下不太丰富的内心世界，让你在抬头仰望天空的时候不至于只是看到星星月亮。 它留给我的愉快记忆也就是骑着车往家里运书的经历，六月的太阳很灼，头顶的头发被烤得又硬又烫，可当时我一点也不觉得热，反而相当轻松。]]></content>
      <categories>
        <category>思想王国</category>
      </categories>
      <tags>
        <tag>读书笔记</tag>
        <tag>读书</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[寻找真实的自己]]></title>
    <url>%2F2018%2Flook-for-the-real-yourself%2F</url>
    <content type="text"><![CDATA[This is a tough reading for me. 不会只有我一个人觉得阅读这本书是一次很艰难的经验吧？《禅与摩托车维修艺术》一共有四个部分 32 小节，为了能够顺利完成阅读，强烈建议每次只阅读 2-3 小节。 ​其实，这本小说描写的并不是教你如何修摩托车，而是记述波西格在与他的儿子克里斯骑摩托车旅行中的一系列关于科技与艺术、古典与浪漫、现代教育的思考。我在这里不想跟其他笔记一样老调重弹，我想说说我自己真实的思考体会，思考体会自然读来也是无趣的，因为这本来数属于我自己的良质嘛。 “ 是什么把他们带进殿堂里的……答案不一而足……逃避平凡生活的芜杂和无可救药的厌倦；逃离自己欲望的束缚。一个脾气好的人想要逃离喧闹、令人紧张的环境，而来到寂静的高山，在这里你极目远眺，透过静谧清新的空气，愉快地描摹永恒宁静的山色。 ” 古典与浪漫古典与浪漫是两种截然不同的思维方式，古典强调理性思考，浪漫则注重直观感受。波西格同行的驴友约翰惧怕科技，即使是有手册，他也不相信自己有能力能修理他昂贵的宝马摩托车。其实，他惧怕的不是技术所带来的困惑，而是技术给他带来智力的羞辱。我的手机坏过几次，第一次手机屏幕坏掉的时候，我不相信自己能有能力修好，我觉得这个应该是挺有技术性质的工作，所以为此我埋了约 200 元的单。第二次手机坏掉的时候，手机被自己修坏的成本已经下降了很多，我觉得自己可以动手弄好。所以我找了一些拆解教程，买了工具和材料，小心翼翼地按照手册上的步骤一步步来。虽然第一次拆机不太熟练，更换电池就花了一个半小时，但是这是一次人与机器沟通的极佳体验。在人们眼中，修行就是寺院的僧人念经打坐从而达到内心的宁静的过程，然而，寻找内心的平静并不仅限此法。观察万事万物的细节，体会他们的构造，揣摩背后的原理也是消除困惑达到内心宁静的途径。再比如阅读，阅读也是一门技术，阅读完整本书就是一次修行的体验。因为在阅读的时候你得思考作者是怎么构思全文的，你会从目录着手，然后规划阅读计划。有的书不会像畅销书那么好读，它的艰涩难懂很容易让你打退堂鼓，这个时候你会想着放弃，同时又为付出的时间成本而懊恼，我什么要来遭受这个罪？你需要集中自己的注意力，5分钟，然后再10分钟，右手拿起笔，突然你会体验到一种愉悦，那是注意力集中的快感。如果说把阅读比作工作的话，这种状态就是波西格所强调的“人和物合二为一”的状态。 何为真正的学习？再说说学习，其实，真正的学习是寻找自身的过程，传统的教育的目的不在于帮助你寻找自身，而是完成训练你思维能力的任务。波西格很痛恨传统的教育模式，他认为最好的教育方式应该是引导学生发现自我的过程，而决定的因素即是自由。他在文中多次提到“游荡式”的学习，最好的学习方式是在卡住之后自由地游荡一段时间然后再回到问题的本身，静静等待“结晶波浪的产生”。这种说法也可以解释多数人从大学毕业之后在工作中发现了现实中的需求，然后继续回学校学习自己真正感兴趣的东西。聪明的人会一直学习，而普通聪明的人则只会在传统教育模式下有所作为，绝顶聪明的人同时领悟了两种模式的真谛。自由的教育模式比传统的教育更会让学生感到痛苦，因为一旦没有了强制设定的目标，他们便会无所事事不知道干嘛。聪明的学生则倍感轻松，因为他们不需要分数来证明自己，他们知道自己理解到了哪个程度。因此，主动的学习方式会让一个人受益终身，当我们在抬头仰望星空，我希望看到得不仅仅是星星，还有宇宙的过去和将来；当面对着壮阔震撼的山川湖海，映入眼帘的不会只是一颗巨大的石头和说不出名字的植物，我们会思考他们背后的历史、地质变迁。波西格和约翰在旅行的途中是两种不同的风格，约翰关心的只是如何尽快的到达当天任务的终点好喝上一点啤酒，他对旅途中的视觉、触觉感官完全没有概念。反观波西格，他的观察细腻，骑摩托车的过程他会对两边的事物有直观的感受，雨水、空气湿度、沼泽里的野鸭、摩托车爬升的海拔高度……这些会给他带来思考，他不仅仅在骑摩托车，他是在跟自己的心灵对话，寻找真正的自己。 良质=道人与工作的态度，波西格认为只有当工作者对自己手中的工作产生认同感。然而，很遗憾，大多数无法对自己手中的工作产生认同感，即认为自己的工作是有意义的，而不是为了屈从与某个体系下。真正把自己投入到工作中去的时候，他才会关注到良质。关于良质，一开始提到的时候他并没有给出定义，而是通过一系列的例子来佐证良质不能被给出定义，否则就不能称之为良质，即能够说出来的良质便不是真的良质。隐隐约约我觉得这个说法相当熟悉，直到后来我读到“The quality that can be defined is not the Absolute Quality.”，良质不就是所谓的“道”吗？道可道，非常道。既然道不能被定义，那么每个人的心中的必然有自己的道。在我的理解中，所谓的道是你了解自己手上的东西，知道自己内心的想法，对于遇到的问题，虽然来自不同的范畴，然而你有自己处理的一套方式。方式不是实实在在死的步骤，而是一套方法论，正如毛选中“ 一个正确的认识，往往需要经过由物质到精神，由精神到物质，即由实践到认识，由认识到实践这样多次的反复，才能够完成 ”，问题是死的，人是活的，方法论是可以通用的。当一个人的心中生成了属于他自己的方法论，即属于自己的道，那么他便能够达到内心的宁静。道可以分为通用的道和自己的道，是游离于物质和精神之外的，因为每个人的道都是不同的，由此老子和波西格才认为道（良质）是不能被定义的，一旦被定义了，它便不是真的道。 《禅与摩托车维修艺术》的中译本似乎被人诟病，毕竟，翻译过来的版本不能百分之百传递原本的意思，我想，花点时间把原版过一过，很好奇那个时候会有什么新的发现。]]></content>
      <categories>
        <category>思想王国</category>
      </categories>
      <tags>
        <tag>读书</tag>
        <tag>禅与摩托车维修艺术</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[那些学校里的“书呆子”]]></title>
    <url>%2F2018%2Fthose-nerds-in-the-school%2F</url>
    <content type="text"><![CDATA[当然，想成为“书呆子”不是一件容易的事。 Nerd，在美国人眼中指的是那些偏爱钻研书本知识和沉迷脑力活动的学生，而这类学生被称为“怪咖”，在学校中往往是不受欢迎的。Nerd 对应中文世界的书呆子，书呆子意为只会读死书、生搬硬套，后来经过影视作品的形象塑造，如《社交网络》中的扎克伯格、比尔·盖茨等依靠脑力而赢得尊重的形象，书呆子甚至逐渐成为成年世界里一个具有能带来自豪感的标签。既然书呆子在学校中如此不受欢迎，这种“失败者”有什么值得研究的呢？事实上，书呆子是一个特别有意思的群体。 Paul Graham 的《黑客与画家》第一章《为什么书呆子不受欢迎？》详细地描述了他眼中的书呆子（当然，我写的肯定没他的好）。他在书中叙述的背景是美国的高中，我们大学校园里的书呆子的境地自然不会苦到有他说的那样被排挤得只能在角落吃饭。在 Paul Graham 的眼里，学校与监狱无异，老师们唯一的奢求是学生按照他们的意愿安安静静混完这几年。 豆瓣：黑客与画家 在校园里，书呆子是个奇葩的存在。书呆子一般比较聪明，倒不是因为与生俱来的天赋，而是他们往往比普通人更加善于专注于自己的事情。书呆子与一般所指的学霸还是有所区别的，学霸不一定是书呆子，而书呆子往往极有可能是个学霸，因为除了漂亮的成绩还有书呆子自己更加看重的东西。据我的观察总结，书呆子有五个特征： 1. 喜欢独处，除非不得已只能装作合群，毕竟他们真的不想一个朋友都没有 2. 他们经常陷入常人无法理解的无谓思考洪流中无法自拔 3. 低调不善言语，在他们眼里真正酷的东西很少，所以显得冷漠 4. 他们会在回报周期极长的事物上投入大量的时间 5. 执行力极其强，为了达到目的不择手段（参考《社交网络》男主角仅仅用半个晚上把学校的网络给整瘫痪了） 6. ······ 书呆子在学校自然是没多大地位的，学校本身是“流量经济”的最佳试验地，你在同龄人中的人气、受关注度跟你在群体中的地位呈显著正相关，你是否受欢迎也是融入某些圈子的入场券。如何获得“流量”？在流量为王的校园时代，极少有人会关心别人真正在想什么、你写了什么，这些要动脑子的东西太复杂了。他们独独喜欢能够迅速抓住眼球的图片，又或是不痛不痒的无病呻吟，这些更加能扣住人的猎奇心理，从而能快速吸引流量。因为人们往往没有耐心去挖掘一个陌生人的精神状态，相比需要大量精力观察的时间成本，更多人愿意通过一张修图过后的照片或者漂亮衣服来快速获得印象，两者的精力、时间成本不可比较。说真的，这样的环境挺没有意思的，整个校园充满了可怕的无聊与空虚，仿佛一座集体诈尸了的坟墓，一具具摇摇晃晃的躯壳四处游荡。 喜欢独处、思考的书呆子受到的关注少之又少（他们本来也不太喜欢），除非无法脱身而只能假装合群他们更喜欢远离团体。在学校，有的人从来不会去想自己是谁？喜欢什么？自己要成为什么样的人？有的人在学校表现良好，但是一走出校园就平平无奇了，为什么？因为他们根本就不知道自己喜欢的是什么，一旦没有人为他们设定目标就蔫了。其实学校中的老师不知道吗？不，即使明知这样他们也不会指出来，因为校方的任务主要是看好我们这群荷尔蒙无处释放的年轻人，四年后一根毫毛不少地还给家长就算完成了任务。学生们把大部分的时间花在如何吸引异性的注意力，如何在朋友圈收获更多的赞，挖空心思迎合老师而换取一个漂亮分数。相比之下，书呆子则把大量的时间花在了思考自身，如何写出漂亮的文章，如何练习一门生存的技能，钻研一些难题从而让自己看起来很酷。他们形成了独有的一套评价体系而对官方的标准嗤之以鼻，he doesn’t give it a shit。他们会对视觉内的事物有所感知，在平庸的人面前，书呆子只是一个离群的异类，也是一面充满威胁的“镜子”。大概是通过提升、训练自己的思维能力是个长周期回报的过程，他们会用短期收获的快感嘲笑书呆子的一无是处，他们也会因为书呆子可怕的认真而选择性视而不见。书呆子的成绩可能不会拔尖，但是他们的所受到的煎熬非常人所比。 然而，书呆子就没有未来吗？就只能当做失败者吗？错了，在格雷厄姆看来，书呆子只是在玩一个更接近成年人世界的游戏而已。书呆子在校园所受到的痛苦到成年人的世界里会转化为相当可观的收获。因为成年人的世界里，只计较对错，“能力”？“优秀”？oh, fxxk you, go home and play with your family！他们才不会因为你初出茅庐就对你放宽标准。比如当你去找应聘工作，你在校园里玩得风生水起的那一套就不大管用了，“具有优秀的团队协作能力”并不能使你成功应聘到管理岗位，“爱好兴趣广泛”、人气旺盛能帮我解决业务问题吗？能迅速学习新的知识并应用吗？至少一个对雇员有所要求的公司都不能，而书呆子在独处时练习的软件、钻研的冷门知识、写文章训练的清晰逻辑、独立的思考能力却能够帮助他们反败为胜。现实世界中需要的是能够解决问题的人，而不是只会傻傻地等待别人来告诉自己如何做以及制造问题的人。 我以上的观点可能有点刺耳，但是终究还是有那么一点作用的。]]></content>
      <categories>
        <category>思想王国</category>
      </categories>
      <tags>
        <tag>扎克伯格</tag>
        <tag>感想</tag>
        <tag>书呆子</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[环青海湖之旅（6.8-6.12）]]></title>
    <url>%2F2018%2Ftour-of-qinghai-lake%2F</url>
    <content type="text"><![CDATA[记录为期五天的环湖之旅。 “有些事情你现在不做以后就再也不会做了”，是这样的。放到现在，不太可能再经历长时间、长途的骑行。 跟好友强哥早早约好，半个月前就买了从长沙直达西宁的火车票，途径湖南、湖北、河南、陕西、甘肃和青海六个地区，历时 24 个小时到达西宁。上次来这还是六年前，相比之下，西宁并未有太多的改变。 在火车上，过了西安广袤的关中平原，然后到了甘肃，黄土高原的地貌逐渐显现出来。特别是陕甘交界的地带，你会切实感受到祖国的发展不均衡；在榆中县，光秃秃的山上连灌木都无法生长，地表基本没有河流，本地居民的经济来源从哪里来呢？ 本次环湖的起点在西海镇，也被称为“原子城”，这里是中国第一个核武器研制基地，想当年成千上万的人来到在这遥远荒凉的地方，只为凭空造出一颗原子弹。起先以为西海镇只是一个普普通通的镇级行政单位，还纳闷为什么无论是绿化、基础设施（图书馆、博物馆、公交车）都比海晏、刚察这些县城要好得多，后来才知道西海镇是海北州的政府所在地。至今，西海镇的建筑风格仍然保留着当年的革命奋斗气息，如沙漠黄的外墙上随处可见的毛语录，复古风格的大礼堂，巨大的毛泽东雕塑，可以想象那时人们的革命激情。 距离上一次长途骑行已经有 6 年了，相比上一次骑行，环青海湖的难度小多了：海拔起伏小、天气相对稳定、道路路况好、全程 300 多公里，唯一的阻碍在于自己想不想骑。每年的 6 月是青海湖旅游的淡季，一路上下来我们遇到的骑友不到十个（据说旺季的时候一天从西海镇出发的骑行者有 1000 多个），所以单车的租金也特别便宜。环青海湖的路况并不糟糕，在选择单车的时候我就选择了最便宜的一款 11 年捷安特的 770，环湖一圈 150 元（不计天数，学生还可以 9 折），骑行必备的装备租车行（明静单车行）的老板都会帮你准备好，十分方便。 提醒一下第一次去青海的骑友，一定要注意温差大的事实，晚上会下雪和冰雹，提前准备好防寒的装备，比如冲锋衣、抓绒衣，不然一到晚上（住的地方一般都备有电热毯）或者逆风有你受的。我就是因为衣服带少了，上身不得不套了两件短袖一件长袖还有一件外套；另外还要注意防晒，带好墨镜和做好防护措施，藏区毒辣的阳光不开玩笑。全程不必担心手机信号问题，除了去茶卡路段无信号，移动和联通全程 4G 信号满格。 Day 1：西海镇-湖东种羊场（40km）骑行的线路非常简单，沿着大路走就好了，藏区的人烟稀少，只见默默吃草的羊和牦牛，不见活人。需要注意的是呼啸而过的越野车和大卡车，车速都在 100 码以上，汽车轮胎与沥青路面摩擦的声音听着有点恐怖，而且大卡车经过的气流往往会影响单车的走向。 青海湖周边除了草原牧场，还有不少的小型沙漠，沙漠和积雪的山并存，不知道沙化会何时覆盖牧场。再往前面走一点就到了一个分叉路口，往右是去鸟岛和沙岛，在这里是环湖路上第一次能见到反射着阳光的湖面。此时我们非常兴奋，没有选择左拐而是直接往湖面骑过去了，由于没有参照物，原本以为只有一两公里的路程实际上有四公里，足足骑了 20 多分钟，后来证明这一切是值得的。阳光十分刺眼，天空万里无云，湖边除了耳边的风声和湖水冲击湖岸的声音，只有映入眼中的一片蔚蓝，此刻你会感受到自己是多么渺小，你只管一声长啸，却听不到任何的回应，波浪依然静静地拍打。 在此地停留了约半个小时，继续赶路，此时有两个选择，第一是原路返回，第二是从牧场中间斜插过去，重新上环湖东路，我们选择了后者。在没有参照物的地面行走时一件很痛苦的事情，软踏踏的草地根本骑不动，而且只知道往前面走，并不知道路况如何。就这样推推骑骑了半个小时，我猜应该是沿着环湖东路，突然听到了浑厚的狗叫声，担心的是会不会没有拴住，高兴的是附近肯定有人。还好狗是拴着的，一个藏族妇女在用锹铲土固定帐篷，男人则在一片漆黑的帐篷里面。藏人说话比较直，谨慎地问我们是怎么进入他的牧场的，我们说明了缘由，然后他才给我们指了路，告诉我们斜插过去是不行的，前面都是沼泽地，得顺着车轮印，然后顺着渠道一路往公路方向走。 按照当地人的提示，好不容易回到了公路，在牧场的转悠耗费了不少体力，加之毒辣的阳光，第一天从中午开始的骑行，我们打算把终点设在 20 km 后的湖东种羊场。也就是在这段路上，我们遇到了接下来几天一路同行的骑友：君哥和杰哥。君哥是四川人，资深骑友，说话不紧不慢，是个有故事的浪子，他骑过的地方太多了，数不过来。杰哥，湖南人，辞了职出来耍，自行车前绑了一个蓝牙音箱，一路踏歌而行。 藏区的日照时间很长，早上大约五点五十天就亮了，晚上九点多天还没全黑。我们到达湖东种羊场的时候刚好经过一个小学，此时已经七点半了，看天色像是早上八九点钟，有趣的是这里正在进行一场露天期末考试。 藏区的孩子学习太艰苦了，这样的教学条件也就是我小学时候才经历过，没想多二十年过后还会亲眼见到这般景象。 种羊场这个乡镇不大，找个住的地方很简单，至于条件就别奢求太多了，20 块一晚的板房还能要求什么呢？有的睡就不错了，热水，不可能的。不过，你要想想自己为什么来呢？如果要吃好睡好，待家里是最好的，但是，那又有什么意思呢？我建议只是想拍几张好看照片的朋友最好是自驾吧，一天半足够环完青海湖了，不用遭这个罪。 藏区昼夜温差大，晚上冷得不行，还下了冰雹。路上我最担心的就是天气，因为衣服没有带够是很麻烦的，然而路上还是比较幸运的，雨衣就用了一小会。 Day 2：湖东种羊场-二郎剑景区-黑马河（106 km）其实骑行中最关键的不是体力，而是耐心，一个又一个的缓长坡会一点一点消耗掉你的耐心，即便你还有力气，你也不想继续骑。第二天的上午的骑行很轻松，出了镇子就是下坡，一条笔直的公路，前方便是一座无名雪山，一路往雪山脚下骑。 然后我们碰到了此行中的第三个骑友——伟为，广东梅州人，今年刚本科毕业，打算环湖后去骑甘南地区。 中午在二郎剑景区稍作休整，担心下雨，整个下午都在为了躲避雨区而暴骑。其实，不必太担心下雨的问题，只要躲过了降雨的那块云就没问题，不必急着拿出雨衣。 因为之前咳嗽一直没有好完全，只要吸入冷空气就会复发，黑马河的海拔高，晚上又特别冷，整个晚上头都特别疼。住宿地方的老板是穆斯林大叔开的，人还不错。因为下午从二郎剑到黑马河出了点汗，晚上想洗个澡再睡觉，我们问老板有没有热水，老板打开水龙头对着三十多度的水，用生硬的汉语说，热水有的有的。这谁敢洗啊，忍忍吧。 伴着头疼，晚上迷迷糊糊睡着了，大概早上3点多，隐隐约约听见外面念念有词，一下就睡不着了，心想也不用这么虔诚吧，凌晨就开始赞美安拉。第二天早上碰到老板，聊了一会知道他们是来自西宁的穆斯林，我们去的时候是穆斯林的封斋期，日出之后和日落之前的时间是不能进食的，只能选择在凌晨吃饭。后来我们一行又跟这个回族大叔聊了一点关于宗教方面的东西，说了一会发现气氛有点尴尬，于是早早脱身去往茶卡。 到了藏区比较不适应的是吃的问题，选择不多，一般就是遍布天下的川菜馆，其余就是兰州拉面，物价贼高，而且味道也不敢恭维。比如，在黑马河乡的早餐，油条 3 块钱一根，豆浆 3 块钱一小碗，花卷 2 块钱一个，没办法，sorry，景区宰客就是这么没道理。青海地区的回族居民密度比较大，到了青海之后你会发现关于吃你可能的选择不会太多，除了清真还是清真，只有偶尔出现的几个川菜馆。西宁的人口少，出了火车站你会发现有地铁口的指示牌，然而此地并无修地铁的必要，即使是高峰期也没有多少车辆。 Day 3：黑马河-茶卡盐湖-黑马河（包车，约 60/人）第二天的骑行太累了，加之去茶卡的路有约 90 km，还有一座大山，我们决定包车去茶卡，谈好了价格一人来回 60（本来 50，由于在景点超时，司机加价 10 块）。我提醒藏族司机小伙，记得在山路上要慢点开啊，他回了一句让我？？？的话，“放心吧，我是有驾照的~的~的~”。之前我是在 318 见识过藏区交通的乱象的，大卡车根本不挂拍照只挂活佛和经幡，真是生死有命啊。 从黑马河乡出发，走过一段长坡，翻过一座积雪的山顶，就进入了炎热干燥的柴达木盆地。国道的旁边就是京藏高速，偏远地区就是法外之地，国道和高速之间是可以任意切换的，也没有交警过来查你。 茶卡盐湖被《国家旅游地理杂志》评为“一生必去的55个地方之一”，被称为中国的天空之境，也是柴达木盆地四大盐湖之一，原本这里是被海水覆盖，后来由于板块运动，海拔升高，逐渐形成了内陆盐湖。藏族司机给我们在景区游玩的时间为两个小时，买票的人并不是很多，花了 70 块买了门票。要亲眼见证天空之境还是需要运气的，如果是阴天的话就什么也看不到，早上和中午的阳光太刺眼也不太适合，下午 5 点多以后是最佳的观赏时间。 Day 4：黑马河-鸟岛-泉吉乡-刚察县（120 km）、Day 5：刚察县-哈尔盖村-西海镇（87 km）不得不说，好久没有长途骑行，你会发现体力着实下降不少，一天 80 公里以上的路程竟然会有点吃力。最后两天的骑行已经渐渐远离青海湖，公路距离青海湖湖面有大约三四公里，湖区周边的藏区人烟稀少，走了好久都见不到一个人，偶尔能够见到赶着牛羊的人们，似乎这里的商业化气息比较浓重，对于远方来到的客人也就见怪不怪了。 鸟岛在六月份还在保护期，不允许游客进入，大街旁边的商店都冷冷清清。此时正值湟鱼洄游的季节，站在布哈河大桥上，黑压压的一片湟鱼逆流而上产卵。初期，由于居民信教，不吃没有蹄子的动物的肉，又认为鱼和龙属于同类，吃了会不吉利，所以青海的湟鱼多得溢到了海面上。而现在，湟鱼由于人为的肆意捕捞而数据急剧下降，现已经被列为了湖区的重点保护动物，据刚察县的川菜馆的老板说只有大胆的本地居民会在晚上去河里捞几条，然后偷偷卖给餐馆，一锅黑市湟鱼的价格是 280。 最后一天的骑行是从刚察到西海镇，最后的几个缓长坡可把我累坏了，还好有惊无险地完成了骑行。 短短的几天，碰到了形形色色的人们，尽管在第一次遇见他们的时候或许不理解他们的举动，然而你在深入了解了以后就会发现，这才是百态的人生。人生不止只有一种设定，人生不止眼前的苟且，更有诗意的远方，有的人没有房子，辞了职，不用结婚，一样可以活的潇洒自在。我不太赞同他们的生活方式，但是我尊重他们选择生活的权利。 总是在书中读到古人的诗词，西塞边疆的大多或悲凉或豪情万丈，以前总是不太理解。后来我明白了，只有当你真正地来到这些地方之后，你就会意识到自己在世间是如何渺小，不过一粒沙子而已，你的声音、情绪在自然面前一无是处，任凭你长啸而过也留不下任何痕迹。这或许就是“渺沧海之一粟”和“凌万顷之茫然”的真实写照吧。 如果下次还有机会的话，新疆见！沿海线见！大西南见！ 只是，我再也不想骑单车了，真的好累，选择摩托车或者自驾吧，走遍中国！]]></content>
      <categories>
        <category>其他</category>
      </categories>
      <tags>
        <tag>游记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《总统是靠不住的》笔记]]></title>
    <url>%2F2018%2Flinda-look-inside-american-1%2F</url>
    <content type="text"><![CDATA[林达写这个系列的时间比较早，上个世纪八十年代，现在读来可能没那么新奇，毕竟信息的来源多了，新闻、美剧（纸牌屋、国土安全）中多少会有点了解美国的情况。对于像我这类对美国历史不是特别了解的人群，通过阅读林达对两任美国总统在任期间出现的危机有条有理的叙述，至少可以在认知程度上不会停留在模模糊糊的程度，有自己的一点体会。其实，美国总统的权力没有想象中的那么大，也没有被限制得只是某些利益团体的代言人。书中给我的几点感觉比较有趣： 为什么美国的各个部门会积极地去推动案件的进行？比如“水门”窃听案和“白水门”，给人的感觉是似乎司法部门接二连三地跟商量好了一样要积极联合起要“整”尼克松和克林顿，为什么会有这么大的动力呢？当然，以我从影视作品中得来的印象，不排除其中有竞争对手作梗的嫌疑，但是更多的时候我会疑惑为什么美国的这些官员会非常天真、积极地去履行所谓选民赋予他们的职责，难道仅仅是为了自己的政治前途吗？ 正好，我认识的一位老师看完了这个系列，我在跟他讨论这个问题的时候，他说大概是因为两边的从政者接受的理念不同，他们会更加忠实自己的政治信仰，宣誓效忠于 1789 生效年至今一字未改的美国宪法，诸如此类的官员有“水门”事件中的独立检察官考克斯、大法官马歇尔。我觉得问题没有出在这个地方，我是也一名共产党员，我们党员也经常要上党课、写思想报告、学习马克思列宁主义，可为什么官员的积极性却不像美国那样积极地去推动某些案件呢？敢于触及某些利益呢？而是非得要等到舆论的匕首刺到眼前才会有所动呢？不排除存在正义感极强的政治家，我觉得官员更多的考虑是民意吧！因为，这些官员是费尽千辛万苦拉选票一票一票竞选上来的，既然是这样，选民们既可以选你上来，自然也可以踩你下去，你不好好干，那些看热闹不嫌事大的媒体可愁着没东西可以写呢，到时候你怎么交代？“水能载舟，亦能覆舟”就是这个意思。 这样折腾真的划算吗？ 当然，这样的制度设计，也许扼杀了一个高瞻远瞩的政治伟人的宏大抱负，也许，也使得美国人民失去了一些“起飞”的历史捷径。但是，他们愿意支付这些代价的原因，是他们不愿意冒险失去他们掌握自己命运的权利。 看本书的时候，自然会拿大洋彼岸的美国和中国对比。美国崇尚自由，个人主义至上，然而中国则强调个人必须得服从集体，这是两种截然不同的价值观，三观不一样，怎么做朋友？ 林达在书中多次强调了“代价”这个词，相对庞大的政府，个人永远是弱势群体，为了个人的权利和自由不受侵犯，制度设计者宁愿不走捷径。所以，新闻上经常说美国修好一条高速公路得要十几年，当美国人还在为了如何不浪费纳税人的钱、如何设计线路而争吵不休的时候，在中国， 1350 吨桥梁整体换梁仅仅需要 43 个小时。对比之下，当中的“代价”显而易见，美国人会在喋喋不休的争吵中逐渐被原来的小弟赶超，中国人则暂时必须忍受粗暴发展带来的后果。孰优孰劣，我自然是没有资格评论的，不过能意识到这个问题我已经感到很欣慰了。 你是保守派还是自由派？看了林达对保守派和自由派的叙述，如果我是美国人的话，我想我应该是保守派吧。即使是在美国，自由和个人的权利也是个争论不休的话题，最明显地就是保守派和自由派之争，比如对于堕胎、持枪、LGBT 的看法，要是把这两拨人放在一块，估计到世界毁灭的那天没有结果吧。 按理说，在美国土地所有者在有权利处置土地上的财产吧，然而一件事打破了这个规律。比如，美国红杉是一种极有价值的树种，能存活几千年，谁家后院长了一颗红杉照道理说就可以卖了发财了。然而，保守派认为树长在我家，我爱怎么地就怎么地，谁也管不了。然而自由派认为红杉是稀有资源，不能随便动，长了几千年砍掉一颗就没了，愤怒的自由派甚至会冲进别人家阻拦。所以，即便在美国，个人的权利也是有限度的，存在很大的争议空间，不是想干嘛就能干嘛的。]]></content>
      <categories>
        <category>思想王国</category>
      </categories>
      <tags>
        <tag>读书</tag>
        <tag>随笔</tag>
        <tag>林达</tag>
        <tag>美国</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[备忘录（二）]]></title>
    <url>%2F2018%2Finterview-in-machine-learning%2F</url>
    <content type="text"><![CDATA[模型的评价指标分类任务 查准率 查全率 ROC 曲线：比较 AUC 的大小，AUC 是研究机器学习模型泛化性能的工具，横轴{假正例率，FP/(FP+TN)}，纵轴{真正例率，TP/(TP+FN)} 混淆矩阵 回归任务 均方差 灵敏度和特异度 R 方：评估模型拟合度的好坏 推荐算法 基于内容 协同过滤：基于用户和物品 基于知识 Bagging 和 Boosting 的区别 Bagging：处理过拟合（方差）；分类器之间相互独立；关注方差（注意数据扰动带来的影响） Boosting：分类器序列相关；降低方差和偏差，关注降低偏差 比较 LR 与 SVM 的区别 LR 是一种概率模型的手段，SVM 试图找到一个超平面 参数估计的方法：LR（最大似然估计法）；SVM（拉格朗日乘子法） SVM 的泛化性能更好，受异常点的影响比较小 LR 在不平衡数据集上的表现优于 SVM 统计学习方法的三要素模型、策略和算法 统计学习方法的步骤数据→模型集合→选择模型→实现算法→选择最优→预测/分析 决策树生成计算方法 ID3：信息增益算法 C4.5：信息增益比 CART：基尼指数 防止过拟合的手段 早停止，如果模型的性能没有提高则停止训练 增大数据量 正则化 交叉验证：留一、K 折 特征选择、降维 dropout 比较 L1 和 L2 正则化 L1 减少特征的数量 L2 降低特征的权重 L1 最优解出现的地方往往在坐标轴，L2 的最优解则比较随机 继续阅读本站其他精彩文章 机器学习 编程语言 技术碎碎念 读书笔记]]></content>
      <categories>
        <category>Machine-Learning</category>
      </categories>
      <tags>
        <tag>正则化</tag>
        <tag>机器学习面试</tag>
        <tag>模型评价</tag>
        <tag>分类</tag>
        <tag>回归</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo 博客多个设备同步详细步骤]]></title>
    <url>%2F2018%2Fsync-hexo-blog-among-different-devices%2F</url>
    <content type="text"><![CDATA[0. 背景hexo 作为一个轻量级博客框架，具备简单易用的特点。作为一个 hexo 的重度用户，笔者在使用 hexo 的时候经常会碰到一个大家都会遇到的问题：更换电脑后如何重新写作？ 下面不用视频了，看我操作就行了，请大家看看我是怎么在新的设备上重新写博客的。 1. 操作流程1.1 安装 node.js 和 git（必需）123sudo apt-get install nodejssudo apt-get install git 1.2 设置 ssh keys更换新的生产环境后，hexo 生成的静态网页文件需要通过部署到 git，所以必然要与 GitHub 通信。设置 ssh key 就好比 github 给你的设备一片钥匙，有了它，你的设备才可以在 github 上进出自如。 （1）检查是否有 ssh key，如果有则删除掉 1ls -al ~/.ssh （2）在本机上生成 key 1ssh-keygen -t rsa -C &quot;your@email.com&quot; （3）添加秘钥 1ssh-add ~/.ssh/id_rsa （4）拷贝公钥到 github 12cat ~/.ssh/id_rsa.pub | xsel -b # 需要安装 xselxsel &lt; ~/.ssh/id_rsa.pub # 拷贝公钥内容至系统剪贴板 这时公钥内容已经在系统剪贴板上了，打开 github 账号：settings -&gt; ssh and GPG keys -&gt; New ssh key，然后 ctrl + v，把公钥粘贴到文本框里，title 可以随便起，添加成功。 1.3 将博客的源文件部署到新的分支hexo 的代码分为两个部分，分别是静态网页文件和网站源代码，每当 hexo 需要重新更换生产环境，不需要像笔者之前那样将整个文件全部拷贝下来，而仅仅需要几个关键文件就好了。 （1）把项目克隆到本地 1git clone https://github.com/username/username.github.io.git （2）新建 source 远程分支 进入 username.github.io 文件夹，创建名为 source 的新分支 1git branch source （3）推送网站源代码至 source 分支 123git add .git commit -m &quot;add SOURCE CODE to source branch&quot;git push origin source 至此，你的项目中就有两个分支了，master 分支负责保存生成的静态网页文件，source 分支负责管理网站的源文件。 但是这样做有些不好，它会把你网站所有的源代码都暴露出来，并且 github 并没有仓库私有化，所以只能将网站的源文件备份到另外一个代码管理网站：coding。 1.4 根据 package.json 安装依赖1npm install 1.5 安装 hexo123npm install hexo -gnpm install hexo-cli -g # hexo 的命令行模式 网站根目录下测试 hexo s、hexo g、hexo d，如果没有错误信息则表示同步成功。 参考链接 Node.js 安装配置]]></content>
      <categories>
        <category>教程</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[再见，2017]]></title>
    <url>%2F2017%2Fgoodbye-2017%2F</url>
    <content type="text"><![CDATA[第一次写个人的年终总结，想给不安分的 2017 年画上一个不完美的句号。人的记忆很多时候不大可靠，如果想活得更加明白一点，每年都企求进步，及时的思考总结是非常有必要的。时至今日，2017 年大概已经积累了大约 10W 字的书写量，定期写作的习惯已经成为了我生活的一部分，自然做一个年终总结是很有必要的。 前前后后写了大概半个月，这份不太长且有点杂乱的总结梳理了我在 2017 年的一些关键节点，我给自己找出哪些地方做得比较好，哪些需要改进，以及如何改进。 1. 这一年发生了什么？ 沪昆线在距离学校不到两公里处，我特别喜欢听远远传来的火车鸣笛声，鸣笛声意味着出发、意味着离开、意味着新的开始。 跟大家一样，今年大部分的时间没有在学校，要么实习，要么独自在 302 机房上机、看书、读论文、写博客，除了每天晚上回去睡觉的宿舍，302 应该是我停留时间最长的地方吧。南来北往，在几个城市停停留留，很庆幸每新到一个地方总会结识一帮志同道合的新朋友或是偶遇老朋友。外面的世界很精彩，也很残酷、现实。我是一个比较喜欢体验新事物和环境的人，“新”代表挑战、不确定，不确定才有意思，一成不变简直要人命，所以极有可能固有的角色会因此而临时发生改变。所以，你新到了一个地方就再也不想待在原地是有原因的，那个时候便会明白什么才是“回不去的故乡”。 2017 年成长了许多，同时我也发现自己越来越无知了，不懂的东西着实太多，时间太少。很多时候，当我花了很多时间写了一个东西，做完之后感觉自己非常牛逼，恨不得马上告诉别人我有多厉害。但是这个冲动仅仅能够维持一分钟吧，因为马上发现你的想法早就被别人实现过了，一文不值，没有炫耀的资本。所谓见得多了，也就知根知底。另外我也学会尽量以平和的语气和态度处理身边的人和事，管理自己的情绪。物以类聚，人以群分，你的为人处世就决定了你身边的圈子。大多数时候是由于我们自身的所作所为而导致自己碰到不公平、令人愤怒的事情。 1.1 日出 打开车的窗，太阳在头上，公路的右方，无边的海洋 过完年，做完一个决定后，心情郁闷，回到学校的时候还比较早，食堂并没有开门。 刚到宿舍，站在阳台上抬头一看，晚春的天气还是不错的！突然就想出去走走！从本科的时候开始就想骑单车去衡山，由于各种原因不了了之。说走就走，把室友从家里叫过来，中午吃完饭就去汽车站坐大巴，记得还买错了车票，因为上大巴之前一直还以为衡山在衡阳市。为了看到日出，晚上简单睡了一下，披星戴月，凌晨两点半就开始出发，同行的还有一个驴友，听说他春节都没有回家，在火车上过的。 幸好大月好光，手电筒也省了。从山脚到半山亭之间登山的人不多，过了半程之后人才开始渐渐多了起来。连续不断一直爬了四个多小时才登上望日台。第一次完整地观察地平线的时候十分震撼，视线的尽头尽是未知与混沌，天与地的界限从未如此清晰。当太阳快要突破地平线，希望似乎也一点点燃起，心里默默在想，今年运气应该不会太差吧。紧接着，视线尽头的混沌慢慢清晰，“上帝说：要有光，于是便有了光”。 1.2 抉择 鱼和熊掌不可兼得 从衡山回到学校后，再次捡起寒假前完成的小论文初稿。此时内心有点纠结，数据处理、技术实现的部分完成得差不多了，但是经济管理的理论解释部分让我提不起太大的兴趣，并且一直也找不到一个合适的切入点。鱼和熊掌不可兼得，去年的这个时候就打算，不管怎样先把论文的草稿写下来，来年尽可能走出学校实习。当时已经三月份了，按照跟导师交待的计划，两者必放弃其一，我选择了后者（这里我要感谢我的导师给予我的充分自由，尊重我的选择）。 既然选择后者，那就走吧，想来想去，考虑了一下自身条件，唯一拿得出手的就是那个写了几篇机器学习算法笔记的博客，关键缺乏动手实践的经验。既然没有经验，那就做吧。不会做？那就照着教程做吧。Google认证的机器学习工程师学费得5k+，学生党自然是掏不出“巨款”的，那就免费的呗。先后尝试了优达学城和fast.ai的深度学习公开课，依葫芦画瓢完成了5个实验。不管怎么样，至少是动了手吧，不会被别人嘲笑只会纸上谈兵。后来发现，这些依葫芦画瓢的东西对后来的学习还真管点用，毕竟实战的经验才是最重要的。 比较幸运的是，两家公司接受了我的实习申请。因为帝都的机会相对其他地方较多，所以地点都考虑在北京。从学校走向企业是一个角色的转变过程，校园里你是“消费者”和“顾客”的角色，学校和老师是为你服务的，而企业不同，在企业里就变成了服务的提供者了，刚开始都会不习惯转变，角色的转变过程还是尽早经历比较好。这段提前的经历给我最大的收获是更加了解了自己，大概知道自己的方向该往哪里走，明白哪些地方做得好，哪些地方还需要继续踩坑。出来走走后对比发现，自己之前眼界很狭窄，人外有人，牛逼的人比你还努力。 1.3 归西湖南的四季并不分明，春天和秋天非常短暂，不是酷热就是严寒。虽然到了四月份，却丝毫没有春天的感觉，一连十几天的阴雨是常有的事，人的情绪很容易受到影响，那段时间失眠也很严重。就在这个时候，我的爷爷，也就是我之前写过的“香嗲”，时隔两年某日再次摔倒在地，这次状况很严重。 从微信看到发过来的视频之后，心想这次爷爷这次应该是撑不过了，当天就从学校回到了家里。人快要归西的时候是很容易感知的，如果说有气味的话，那就是死亡的气味。我到家的时候他说话都不利索了，唯一一句听清楚的话是晚上问他开不开灯，他说“不限定”（方言：没必要）。爷爷一生勤俭持家，直到临死前还惦记着不要浪费用电。等子女都到了家里，没过两天他就走了，走的时候我在场。等家里的事情处理完毕，回到学校，四月已经过了三分之一。整理好心情，继续准备寻找实习工作。接下来发生的事情在北京，后会有期一文中有详细的记录，这篇总结就不再赘述了。 1.4 秋招 江湖上闯名号，从来不用刀 秋招市场是惨烈的，野路子文科生屡屡受挫。但是我又不太容易妥协，如果认定了要做什么的话就不会轻易地去改变，不大可能中途修改路径。身边的同学清一色考银行和公务员，没办法，想组个团都没办法，只能靠自己。基础差能怪谁呢？你说你快速学习能力强又怎么样呢？你说你有相关实习经历又怎么样呢？我需要你是 cs 科班出身，精通算法，最好是 985/211 学校毕业的。从北京回来的第二天就去长沙参加菊厂的宣讲会，没想到宣讲会日期早就改了，扑了一场空。9 月和 10 月很焦灼，每天不是在做笔试就是在寻找招聘信息，大大小小的笔试做了快二十几个了吧。对于我来说，笔试很难，因为我根本不会做，大多数笔试最后的算法题直接交了白卷。笔试大多挂了，好不容易笔试过了，面试又挂了。面试过了，跟 HR 又直接谈崩。挂的次数多了，心里也就坦然了，急也急不得，慢慢来吧，此处不留爷，自有留爷处。如果你把握好了自己的节奏，内心就不会被外界所干扰（脸皮变厚），理想主义者修炼的秘诀就在于此。 种一棵树最好是 10 年前，其次是现在 可是我就是想做编程！可以很负责任的说：计算机是最不需要背景并且容易能够凭借个人努力而有所收获的，你想要的答案在 Google 上都可以找到。学习编程什么时候都不算晚，算起来，真正接触编程也不过一年多一点时间，如果真正想学，什么时候都不晚吧！回想研一刚开始的时候，根本不知道编程语言为何物，亦不知道该从哪处着手，我的导师告诉我：Python 和 R 里边随便选一个吧！R 语言在数据分析、经济领域的应用比较多，而且比较容易上手，所以 R 成为了我的入门语言。然后，我又买了很多 R 语言方面的编程书籍，打印网上下载的 PDF 资料，没事就去翻翻看看，在笔记本上敲敲，就算是入了个门。 后来 R 语言入门了之后（其实就是明白了一点点皮毛），我觉得自己的统计学还不够好，所以买了 统计学习基础（ESL）和统计学习方法两本书（后来才知道它们根本不是统计学！==！）。统计学习方法很难，刚开始看的时候根本看不懂，异常吃力，大概就看了前面几章，对照着书本把公式推了一遍，然后写了一系列的笔记。再到后来大量接触 Python，Python 逐渐成为了我的主要语言，陆陆续续用 Python 做了一些算法模型、后端开发的工作。 “商学院的文科生怎么也来搞编程了？！”、“你去银行吧，他们说银行年薪10w+”、“考个公务员也行啊”、“为什么要去北京，湖南不也挺好的吗？”。校招季耳边也响起了不少这样的声音，如果听到了，最好想想当初为什么要出发吧！怎么说呢，后来想通了，这个过程是必须要经历的，迟早要来的，一眼也可以看出来这不是最后一次，习惯就好了。 2018，继续迎风向前吧！ 1.5 写作这一年里共计发布了 74 篇原创文章，长短不一，累计约 10W 字，涵盖技术、读书笔记、心得体会，文章主题分布在 4 个板块： 机器学习相关：http://www.libinx.com/categories/Machine-Learning/ 编程语言相关：http://www.libinx.com/categories/coding/ 技术碎碎念：http://www.libinx.com/categories/%E6%95%99%E7%A8%8B/ 读书笔记：http://www.libinx.com/reading/ 2. 道系青年 道不同，不相为谋 我对无聊的社交深恶痛绝，在校的圈子很少超出两个寝室的范围。读研的生活不同于大学，大家关注的内容、兴趣喜好差别巨大，从而导致社交网络中的圈子界限很清晰，联系次数少了就越走越远。我比较厌恶无聊的聚会，我觉得“表面兄弟”是一件很浪费时间的事情。大多数时候，孤独是一种常态，因为我想腾出比较多的时间来做自己的事情，也不管这些事情是否能够被认同，我想我大概已经渐渐习惯这种状态了吧。一个人工作的时候很爽，不用征求别人的意见。比如，周末想去闲逛或者去公司学习都不用跟谁商量，而且执行效率奇高。再比如，到了饭点不用迁就别人，米饭、面条、面包什么都可以，不用一块跟着吃垃圾食品。 有人总结过，世界上所有的事情可以归为两类：一类是关你屁事，一类是关我屁事，如果明白了这个道理，许多问题会变得异常简单或者根本不会出现。当然，这个总结有些极端，但不乏借鉴之处。并且有一点可以很明白地说，成年人的世界里，绝大多数的聚会、社团活动都是“表面兄弟”和“塑料姐妹花”们在一起浪费时间。 在运营网站的时候，我经常会拿自己的站点跟别人对比，思考为什么他们的流量、他们的文章就是有那么多人看，而我认认真真写的东西却无人问津。是因为文章水平不够吗？原因肯定是一方面，但觉得不是主要因素。从我自身的角度来看，我想到了以下两个点，一来我不太善于表达自己的想法，对自己的信心不足，往往过不了自己这关。不得不说，我发自内心地不带任何偏见地佩服那些能把六十分的作品包装到八十分甚至更高的人。二来自己的技术圈子实在太小，有的时候碰到问题实在无处求解，再加上自己急躁，往往解决的过程会很漫长。孤独的状态是极好的，同时也非常糟糕。极好之处在于，你的注意力会高度集中而不会受到外界干扰。糟糕之处在于，需要在技术路线选择上走很多弯路，再者出了比较麻烦的技术问题，往往需要在搜索引擎和书籍中花费大量的时间。比如，今年花了不少时间学习 C++，学习 fast.ai 提供的公开课，鼓捣网站顺便学了一点前端，Hadoop ，学习 Java，死磕算法，但是遗憾的是，这些都由于各种原因没有继续坚持下来。 3. 如何成为一名真正的工程师？ 11 月是充实的，每天都在接触新的东西，Python、git、webhook、flask……我的征途是星辰大海 11 月是压抑的，不想打开微博，三色幼儿园、江歌、低端入□……看多了只会对这个世界越来越失望，似乎这个城市一夜之间不欢迎你了，而不管你正义感如何爆棚，然而你发现你并不能改变什么，毕竟大众的记忆力不过一周，风头过后一切照旧，他们好像也知道这个秘密 两年前还没入学的时候，我给自己定下了一条比较独特的路：立志从文科生转型成为一名优秀的工程师。在我看来，工程师是一个比较有趣的职业：一、在工程师的职业生涯中，在项目中把自己或者别人的想法实现是一件很有成就感的事情；二、它做不来“假”，你的简历做得再漂亮，杂七杂八的证书水得再多，经历写得再精彩，面试官几句就能把你问倒；三、人际圈子比较简单，不用在处理社交关系上花费太多的时间，实际上，我接触过的工程师大多还比较 nice。一句话总结也就是，你行你就可以上，不行就下去把技术练好再上来，两边都服气，这也是我特别喜欢这个职业的原因。 对比一下，距离成为一名优秀的工程师还有很长的路要走。 职业生涯的第二次实习是在文因互联度过的，说起跟文因的渊源，还得从鲍老师的微博说起。大概在 15 年，无意中感觉微博的时间线上“西瓜大丸子汤”这个 id 出现的频率很高，当时的感觉是，我艹，他怎么什么都知道！后来文因互联一周年的时候，鲍老师在微博 po 了一张公司全家福，从那个时候才知道了文因这家创业公司。鲍老师在微博发了很多干货文章（实在的“招聘广告”），其中那篇教你如何写一封得体的求职信对我影响最大，第一份实习工作就是写了大量的求职信才得到的。 因缘际会，十月的某个时候，鲍老师的微博转发了大苏写的一篇文章，顺手文章添加了底部的微信公众号 wenyinai42，小助手很热情地询问了我的一些情况，当时我正在找工作，碰了不少壁。转发了简历后，没想到我的博客成为了为数不多的亮点，第一次我感觉坚持做的东西还是有人认可的。10 月 23 号下午，接到一个来自北京的电话，当时很紧张，巴拉巴拉说了一大堆。最后，鲍老师问我什么时候方便来北京看看，毫不犹豫地就说下个星期。 要问北漂者最头疼的是什么？我想除了“租房子”，大概想不出更麻烦的了，短租尤其恶心！11 月 1 号，天气不错，坐了一个晚上的火车，第二次来北京没有那么生疏了。到达住的地方，实在太挤了，那一阵着实体验了一把什么才是真正的北漂！简单洗了个澡，中午饭也没吃，两点到了公司，领了器材、注册好邮箱、协同工作账号，还没缓过神来，啪，开始处理一个接口适应性更新的任务。或许这才是真正的创业公司吧！ 在文因实习的一个月时间里，却让我实实在在体验了什么才是真正的工程师文化。这里的工程师都很 nice，即使他们比你的技术高了很多段位，他们仍然会很平和地回答你提出的“小白”问题。这里的工程师很有创造力，都有自己的闪光点，也就是不可代替性。hack 一下，没有什么问题是一段代码解决不了的，如果有，那就是两段。在解决一个问题的时候，不要想着一开始就 bug-free，如果你想造一辆汽车，别的不说，四个轮子，两张沙发，先跑起来再说！这家公司更像是一所学校吧，记得有次开会开玩笑说，工作三年应该可以算是修了一个 nlp 的硕士学位了。 实习涉及的工作主要涉及到后端的开发，一个星期处理一个任务。因为之前接触大多跟数据模型和算法，后端开发的还不太熟练，好多问题都需要请教小组的其他前辈。我逐渐发现自己的两个缺陷：一、逻辑抽象能力比较弱，实现的思路往往比较混乱，思路想好之后又怕麻烦吝于动手，导致浪费了不少时间。二、迷信算法，算法不能解决所有的问题，完全依赖算法是一种很不负责任的行为，实际的工程中大大小小的问题都需要考虑到，异常怎么处理？如何让程序更稳定？比如，文本信息的摘要，一开始我就想着怎么调用现成的库生成，后来还是老老实实地用正则表达式一条一条去匹配。后来又接触了 IDE，学了几招断点 debug，抛弃了一次性 bug-free 的想法，能写多少是多少，出了问题再增删改，这时才感觉慢慢步入正轨。工作的时候很担心自己做不好，倒不是怕有什么后果，只是觉得感觉自己很没用，所以给自己施加了挺大的压力，再加上住宿的条件又不太好，经常性睡不着，一个月体重掉了好几斤。不过谢天谢地，没有出什么大乱子。 这段日子过得很充实，能够看得到自己一点一点的成长，从一开始大量 copy 别人的代码到慢慢积累了一些独立处理任务的经验，心里感觉也越来越有底。 在文因，六点吃完阿姨做好的饭菜，六点半 demo 后基本上你的时间就自由了。demo 有一个好处，它能够强制性地让你梳理当天的工作内容。以我的习惯，demo 完成后，一般还会在工位回顾和处理当天没有解决的问题，整理一下开发笔记，然后 deploy 到博客上。通常，做完这些工作也就差不多十点半多了（我得声明，这不是“图表现”什么的，纯粹是个人的习惯）。不像南方，北方的室内和室外温差极大，下班走出圣世一品，再走到垃圾回收站，室内储存的热量消耗完毕，全身的血便开始全部往唯一暴露的部位头顶上冒，充满血的脑袋奇涨无比，导致几分钟内说话都不利索。其实，这次来的时候就带了厚的裤子和羽绒服，只是放在包里实在懒得打开。后来，在会芳小姐姐的提醒下终于穿上了保暖裤和羽绒服，问题解决！ 在北京的日子时候蓝天白云居多，很遗憾没有目睹纯正的“京霾”。印象比较深刻的是 11 月份的时候一个人去爬了香山，那天天气不错，整个北京城都在视线范围内，给我的感觉是：真的好大！另外，还短暂体验了一把有暖气的冬天，给我的感觉是：北方人真幸福，室内就算不开暖气都比南方暖和，不像在湿冷的南方无处可躲。唯一比较不适应的是饮食习惯，作为一个嗜辣的湖南人，饭菜里怎么能没有辣椒呢？于是经常产生以下对话： Q：“这个菜为什么也要放辣椒？”A：“为什么不放辣椒啊？我们的汤里面都放辣椒！” 下次有机会一定要好好找找湖南米粉店，实在受不了只有煎饼果子和豆浆油条的日子。 4. 方法论这世上本没有坑，踩得人多了，也便有了坑。工程实践的路上永远布满了坑坑洼洼，它们就在那里，经常在路上走，哪里有坑，哪里比较近，自然心里会有数。我的工程经验还不是特别丰富，不过我也渐渐总结了几条“心法”，这里总结下来在以后的实践中作为参考。 4.1 今天没有解决的问题，明天说不定就好了，不行就后天做开发，一定不能急躁，急躁也没有用。情侣间的矛盾处理有不隔夜之说，矛盾尽早解决比较好。然而在解决难题的时候，火候没有到，再急躁也没有用，手忙脚乱只会徒劳无功，倒不如耐心等待。这似乎成为了一条没有证明的定理，曾多次在开发中碰到难题，一般最多不超过两天，问题自然而然地就水落石出。我也说不清楚个缘由，也许是遇到的问题根本就不够复杂吧！然而，每当我想破脑袋也解决不了问题，我就安慰自己，“说不定明天就好了”，结果，神奇般的是往往如人所愿。其实，我更喜欢将此方法论命名为“take it easy”，无论做什么事，不要急。 4.2 分清楚主次和优先级分清主次是解决问题的高阶技能，它意味着你必须得权衡，权衡之后便会存在损失和放弃，放弃则存在着机会成本。每当需要解决一个崭新的问题，思绪往往非常混乱，不知该如何着手，你会一头扎进搜索引擎的海洋无法自拔。或许最终你还是可以弄清楚问题的来龙去脉，但是过程中却需要耗费大量无关的时间。 那么，适合我的方式是什么？经过几次这样的心里挣扎，我总结了这么一套思考过程：先问问自己，我要解决什么？我该如何解决？问题可以分解成为几个模块？各个模块各自实现的难易程度如何？想了很久，仍然没有思路？站起来，去上个厕所，喝口水，走动走动，重新回到椅子上，相信我，会有惊喜发生的。不要相信记忆，脑子一片混乱的时候，记忆还可靠吗？写下来，写在草稿本上，在本子上随意写下任何思路。当然，回答以上的几个问题也难以摆脱扎入搜索引擎的命运，不过相对来说，效率会更高一点。 4.3 尝试将问题切分成小块紧接上一条，分清楚了主次和优先级，先做什么后做什么已经明了，接下来便是各个击破。不要想着一个大招就完成，take it easy，先写草稿，细节方面的暂时不要太在意，写得烂不要紧，代码能跑通再说。我在很多地方都提过马斯克造汽车的例子：如果要造一辆汽车，先给四个轮子两排沙发，能动起来再说。 4.4 清晰地表述问题关于如何清楚地表达自己想要问的问题，《学会提问——批判性思维指南》专门有讲。清楚地表述问题有两个好处，第一、当你遇到一个棘手的难题，往往会不知所措，从而一头扎进搜索引擎，这样的做法往往效率十分低下。这时你便会去求助别人，伸手党是最不招人待见的，随手在论坛、QQ群抛出一个问题就想得到满意的回答，这么自私的行为有人会帮你吗？不会。最基本的，先问问自己：（What？）你需要解决的问题是什么？有没有在搜索引擎上寻找相关的资料？你有做过哪些尝试？有仔细看过尝试后的错误信息吗？如果你做了这些尝试后仍旧无法解决，极有可能这个问题超出了你的解决范围，此时便是你向其他人寻求帮助的最佳时机，懂得人会非常乐意帮你解决。不过以我的经验来看，一般问过自己这些问题之后，解决的思路往往就很清晰了。 4.4 “少谈点主义”我讨厌泛泛而谈，喜欢说大话的人。别人不会关心你想做什么，只会关心你做了什么。 写了这么多，该跟 2017 告别了，你好 2018！ 最后给大家分享今年看到几篇印象比较深刻的文章： 阮一峰：炫耀从来不是我的动机，好奇才是（图灵访谈） 西瓜大丸子汤：最快的成长方式就是慢慢来 西瓜大丸子汤：一份优秀的求职信样本 Will Knight: China’s AI Awakening 程序员，如何从平庸走向理想？ 丧文化和中年危机背后的大国哲学 继续阅读本站其他精彩文章 机器学习 编程语言 技术碎碎念 读书笔记]]></content>
      <categories>
        <category>思想王国</category>
      </categories>
      <tags>
        <tag>总结</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[中文文本的词同现矩阵生成方法]]></title>
    <url>%2F2017%2Fthe-co-occrrence-matrix-generation-of-Chinese-text%2F</url>
    <content type="text"><![CDATA[本文将解释如何根据一段中文文本生成词同现矩阵？ 生成词同现矩阵步骤：1. 分词，将文章中所有的词置于列表中2. 统计词同现词组频率，统计3. 统计同现词组频率，并按频率的降序排列4. 获取所有同现词组列表，保证列表中的元素是唯一的5. 生成一个空矩阵，并使得矩阵的长宽为同现词组列表长度加一6. 构建一个关键词集合，用于作为同现矩阵的首行和首列7. 从同现词组中获得词组同现次数并填入同现矩阵123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102import jiebaimport numpy as npimport operatorfrom collections import defaultdictfile_path = ''def word_segment(file_path): """分词""" word_list = [] f = open(file_path, encoding='utf-8') for line in f.readlines(): seg_list = jieba.lcut(line, cut_all=False) filter_seg_list = [fil for fil in seg_list if len(fil) &gt;= 2] for item in filter_seg_list: word_list.append(item) return word_listdef build_co_network(seg_list): """统计词同现词组频率""" co_net = defaultdict(lambda: defaultdict(int)) for i in range(len(seg_list)-1): for j in range(i+1, i+2): w1 = seg_list[i] w2 = seg_list[j] if w1 != w2: co_net[w1][w2] += 1 return co_netdef get_co_terms(co_net): """统计同现词组频率，并按频率的降序排列""" com_max = [] for t1 in co_net: t1_max_terms = sorted(co_net[t1].items(), key=operator.itemgetter(1), reverse=True) for t2, t2_count in t1_max_terms: com_max.append(((t1, t2), t2_count)) terms_max = sorted(com_max, key=operator.itemgetter(1), reverse=True) return terms_maxdef get_all_words(terms_max): """获取所有词汇列表，列表每个元素都是唯一的""" word_list = [] for item in terms_max: for word in item[0]: word_list.append(word) return word_listdef build_matrix(word_list): """生成空矩阵，矩阵的高度和宽度为词汇集合的长度 +1""" edge = len(set(word_list)) + 1 matrix = [['' for j in range(edge)] for i in range(edge)] return matrixdef get_set_key(data): """将词汇列表集合作为同现矩阵的首行和首列""" all_key = '/'.join(data) key_list = all_key.split('/') set_key_list = list(filter(lambda x: x != '', key_list)) return list(set(set_key_list))def init_matrix(set_key_list, matrix): """初始化矩阵，将关键词集合赋值给第一列和第二列""" matrix[0][1:] = np.array(set_key_list) matrix = list(map(list, zip(*matrix))) matrix[0][1:] = np.array(set_key_list) return matrixdef count_matrix(matrix, test): """从 term_max 中获得词组同现次数并填入同现矩阵""" for row in range(1, len(matrix)): for col in range(1, len(matrix)): if matrix[0][row] == matrix[col][0]: matrix[col][row] = str(0) else: count = 0 for item in test: if matrix[0][row] in item[0] and matrix[col][0] in item[0]: count += int(item[1]) else: continue matrix[col][row] = str(count) return matrixif __name__ == '__main__': segment_word_list = word_segment(file_path) co_net = build_co_network(segment_word_list) terms_max = get_co_terms(co_net) word_list = get_all_words(terms_max) key_word_set = get_set_key(word_list) matrix = build_matrix(key_word_set) init_matrix = init_matrix(key_word_set, matrix) co_occurrence_matrix = count_matrix(init_matrix, terms_max) 推荐阅读 文献关键词同现矩阵python实现 stackoverflow: word-word co-occurrence matrix Co-occurrence Matrix from list of words in Python Constructing a co-occurrence matrix in python pandas Mining Twitter Data with Python (Part 4: Rugby and Term Co-occurrences) python简单实战项目：《冰与火之歌1-5》角色关系图谱构建——人物关系可视化 python构建关键词同现矩阵 Gephi 中文教程 Mining Twitter Data with Python (Part 4: Rugby and Term Co-occurrences) How to Use Words Co-Occurrence Statistics to Map Words to Vectors 继续阅读本站其他精彩文章 机器学习 编程语言 技术碎碎念 读书笔记]]></content>
      <categories>
        <category>coding</category>
      </categories>
      <tags>
        <tag>词同现</tag>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[小记在中经网的一次爬虫踩坑经历]]></title>
    <url>%2F2017%2Fa-government-report-crawl-experience%2F</url>
    <content type="text"><![CDATA[最近几天花了点时间写了一个爬虫，过程不太复杂，但是对我这样的爬虫小白来说还是花了点时间的。没有涉及到太多技术，收拾的拦路小妖怪倒是不少，下面记录一下具体的实现过程。 我的需求是抓取历年中国地方政府工作报告全文，简单 Google 一下发现没有现成的数据集，或许再花点时间也可以搜索到或者找别人可以要到，为了多写点代码，最终还是决定通过自己动手写爬虫抓取下来。经过搜索，中国经济网整理了从 2003 年到 2017 各省、直辖市、自治区的政府工作报告数据，目标网址是: http://district.ce.cn/zg/201702/26/t20170226_20529710.shtml，粗略统计约 700 多篇，不算很多，对数据感兴趣的读者可以访问链接下载使用：https://pan.baidu.com/s/1clUF2Q 密码：uxg1 在代码中用到的库有 urllib 、BeautifulSoup 、pandas 等，urllib 负责从 URL 地址中请求得到 HTML 标签数据，urllib2 是 python 自带的模块，不需要下载，urllib2 在 python3.x 中被改为 urllib.request。BeautifulSoup 负责解析 HTML 数据，剩下的工作就是编写几个小功能函数了，最后再将这些函数组装起来。 1. 实现思路说一下具体的实现思路，这里我们先将 http://district.ce.cn/zg/201702/26/t20170226_20529710.shtml 称为父网址（father_url）好了，这个网址列出了从 2003 年 到 2017 年地方政府报告合集的链接。首先从父网址解析得到历年报告合集页面的 URL 地址，如“2017 年汇编”的 URL 地址是 http://district.ce.cn/zg/201702/26/t20170226_20528713.shtml，我们称之为子网址（child_url_year）；然后，再从子网址中解析得到某个年份地方政府报告的 URL 地址（child_url_city）。当我们得到了所有报告的标题(report_title)和链接(report_url)，接下来的工作就很简单了，只需要从一个一个的链接中提取出正文即可。 2. 遇到的几个坑但是在实现过程中还是遇到了几个小问题需要解决： 2.1 一般报告不止一页，如何知道总页数？刚遇到这个问题，我准备打算做个简单粗暴的遍历去实现，一旦解析 404 后直接跳出，后来一想效率太低并且也不太好看。然后在网页源代码中找到这么一段，报告的页数统计是用 js 实现的，也就是动态的，所以 HTML 代码中解析不到页数统计的数据，而是隐藏在一段被注释 js 的代码中。比如，下面这段代码中，页面总数 4 就在 createPageHTML(4, 0, &quot;t20170207_20021665&quot;, &quot;shtml&quot;) 里。 123456789101112&lt;!--function createPageHTML(_nPageCount, _nCurrIndex, _sPageName, _sPageExt)&#123; if(_nPageCount == null || _nPageCount&lt;=1)&#123; return; &#125;//中间代码省略&#125;//函数结束符//WCM 置标createPageHTML(4, 0, &quot;t20170207_20021665&quot;, &quot;shtml&quot;); //--&gt; 在这里，我用了一个正则表达式把参数取了出来 123tmp_html = standard_html.find_all(&apos;script&apos;, language=&apos;JavaScript&apos;)tmp_str = re.search(r&apos;createPageHTML\((\d+)&apos;, str(tmp_html))page_sum = tmp_str.group(1) 2.2 爬取速度变慢编写爬虫中，考虑到数据量并不大，一开始并没有设置请求的代理头、也没有为 urllib 设置超时，所以在抓取数据的时候，爬虫程序经常超时或者 HTTP Error 502。幸好伪装浏览器解决了这个问题，否则得要按照前辈说的上代理池了。 123headers = &#123;&apos;User-Agent&apos;:&apos;Mozilla/5.0 (Windows NT 10.0; Win64; x64)AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.84 Safari/537.36&apos;&#125;req = urllib.request.Request(url=url, headers=headers)html = urllib.request.urlopen(req, timeout=60).read() 关于如何伪装浏览器见：https://zhidao.baidu.com/question/2117242032496816307.html 2.3 爬取速度过于频繁这里我采取了一种比较笨的办法，每访问一次网站就间隔几秒或者随机间隔几秒，经过这个设置后，网站终于没有限制抓取了。这应该不是长久之计，毕竟效率太低了。 2.4 两次面临 HTML 解析器选择之前写爬虫时用得最多的 HTML 解析器是 html.parser，后来在使用的过程中发现，html.parser 的容错率太低了，很多好端端网页的源代码抓下来一解析就变乱码了，比如这个网址：http://district.ce.cn/zg/201602/04/t20160204_8740940.shtml。开始，我认为是编码的问题，但是后来马上又否定了，中经网所有的网页都是采用 gb2312 的编码，为什么其他网址解析正常，唯独偏偏这个网址就出问题？ 苦苦思索无果后，我在一个爬虫交流群里提出了这个问题，热心的群友亲自在本地给我测试了一下这个网址，一切正常。为什么？比较代码之后才发现，是 HTML 解析器的缘故，把 html.parser 换成了 lxml，岁月静好。 第二次面临的选择则是在获取报告正文总页数上，解析又出现了错误，总页面数解析不到，再次把 lxml 换成 html5.lib，岁月静好。根据这轮踩坑，如果求稳的话，以后优先选择 html5.lib 作为第一解析器。 关于解析器的选择，见 BeautifulSoup 文档中的比较：安装解析器 2.5 URL 地址或标题非法导致文件写入不成功爬下来的数据往往不是你想象中的那么干净，数据永远是脏的，比如报告的链接地址和标题就有各种奇奇怪怪的情形，比如标题中混入了 \xa0、不是报告的 URL 或者有标点符号，又或者 URL 地址不合法导致无法访问。所以，这里采取的解决办法是在抓取的时候过滤掉这些地址 123if re.match(r'^https?:/&#123;2&#125;\w.+$', province_report_url) and re.match(r'[^?!(\d) | ^?!(（)]', report_title) and len(report_title) &gt; 6: 2.6 异常处理异常处理非常重要！！！ 在异常处理这块踩了好多坑，你一定不想看到程序跑了一个小时候突然因为一粒“老鼠屎”而挂了吧？记住，进行字典处理、文件 IO 的时候一定一定要考虑异常，火星人，地球是很危险的，一不小心会挂。 3. 实现代码 其他的后续再补充，下面是代码，欢迎指正！ 3.1 过滤年度报告的标题及地址123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133#!/usr/bin/python#-*- coding:utf-8 -*-"""@author: Li Bin@date: 2017-12-13"""import configparser as ConfigParserimport csvimport loggingimport reimport timeimport urllib.requestfrom bs4 import BeautifulSoupconfig = ConfigParser.ConfigParser()config.read('config.ini')local_reports_collect_url = config['url']['collect_url']file_path = config['file_path']['url_data_file']def get_standard_html(url): """获取网页的标准 HTML 文本 :return: """ headers = &#123;'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) ' 'AppleWebKit/537.36 (KHTML, like Gecko) ' 'Chrome/63.0.3239.84 Safari/537.36'&#125; req = urllib.request.Request(url=url, headers=headers) try: html = urllib.request.urlopen(req, timeout=60).read() standard_html = BeautifulSoup(html, 'html5lib') except Exception as e: # 如果抓取失败则退出，并令 standard_html 为 None logging.info(url + "抓取超时......") standard_html = None pass return standard_htmldef filter_report_collect_url(standard_html): """从 standard_html 中过滤得到历年地方政府报告合集页面的 URL :return: url_dict， [&#123;'year': "2017年汇编", 'url': 'http://district.ce.cn/zg/201702/26/t20170226_20528713.shtml'&#125;] """ url_dict = [] raw_html_text = standard_html.find_all('div', class_='TRS_Editor') html_text = raw_html_text[0].find_all('a') for text in html_text: temp_dict = &#123; 'year': text.get_text(), 'url': text.get('href') &#125; url_dict.append(temp_dict) return url_dictdef filter_local_gov_report_url(url): """地方政府报告合集页面的省市报告的 URL :param url: :return: [&#123;'report_title': '北京市政府工作报告 (2017年1月14日 蔡奇)', 'report_url': 'http://district.ce.cn/newarea/roll/201702/07/t20170207_20015641.shtml'&#125;] """ local_gov_report_url_dict = [] standard_html = get_standard_html(url) raw_html_text = standard_html.find_all('div', class_='TRS_Editor') """个别网页的链接在 div 标签的 content 类中""" if len(raw_html_text) &gt; 0: html_text = raw_html_text[0].find_all('a') for text in html_text: temp_dict = &#123; 'report_title': (text.get_text()).replace('\xa0', '').replace('资料:', ''), # 1. 替换 \xa0，防止写入 csv 错误 'report_url': text.get('href') # 2. 替换“资料:”，文件名中有冒号无法写入 &#125; local_gov_report_url_dict.append(temp_dict) else: raw_html_text = standard_html.find_all('div', class_='content') html_text = raw_html_text[0].find_all('a') for text in html_text: temp_dict = &#123; 'report_title': (text.get_text()).replace('\xa0', '').replace('资料:', ''), 'report_url': text.get('href') &#125; local_gov_report_url_dict.append(temp_dict) return local_gov_report_url_dictdef get_annual_report_url_list(url_dict): """ 获取所有的省市报告地址 :param url_dict: :return: """ annual_report_url_list = [] # 年度省（直辖市）报告 URL 列表：2003-2017 年所有省市报告的地址 for item in url_dict: report_url = item.get('url') # report_url 是某年报告汇编合集的地址 local_gov_report_url = filter_local_gov_report_url(report_url) annual_report_url_list.append(local_gov_report_url) print(report_url) time.sleep(2) # 为了防止 ip 被封，每抓完一年延迟两秒 return annual_report_url_listdef write_data_to_file(url_list): with open(file_path, 'w', newline='') as f: fieldnames = ['report_title', 'report_url'] writer = csv.DictWriter(f, fieldnames=fieldnames) writer.writeheader() for item in url_list: for sub_item in item: report_title = sub_item.get('report_title') province_report_url = sub_item.get('report_url') """ 1. 判断 URL 地址是否合法 2. 判断标题是否数字或者以中文括号‘（’开头，如果是，则不是报告地址 3. 标题长度至少大于 6：“政府工作报告” """ if re.match(r'^https?:/&#123;2&#125;\w.+$', province_report_url) and \ re.match(r'[^?!(\d) | ^?!(（)]', report_title) and \ len(report_title) &gt; 6: writer.writerow(sub_item) f.close()if __name__ == '__main__': standard_html = get_standard_html(local_reports_collect_url) url_dict = filter_report_collect_url(standard_html) annual_report_url_list = get_annual_report_url_list(url_dict) write_data_to_file(annual_report_url_list) 3.2 遍历所有的地址并抓取全文123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138#!/usr/bin/python#-*- coding:utf-8 -*-"""@author: Li Bin@date: 2017-12-17"""import configparser as ConfigParserimport loggingimport pandas as pdimport randomimport reimport timeimport urllib.requestfrom bs4 import BeautifulSoupconfig = ConfigParser.ConfigParser()config.read('config.ini')local_reports_collect_url = config['url']['collect_url']file_path = config['file_path']['url_data_file']root_file_path = config['file_path']['root_directory']def read_url(annual_report_url_list_file_path): """ 读取 CSV 文件 :return: DataFrame """ df = pd.read_csv(annual_report_url_list_file_path, encoding='gbk') return dfdef get_standard_html(url): """获取汇总合集页面的标准 HTML 文本 :return: """ headers = &#123;'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) ' 'AppleWebKit/537.36 (KHTML, like Gecko) ' 'Chrome/63.0.3239.84 Safari/537.36'&#125; req = urllib.request.Request(url=url, headers=headers) try: html = urllib.request.urlopen(req, timeout=60).read() standard_html = BeautifulSoup(html, 'html5lib') except Exception as e: logging.info(url + "抓取超时......") standard_html = None pass return standard_htmldef get_report_text(url): """从 URL 地址获取网页正文部分 :param local_report_url: :return: text：list，网页正文 """ standard_html = get_standard_html(url) if standard_html: # 如果从地址中解析得到了网页源代码则获取正文，否则返回 None raw_html_text = standard_html.find_all('p') text = [] for item in raw_html_text: text.append(item.get_text()) return text else: return Nonedef get_page_sum(url): """ 获取报告总页数：如“共（n）页”的 n 参数 """ standard_html = get_standard_html(url) if standard_html: try: tmp_html = standard_html.find_all('script', language='JavaScript') tmp_str = re.search(r'createPageHTML\((\d+)', str(tmp_html)) page_sum = tmp_str.group(1) except: page_sum = 1 return int(page_sum) # 如果 standard_html 为 None，则返回 1，至少抓一页def generate_page_urls(url, page_sum): """ 当报告有多个页面时，按照后缀递增的方式生成每个页面的 URL """ i = 1 url_list = [url] try: while i &lt; page_sum: page_url = ''.join([url[:-6], '_', str(i), '.shtml']) url_list.append(page_url) i += 1 except Exception: # 上面的 while 判断偶尔出现一次错误，暂未找到原因，先忽略 url_list = [url] return url_listdef crawl_province_report(report_title, province_report_url): """抓取报告全文并写入 txt """ global file_path page_sum = get_page_sum(province_report_url) url_list = generate_page_urls(province_report_url, page_sum) text = [] for page_url in url_list: tmp_text = get_report_text(page_url) text.append(tmp_text) file_path = root_file_path + str(report_title) + '.txt' print('正在抓取' + str(report_title) + '......') time.sleep(random.randint(0, 5)) try: flatten_text = [item for sublist in text for item in sublist] report_text_handler = open(file_path, "ab+") for item in flatten_text: report_text_handler.write((item + '\r\n').encode('UTF-8')) print('正在写入文件......') except Exception: passdef get_all_report(): """ 从省市报告地址中提取出网页正文，并写入 TXT 文件 """ url_list_df = read_url(file_path) for item in url_list_df.iterrows(): report_title = item[1]['report_title'] province_report_url = item[1]['report_url'] crawl_province_report(report_title, province_report_url) time.sleep(3) # 间隔 3 秒抓取一次，防止抓取过于频繁if __name__ == '__main__': get_all_report() 后记本次爬虫的数据量很少，总共也就不到 700 篇，真正恼人的是时不时蹦出的小错误。当然，这个爬虫的代码很烂，根本没有考虑效率因素，大规模的爬虫实践肯定不是这种操作，后面有时间再实践实践多线程、多进程、ip 代理池的技术。 推荐阅读 http://wiki.jikexueyuan.com/project/python-crawler-guide/advanced-usage-of-urllib-library.html python 高度健壮性爬虫的异常和超时问题 http://www.cnblogs.com/ly5201314/archive/2008/09/04/1284139.html http://caibaojian.com/zhongwen-regexp.html 继续阅读本站其他精彩文章 机器学习 编程语言 技术碎碎念 读书笔记]]></content>
      <categories>
        <category>coding</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[备忘录（一）]]></title>
    <url>%2F2017%2Fnotes-in-a-mess%2F</url>
    <content type="text"><![CDATA[备忘 连接数据库连接本地数据库的时候一定要记得把 VPN 代理关闭，否则连接失败。 http://www.postgresqltutorial.com/postgresql-python/connect/ PostgreSQL SELECT UUIDhttp://www.postgresqltutorial.com/postgresql-uuid/ limithttp://www.postgresqltutorial.com/postgresql-limit/ upsert1）UPSERT UPSERT 是 INSERT, ON CONFLICT UPDATE 的简写，简而言之就是：插入数据，正常时写入，主键冲突时更新。以下给个简单的例子： — 创建测试表，并插入一条数据 12CREATE TABLE customer (cust_id INTEGER PRIMARY KEY, name TEXT);INSERT INTO customer VALUES (100, ’Big customer’); — 常规INSERT语句，主键冲突，报错 1234INSERT INTO customer VALUES (100, ’Non-paying customer’);ERROR: duplicate key value violates unique constraint&quot;customer_pkey&quot;DETAIL: Key (cust_id)=(100) already exists. — 新特性，主键冲突时，自动更新数据 12INSERT INTO customer VALUES (100, ’Non-paying customer’)ON CONFLICT (cust_id) DO UPDATE SET name = EXCLUDED.name; SELECT * FROM customer;cust_id | name————-+——————————-100 | Non-paying customer Cannot INSERT: ERROR: array value must start with “{” or dimension informationhttps://stackoverflow.com/a/31519156/6920589 获取百度首页的文档树 1234567891011121314&gt;&gt;&gt; html_doc = urllib.request.urlopen(&quot;https://www.baidu.com&quot;).read()&gt;&gt;&gt; standard_html = BeautifulSoup(html_doc, &apos;html.parser&apos;)&gt;&gt;&gt; standard_html&lt;html&gt;&lt;head&gt;&lt;script&gt; location.replace(location.href.replace(&quot;https://&quot;,&quot;http://&quot;)); &lt;/script&gt;&lt;/head&gt;&lt;body&gt;&lt;noscript&gt;&lt;meta content=&quot;0;url=http://www.baidu.com/&quot; http-equiv=&quot;refresh&quot;/&gt;&lt;/noscript&gt;&lt;/body&gt;&lt;/html&gt;&gt;&gt;&gt; 1. find_all() 找到所有标签例如，从文档中找到所有 &lt;a&gt; 标签的链接: 12345for link in soup.find_all(&apos;a&apos;): print(link.get(&apos;href&apos;)) # http://example.com/elsie # http://example.com/lacie # http://example.com/tillie 2. get_text() 获取标签与标签之间的内容find_all() 方法用于定位 HTML 文本中的标签，get_text() 方法用于获取标签与标签之间的内容。注意，通常用 find_all() 得到的是一个 list，获取标签之间的内容还要通过索引一下。 1234567&gt;&gt;&gt; a = standard_html.find_all(&apos;script&apos;)[&lt;script&gt; location.replace(location.href.replace(&quot;https://&quot;,&quot;http://&quot;)); &lt;/script&gt;] &gt;&gt;&gt; a[0].get_text()&apos;\r\n\t\tlocation.replace(location.href.replace(&quot;https://&quot;,&quot;http://&quot;));\r\n\t&apos; 3. decompose该方法将当前节点移除文档树。 12345678910&gt;&gt;&gt; standard_html.script.decompose()&gt;&gt;&gt; standard_html&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;&lt;noscript&gt;&lt;meta content=&quot;0;url=http://www.baidu.com/&quot; http-equiv=&quot;refresh&quot;/&gt;&lt;/noscript&gt;&lt;/body&gt;&lt;/html&gt; 参考链接 BeautifulSoup4 中文文档 BeautifulSoup4 decompose() BeautifulSoup4 get_text() BeautifulSoup4 find_all() source在当前Shell环境中从指定文件读取和执行命令，命令返回退出状态。 读取和执行/root/.bash_profile文件。 1[root@localhost ~]# source ~/.bash_profile unzip解压缩文件 http://wangchujiang.com/linux-command/c/unzip.html unzip test.zip 将压缩文件text.zip在当前目录下解压缩。 将压缩文件text.zip在指定目录/tmp下解压缩，如果已有相同的文件存在，要求unzip命令不覆盖原先的文件，unzip -n test.zip -d /tmp。 查看压缩文件目录，但不解压，unzip -v test.zip。 将压缩文件 test.zip 在指定目录 /tmp 下解压缩，如果已有相同的文件存在，要求 unzip 命令覆盖原先的文件，unzip -o test.zip -d tmp/。 Another git process seems to be running in this repositoryhttps://stackoverflow.com/a/40453027/6920589 还有一个原因，就是同时向 Git 添加了太多的文件，一个一个添加就不会出现这个问题。 列表合并https://zhidao.baidu.com/question/328728387.html 人工智能标记语言 AIML 知识单元（knowledge unit），包含输入问题、输出问题和可选正文（optional context）。 根据用户的问题而回复的模板，问题 给出的回答 推荐阅读https://www.pandorabots.com/pandora/pics/wallaceaimltutorial.html https://www.v2ex.com/t/41072 继续阅读本站其他精彩文章 机器学习 编程语言 技术碎碎念 读书笔记]]></content>
      <categories>
        <category>其他</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>SQL</tag>
        <tag>BeautifulSoup</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python | 再谈虚拟环境]]></title>
    <url>%2F2017%2Fdiscussion-virtualenv-again%2F</url>
    <content type="text"><![CDATA[在之前的一篇文章PyCharm IDE 使用笔记（1）：新建虚拟环境和更换源有讲过如何通过 virtualenv 库来建立和管理虚拟环境，但是后来配合使用 IDE 后发现了不少弊端，就是每次在 IDE 下要运行别的程序的时候，必须去项目文件夹下重新配置虚拟环境路径，久而久之，就慢慢忘记到底新建了多少虚拟环境。并且每次开启虚拟环境之前要去虚拟环境所在目录下的 bin 目录下 source 一下 activate，这就需要我们记住每个虚拟环境所在的目录，十分不利于管理。今天看到了 virtualenvwrapper 这个虚拟环境管理神器，感觉比较好用，记录一下基本的使用方法，供大家参考。 1. virtualenvwrapper1.1 安装使用 pip 安装 virtualenvwrapper 1pip install virtualenvwrapper 1.2 如何使用？首先，需要对 virtualenvwrapper 进行配置。 它需要指定一个环境变量，叫做 WORKON_HOME，并且需要运行一下它的初始化脚本 virtualenvwrapper.sh，这个脚本在 /usr/local/bin/ 目录下（如果你的 Python 解释器来自 anaconda，那么初始化的脚本则在 /Users/yourname/anaconda3/bin/virtualenvwrapper.sh 下）。 如果找不到virtualenvwrapper.sh 文件，使用sudo find / -name &quot;virtualenvwrapper.sh&quot;搜寻并找到路径。 WORKON_HOME 就是它将要用来存放各种虚拟环境的目录，这里我们可以设置为 ~/.virtualenvs。 这两步的操作如下： 12export WORKON_HOME=&apos;~/.virtualenvs&apos;source /usr/local/bin/virtualenvwrapper.sh 我们可以把这两条命令写入到终端的配置文件中，这样每次重启就不用重复执行以上的初始化操作了。在 bash 下，将以上两条命令添加到 ~/.bashrc_profile 中即可。 1.3 创建一个虚拟环境使用 mkvirtualenv 命令新建，bash 下自动补全。 1mkvirtualenv env_name 然后在 ~/.virtualenvs 目录下就多了一个名为 env_name 的虚拟环境，新建虚拟环境之后会被自动激活。 在 bash 下其他任何时候任何目录下，如果想进入某个建立好的虚拟环境，使用命令 1workon env_name 1.4 离开虚拟环境1deactivate 1.5 删除虚拟环境删除虚拟环境也很简单， 1rmvirtualenv my_project 1.6 其他有用的命令 lsvirtualenv，列举所有的虚拟环境。 cdvirtualenv，导航到当前激活的虚拟环境的目录中，比如说这样你就能够浏览它的 site-packages 。 cdsitepackages 和上面的类似，但是是直接进入到 site-packages 目录中。 lssitepackages，显示 site-packages 目录中的内容。 推荐阅读 http://pythonguidecn.readthedocs.io/zh/latest/dev/virtualenvs.html 聊聊 virtualenv 和 virtualenvwrapper 实践 继续阅读本站其他精彩文章 机器学习 编程语言 技术碎碎念 读书笔记]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>虚拟环境</tag>
        <tag>virtualenvwrapper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[语音合成的几种方案实现与分析]]></title>
    <url>%2F2017%2Fseveral-solutions-of-tts-in-python%2F</url>
    <content type="text"><![CDATA[1. 语音合成平台介绍 语音合成是将人类语音用人工的方式所产生。若是将电脑系统用在语音合成上，则称为语音合成器，而语音合成器可以用软/硬件所实现。文字转语音（text-to-speech，TTS）系统则是将一般语言的文字转换为语音，其他的系统可以描绘语言符号的表示方式，就像音标转换至语音一样。 - Wikipedia 最近工作中需要基于 Python 开发实现一个实时语音播报系统，故借此机会了解了一下目前语音合成开放 API 的情况。 百度语音合成 REST API首先采用的是百度的语音合成服务，Google 搜索“语音合成”，第一个结果是维基百科，第二个就是百度的语音合成服务（很奇怪为什么像科大讯飞、云知声这样的公司排名很靠后）。它基于 HTTP 请求的 REST API 接口，拿到 token，只需要将需要合成的文本上传，合成服务便会返回一段可以播放的 MP3 文件，详细的请求方式及参数说明可以见百度语音合成开发文档。百度语音合成提供了四种发音人选择，不过，个人觉得，合成效果一般，听起来声音比较生硬，被人吐槽，故放弃。 注意：需要合成的文本，使用 utf-8 编码 123456789101112131415161718192021222324def get_voice(query_msg): '''从百度语音合成 API 生成语音文件 注意 tex 参数需要按照 url 参数标准使用 urlencode utf-8 编码 ''' token = config['authority']['baidu_token'] cuid = config['authority']['cuid'] audio_file_path = config['file_path']['audio_path'] url = 'http://tsn.baidu.com/text2audio?' # 请求地址 data = &#123; 'lan': 'zh', 'cuid': cuid, 'tok': token, 'tex': query_msg, 'vol': 10, # 音量 'ctp': 1, 'per': 3, # 发音人选择:0为普通女声，1为普通男生，3为情感合成-度逍遥，4为情感合成-度丫丫，默认为普通女声 'spd': 2 &#125; params = urllib.parse.urlencode(data, encoding='utf-8') voice_data = urllib.request.urlopen(url + params).read() voice_file = open(audio_file_path, 'wb+') voice_file.write(voice_data) voice_file.close() 阿里云语音合成 REST API放弃了百度的服务后，转向阿里云提供的语音合成Restful API，它的合成方式与百度的一样，用户通过 API 上传需要合成的中文文本，云端合成成功之后，返回合成结果语音。官方文档提供了一个 Java 的例子，还好，有热心人已经将合成服务封装好了一个 Python 库：aliyun-voice 1.0.2 ，遗憾的是，目前这个库只能在 Python2 下安装运行，不过这也影响不大，新建一个 python2 的虚拟环境就好了。 通过 aliyun-voice 库认证阿里云很简单，先开通智能语音服务https://data.aliyun.com/product/nls，然后从 https://ak-console.aliyun.com/ 页面获取的 Access id 和 key。同样需要注意一点的是，用户必须上传 utf-8 编码的合成文本，否则会出现麻烦的编码错误，所以传入的字符串还需要经过 encode 一层操作。 阿里云认证 12from aliyun_voice.voice import Voiceauth = Voice(ALIYUNACCESSID, ALIYUNACCESSKEY) 获取语音数据，并存储到指定目录 dist 1auth.save_voice(text, dist) 奏是这么简单！下面看一个具体实现的代码例子 1234567891011121314151617181920212223242526def play_query_msg(): &apos;&apos;&apos;播放查询的问题文本 Args: receive_data: 用户查询后返回的数据，数据类型为列表， 用户查询问题在 content 中的 msg 字段 query_msg: 用户查询的问题 ALIYUNACCESSID: 阿里云 API 密钥 id（请使用https://ak-console.aliyun.com/ 页面获取的Access 信息） ALIYUNACCESSKEY: 阿里云 API 密钥 key &apos;&apos;&apos; ws = create_connection(&quot;&quot;) while True: time.sleep(0.5) try: receive_items = json.loads(ws.recv()) for item in receive_items: if item[&apos;action_type&apos;] == &apos;query&apos;: query_msg = json.loads(item[&apos;content&apos;])[&apos;msg&apos;] ALIYUNACCESSID = config.get(&apos;authority&apos;, &apos;ALIYUNACCESSID&apos;) ALIYUNACCESSKEY = config.get(&apos;authority&apos;, &apos;ALIYUNACCESSKEY&apos;) voice_file_path = config.get(&apos;file_path&apos;, &apos;audio_path&apos;) auth = Voice(ALIYUNACCESSID, ALIYUNACCESSKEY) auth.save_voice(query_msg.encode(&apos;utf-8&apos;), voice_file_path) # 必须是 UTF-8 编码的合成文本 except WebSocketConnectionClosedException: ws = create_websocket_connection() continue 腾讯语音合成腾讯语音合成服务需要审核，而且审核速度有点慢，5 天过去还没有申请成功，不知道合成效果咋样。 其他其他的语音合成提供商还有讯飞在线语音合成开放平台、云知声在线语音合成开放平台，科大讯飞提供了许多种发音人选择，而且效果很不错，不过这两个服务目前都没有提供可以供直接调用的 REST API 接口，云知声提供了一个 C 语言的接口，没有找到用 Python 实现的案例。 2. Python 播放 MP3 文件获得了语音文件后，如何在 Python 下播放又是一个问题，在实现播放语音文件的过程中踩了不少坑。在 Python 下实现的方法有很多种，有繁有简，可用的库有 pygame、pyglet 等等 ，不过这几个库我都一一尝试了，得到了两种真正能用或者比较稳定的实现方案。 pygamepygame 是 Python 下一个开发游戏的库，用它来播放 MP3 文件很简单，三行代码就可以实现，算是最简单的的一个了（其他的库，要么调用很麻烦，远远不止三行，要么各种平台不兼容），如果没有太多语音播放需求如重复播放、延迟播放，可以采用此法。 1234567def play_voice(voice_file_path): &apos;&apos;&apos;用 pygame 库播放语音文件 &apos;&apos;&apos; pygame.mixer.init() track = pygame.mixer.music.load(voice_file_path) pygame.mixer.music.play() 不过在使用的过程中我发现了一个 bug，如果音频文件的采样率不高（我的是 16000）的话，播放语音时会出现失真的情况，原本的正常的男声会变得语速很快、音调很高。为了解决这个问题，我花了几个小时反复查阅了 pygame 的文档，尝试使用其他方法，然而发现并没有控制播放速率或者其他控制声音质量的选项。 系统自带播放器pygame 出现了声音失真怎么办？又不想调用其他库的时候需要十几行代码才能实现？其实还有解救的办法。经过少校指点，在 Mac 和 Linux 下是可以通过 subprocess（Python 下自带 subprocess）直接调用系统播放器的，经过测试，声音没有出现失真现象。 12345def play_voice_by(voice_file_path, query_msg): &apos;&apos;&apos;- 调用 mac 系统播放器 afplay 播放 MP3 文件 - linux 下安装 sudo apt install mplayer，调用方法为：subprocess([&apos;mplayer&apos;, voice_file_path]) &apos;&apos;&apos; subprocess.call([&apos;afplay&apos;, voice_file_path]) 推荐阅读 https://stackoverflow.com/questions/3498313/how-to-trigger-from-python-playing-of-a-wav-or-mp3-audio-file-on-a-mac https://stackoverflow.com/questions/307305/play-a-sound-with-python https://www.whatled.com/post-1007.html 百度语音合成 REST API 文档 阿里云语音合成文档 讯飞在线语音合成开放平台 云知声语音合成开放平台 继续阅读本站其他精彩文章 机器学习 编程语言 技术碎碎念 读书笔记]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>语音合成</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python | 项目开发中的几个习惯]]></title>
    <url>%2F2017%2Fseveral-good-habits-in-project-development%2F</url>
    <content type="text"><![CDATA[良好的项目开发习惯不仅造福自己也造福他人。 1. requirement.txt项目协作开发中，因为不同工作环境的库配置都有差别，所以 要将自己的代码在另外一个工作环境中成功复现是一件极其需要运气的事情。如果在项目中附带了运行所需的环境要求文件，并且还可以根据这个要求文件快速搭建运行环境那就再好不过了。在 Python 中，requirement.txt 用于记录所有依赖包及其精确的版本号，其作用是用来在另一台 PC 上重新构建项目所需要的运行环境依赖。 requirements.txt 可以通过 pip 命令自动生成和安装 生成 requirements.txt 文件 1pip freeze &gt; requirements.txt 安装 requirements.txt 依赖 1pip install -r requirements.txt 自动生成的 requirements.txt 文件是长这样的 123456gevent==1.2.2greenlet==0.4.12pygame==1.9.3six==1.11.0websocket==0.2.2websocket-client==0.44.0 2. 注释与代码规范 代码的头部加入作者、创建日期、最后修改日期、代码用途等必要信息 每个函数必须简要描述功能，说明变量类型及其作用，说明返回数据的数据类型和用途 变量命名能有多详细就有多详细，代码注释能有多详细就多详细，造福自己也造福他人 函数与函数之间空两行 库的导入按照库名的首字母顺序，先 import 后 from xx import xx 3. Git 提交规范 commit 必须说明提交类型，比如添加 [feature] 表示添加新功能、[bugfix] 表示修补 bug、[style] 表示格式改动 4. 为每个项目创建虚拟环境最好在项目开发时，就在项目的根目录下创建虚拟环境，所有的代码测试、库的安装都必须在虚拟环境被激活的条件下运行。关于如何创建虚拟开发环境，见先前的文章： PyCharm IDE 使用笔记（1）：新建虚拟环境和更换源。 5. 配置文件写好配置文件很关键，好的配置文件能够做到仅仅修改几个参数就能快速在另外一台机器复现，关于如何写配置文件，见先前的文章：Git | 项目开发中的权限管理。 （未完，持续更新） 推荐阅读 Google Python 风格指南 PyCharm IDE 使用笔记（1）：新建虚拟环境和更换源 Git | 项目开发中的权限管理 继续阅读本站其他精彩文章 机器学习相关 编程语言相关 技术碎碎念 读书笔记]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>代码规范</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Git | 项目开发中的权限管理]]></title>
    <url>%2F2017%2Fauthorities-management-in-project-development%2F</url>
    <content type="text"><![CDATA[为什么需要权限管理？项目开发中，访问各种外部资源时，经常会遇到各种权限认证的情形，比如发起 HTTP 请求时，通常需要提供 token 、私人 id 或其他密钥。通常来说，最好把权限的使用控制在一定的范围之内，将开发环境和生产环境的权限使用隔离开来。在协同开发的场景下，通常你可以在项目下建立两个不同的文件：config.ini 和 config.ini.example，前者是你在本地运行代码需要的验证信息，后者是生产环境验证信息模板。但是，这样做有一个问题，当开发者 git push 的时候，岂不是两个验证文件大家都可以看到了？其实，要解决这个问题很简单，只需要在项目下新建一个 .gitignore 文件。.gitignore 文件的作用是告诉 Git 在推送本地修改的时候忽略哪些文件，比如该例中可以添加一行 *.ini，即项目中所有以 .ini 后缀结束的文件在推送中都会被忽略。这样，项目中的其他成员就看不到 config.ini 配置文件了，而他们可以按照你提供的 config.ini.example 模板在他们自己的开发环境中建立一个 config.ini 文件，并且也按照你的方法建立 .gitignore 文件，所以你也看不到他的配置信息。 configparser 在权限管理中的应用下面简单介绍在 Python 中如何用 configparser 来完成项目中的参数配置需求。 在 Python 中，一般可以通过 configparser 库来灵活配置参数。Python2 下该模块名为 ConfigParser，到 Python3 才改为 configparser。ini 文件模板， 123456789101112[DEFAULT]ServerAliveInterval = 45Compression = yesCompressionLevel = 9ForwardX11 = yes[bitbucket.org]User = hg[topsecret.server.com]Port = 50022ForwardX11 = no 数据读取configparser 的使用方法很简单，配置文件信息读取进来之后，默认 DEFAULT 的节不显示。你可以把每个 section 或变量的名字视为字典中一个个的 key，这样通过调用 key 就能取到对应的 value 值。有两种方式，一种是 section[&#39;key&#39;]，另外一种是 section.get(&#39;key&#39;)，两种方式的效果一样。 123456789101112131415161718192021222324252627282930&gt;&gt;&gt; import configparser&gt;&gt;&gt; config = configparser.ConfigParser()&gt;&gt;&gt; config.sections()[]&gt;&gt;&gt; config.read(&apos;example.ini&apos;)[&apos;example.ini&apos;]&gt;&gt;&gt; config.sections()[&apos;bitbucket.org&apos;, &apos;topsecret.server.com&apos;]&gt;&gt;&gt; &apos;bitbucket.org&apos; in configTrue&gt;&gt;&gt; &apos;bytebong.com&apos; in configFalse&gt;&gt;&gt; config[&apos;bitbucket.org&apos;][&apos;User&apos;]&apos;hg&apos;&gt;&gt;&gt; config[&apos;DEFAULT&apos;][&apos;Compression&apos;]&apos;yes&apos;&gt;&gt;&gt; topsecret = config[&apos;topsecret.server.com&apos;]&gt;&gt;&gt; topsecret[&apos;ForwardX11&apos;]&apos;no&apos;&gt;&gt;&gt; topsecret[&apos;Port&apos;]&apos;50022&apos;&gt;&gt;&gt; for key in config[&apos;bitbucket.org&apos;]: print(key)...usercompressionlevelserveraliveintervalcompressionforwardx11&gt;&gt;&gt; config[&apos;bitbucket.org&apos;][&apos;ForwardX11&apos;]&apos;yes&apos; 有一点需要注意的地方，configparser 并不会去猜测配置文件中的数据类型，因为它会把所有的数据都视为字符串型，这就意味着如果你需要其他类型的数据，需要额外的转换数据操作。比如，对整数型和浮点型的需求 1234&gt;&gt;&gt; int(topsecret[&apos;Port&apos;])50022&gt;&gt;&gt; float(topsecret[&apos;CompressionLevel&apos;])9.0 也可以这样来实现 getint() getfloat() 1234&gt;&gt;&gt; config.getint(&apos;topsecret&apos;, &apos;Port&apos;)50022&gt;&gt;&gt; config.getfloat(&apos;topsecret&apos;, &apos;CompressionLevel&apos;)9.0 比如，对逻辑判断型 boolean 数据的需求 getboolean() 123456&gt;&gt;&gt; topsecret.getboolean(&apos;ForwardX11&apos;)False&gt;&gt;&gt; config[&apos;bitbucket.org&apos;].getboolean(&apos;ForwardX11&apos;)True&gt;&gt;&gt; config.getboolean(&apos;bitbucket.org&apos;, &apos;Compression&apos;)True 数据写入数据写入的方式也很简洁，比如用 add_section(&#39;new section&#39;) 方法给配置文件添加与 [bitbucket.org] 同等级别的节 new section。用 set() 方法给新添加的节设置新的配置信息，比如 123config.set(&apos;new section&apos;, &apos;an_int&apos;, &apos;99&apos;) # 整型config.set(&apos;new section&apos;, &apos;a_boolean&apos;, &apos;true&apos;) # 布尔型config.set(&apos;new section&apos;, &apos;a_float&apos;, &apos;2.17&apos;) # 浮点型 本文主要介绍了在项目开发中权限处理的背景以及实现方式，简单介绍了 configparser 的使用方法，深入了解可以继续参考官方文档。 推荐阅读 Git忽略文件.gitignore的使用 14.2. configparser — Configuration file parser]]></content>
      <categories>
        <category>教程</category>
      </categories>
      <tags>
        <tag>Git</tag>
        <tag>权限管理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Webhook + Flask 实现 Code Review 自动化]]></title>
    <url>%2F2017%2Fwebhook-flask-automate-code-review%2F</url>
    <content type="text"><![CDATA[这是一篇流水账技术笔记，大概也许只有我自己能看懂 什么是 Webhook？官方的语言解释 Webhook（来自 coding.net）： Webhook 允许第三方应用监听 Coding.net 上的特定事件，在这些事件发生时通过 HTTP POST 方式通知( 超时5秒) 到第三方应用指定的 Web URL。例如项目有新的内容 Push，或是 Merge Request 有更新等。 Coding.net 用户可以在自己的项目 → 设置 → Webhook 中创建、设置 Webhook 所需监听的事件，并配置第三方应用的 Web URL 。WebHook 可方便用户实现自动部署，自动测试，自动打包，监控项目变化等 通俗一点，可以将 Webhook 理解为一个勤勤恳恳的“卧底”，它被你安插到对手的队伍中，一旦对方有任何动静，它会第一时间把“情报”发送给你。所以，一旦“卧底”成功打入（配置应用 URL），你就可以远程监视对手的一举一动（HTTP POST 或 PUT 方法）了。 为什么要 Code Review 任务分配自动化？任务的大致描述是，在团队项目中，除了主分支之外成员还会建立多个分支，当分支要与主分支合并时，通常需要审核分支的代码才能确认是否合并，也就是 code review。 那为什么要需要 code review？一，merge 发起者需要手动在网页上操作才能完成“分配”的动作，并且需要额外思考需要将任务分配给谁，次数多了就比较麻烦；二，有助于团队形成统一的代码规范。所以， 不如让其自动化随机分配，大家谁也怨谁，最后还可以提升自己代码审查的能力。这个想法可以通过 webhook + Flask 实现，思路如下图： 在思路的实现中，需要关注三个参数的获取：project_id assignee_id merge_request_iid 的获取。 Code Review 任务自动化分配的实现平台将以 GitLab 为例，并且需要大量调用 GitLab 的 API 以获取必要的参数。目前参数的调用有两种方法，一种是通过 Python 封装好的 python-gitlab 库进行调用，一种是通过请求 gitlab 的 GitLab API。这两种方法功能上可以互补，相互结合使用，因为毕竟通过封装好的库，使用更加方便。 1. python-gitlab 库在这一小节主要介绍在实现自动化过程中需要用到的一些参数获取方法。 初始化一个 project 变量 12345678import gitlabproject_url = &apos;http://X.X.X.X&apos;# 在 gitlab 的 profile-&gt;account 中可以找到，唯一token = &apos;yourtoken&apos;# 初始化 projectgl = gitlab.Gitlab(project_url, token)gl.auth() 1.1 获取你所有参与的项目 id调用该方法会列出你目前所有参与项目的 id，有了这个 id 才能获得某个项目下的详细信息，包括成员信息、项目描述、merge request 提交情况。 12projects = gl.projects.list()print(projects) 1.2 由 project_id 获取某项目下成员的信息1234567project = gl.projects.get(project_id)# 获取该项目下所有成员信息me = project.members.list()# 获取项目下所有成员 id 列表member = get_member_id(project)# 打印第一个成员的姓名print(me[0].name) 1.3 由 project_id 获取某项目下 merge request 提交信息12345678910project = gl.projects.get(project_id)mrs = project.mergerequests.list()# 打印最近一次 merge request 信息print(mrs[0])# 获取发起 merge request 者 IDprint(mrs[0].id)# 获取 merge request iid# 这个 id 在更新 merge request 的时候会用到# 这个参数很关键print(mrs[0].iid) 1.4 Update MR当用户发起 merge request，发起者会将 compare and continue 的任务分配给一个成员。现在的任务是，通过调用 GitLab 的 API 来更新改动后的 merge request。在 Python-gitlab 库里边，更新 merge request 步骤如下： 1234567# Get a single MRmr = project.mergerequests.get(mr_id)# Update a MR# 上一个变量 mr 有许多 key，可以通过修改 key 下# 的 value 实现修改 merge request 信息mr.description = &apos;New description&apos;mr.save() 不过貌似这里有一个 bug，保存之后 merge request 的信息 并没有即时更新，所以通过这个方法更新 merge request 的想法就终止了。 到此为止，我们已经在 1.3 中得到了 merge request iid、 assignee_id（有了 merge person id 和 member_list 写个函数可以得到，注意把 merge_person_id 排除）。还有最后一个 project_id 的获取将要在后面的 flask 中实现。 2. GitLab API通过 requests 库调用 GitLab 的 API Update MR由于通过 python-gitlab 库暂时没有实现更新功能，我们转向 GitLab 的 Update MR API ，详细的 API 说明见 Update MR。 1PUT /projects/:id/merge_requests/:merge_request_iid 在 Python 中实现该过程，需要通过 request 发起一个 HTTP PUT 请求，且至少需要 project id 和 merge request iid 两个参数，目前两个参数通过 Python-gitlab 库已经轻松得到。 在 Python 下，通过 requests 库发起一个 PUT 请求（requests.put(url, data=payload, headers=headers)），然后在 gitlab 的信息流中就会出现 merge request 的消息。在这个过程中，有两个地方需要注意： 一、一定要注意请求的参数是 merge_request_iid （一般只有一位，如 1 或 6）而不是 merge_request_id（有四位，如 1526），否则无法发起 PUT 请求，会返回 401 错误或 404 错误，正常情况下应该返回 200 二、assignee_id 是通过 data 参数传入，private-token 需要写入到头部信息 headers 中 另外一些 no required 的参数则是你需要按需添加的了，比如本例中要选择任务执行人（assignee_id）。一旦 PUT 请求成功后，你的 GitLab 主页便会收到任务提示信息。 123456789import requestsheaders = &#123;"Private-Token": 'your-private-token'&#125;payload = &#123; "assignee_id": [member_id] &#125;r = requests.put(""http://x.x.x.x/api/v4/projects/" + str(project_id) + "/merge_requests/" + str(merge_request_iid)", data=payload, headers=headers) 3. Flask经过上面的介绍，我们知道这几个参数是如何获取的，接下来就需要在项目中设定一个 webhook 了，即“安插卧底”。首先在项目的集成页面开启一个 webhook，设置好服务器地址和需要监控的行为。 然后还需要将配置信息用 flask 写成一个简单的应用，最后在服务器中运行该服务就可以监控每次 merge request 动作，并随机将 code review 任务分配给成员。其中，flask 会监控到 GitLab 收到的 POST 请求能够得到相应的数据，在返回的 body 中可以将 project_id 获取。到此为止，三个参数全部收集完毕。 1234body = request.data.decode(&apos;utf-8&apos;)params = json.loads(body)project_info = params.get(&quot;object_attributes&quot;, &quot;&quot;)project_id = project_info[&apos;source_project_id&apos;] PS：将 app.run() 改为 app.run(debug=True) 即可。这样每次修改代码之后，不需要每次都重启服务器。 12345678910from flask import Flaskfrom flask import request@app.route(&apos;/&apos;, methods=[&apos;POST&apos;])def index(): body = request.data.decode(&apos;utf-8&apos;) ··········if __name__ == &apos;__main__&apos;: app.run(debug=True, port=8001, host=&quot;0.0.0.0&quot;) 总的来说，GitLab 让用户可以设置 webhook 以监控用户的某些行为，这个想法可以用 flask 包装成为一个后台应用。 由于某些原因，完整的代码没有全部给出，相信这些教程网上应该大把大把的有。如果大家感兴趣的话可以给我发邮件交流详细的实现过程。后续还有一些比较好玩的地方，比如调用外部语音合成接口，每当任务分配完成后，语音播报具体的情况，这个应该会比较好玩。总之，你能想到干什么，GitLab 提供的 API 基本上都能帮你实现。 其他123# 在浏览器中直接输入，获取成员的信息# get memebers&apos; informationhttp://x.x.x.x/api/v3/users?secret-token=XXXXXXXXXXXXX 1234# 通过命令行获取curl --header &quot;PRIVATE-TOKEN: XXXXXXXXXXXXX&quot; http://XXX.XXX.XXX.XXX/api/v4/groups/:id/members/:user_idcurl --header &quot;PRIVATE-TOKEN: your-token&quot; http://XXX.XXX.XXX.XXX/api/v4/projects/:project_id/members/:member_id 获取 gitlab 项目 ID，通过命令行返回用户建立的 projects 或 groups 的信息 1curl -XGET --header &quot;PRIVATE-TOKEN: a994TnhXxoLMHHE_vgHw&quot; &quot;http://x.x.x.x/api/v3/projects/owned&quot; 推荐阅读 coding.net：webhook 介绍 GitLab documentation: Update MR Flask 快速入门中文文档]]></content>
      <categories>
        <category>教程</category>
      </categories>
      <tags>
        <tag>webhook</tag>
        <tag>Flask</tag>
        <tag>自动化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python | 异常处理]]></title>
    <url>%2F2017%2Fpython-try-exception-raise%2F</url>
    <content type="text"><![CDATA[1. try except在使用这种异常处理时需要注意，try 中的代码不要过于复杂，因为过多的代码容易导致定位异常变得十分困难。 同时，也不要单独使用一个 except 来捕获，否者错误定位容易出错。原因有，一、异常有很多种类，一个 except 可能捕捉不到；二、在 Python 中各种异常是内建的，具有一定的继承结构，所以父类异常一般要写得靠后，子类异常写得靠前。 通常在循环遍历变量时，如果在 try 语句中出现了异常，我们经常需要忽略异常继续执行 try 中的语句，这时候只需要在 except 后添加 continue 即可。 1234567try: # 需要执行的操作except () as e: # 可以依次排列多个异常处理 # 一旦出现错误，即跳出 try except print(e)continue: 2. try finallytry finally 语句有一项常见的用途，就是确保能够关闭文件句柄。比如，在写入文件过程中由于找不到文件而导致 IOError，不管怎么样，finally 中的 file.close() 是一定会执行的。 12345file = open(&apos;file_path&apos;, &apos;w&apos;)try: file.write(&quot;I&apos;m the best programmmer!&quot;)finally: file.close() 在阅读《编写高质量代码——改善python程序的91个建议》这本书的时候，书中有讲到在使用 try-finally 语句时，在 finally 中有个地方很容易出错。比如，在 finally 中使用 return 进行返回： 1234567891011def try_finally(a): try: if a &lt; 0: raise ValueError(&quot;输入不能为负&quot;) # 当 a = 2 过了 if 判断语句，else 语句并不会执行 else: return a except ValueError as e: print(e) finally: return -1 当输入为 -1 时，执行程序会抛出异常并且返回 -1。当输入为 2 的时候，想当然程序应该会返回 2。其实不然，当程序运行到 else 语句之前会先执行 finally 中的 return 语句，所以没等到先执行 else 语句，程序就提前结束了。 12345try: # 需要执行的操作finally: # 无论 try 中发生了什么 # finally 后的语句都会执行 3. raise exception如果在捕获异常之后要显示重复地抛出，特别是当你觉得系统给出的错误信息不足需要补充，raise 语句能够帮助你做到这一点。 1234567891011121314&gt;&gt;&gt; try:... print(1 / 0)... except:... raise RuntimeError(&quot;Something bad happened&quot;)...Traceback (most recent call last): File &quot;&lt;stdin&gt;&quot;, line 2, in &lt;module&gt;ZeroDivisionError: division by zeroDuring handling of the above exception, another exception occurred:# 除了系统抛出的错误，raise 语句还会重复抛出错误Traceback (most recent call last): File &quot;&lt;stdin&gt;&quot;, line 4, in &lt;module&gt;RuntimeError: Something bad happened 4. 例子新打开一个文件 test.txt，使用 chmod 取消文件的写入权限（chmod -w test.txt），人为“故意”制造一个错误。 1234567891011121314151617181920#!/usr/bin/python# -*- coding: UTF-8 -*-try: f = open(&quot;test.txt&quot;, &quot;w&quot;) f.write(&quot;this is a test file for error!&quot;) print(&quot;success!!!&quot;)# 程序会依次经过两个 except 语句# 如果没有则忽略# 本例中的 ZeroDivisionError 显然不会被打印出来except ZeroDivisionError as e: print(&quot;这是 except 语句，显示除法错误！！！&quot;)except IOError as e: print(&quot;这是 except 语句，显示文件写入错误！！！&quot;) raise else: print(&quot;没有出现错误，大吉大利！！！&quot;)# 不管最终咋样，finally 语句依旧执行finally: print(&quot;这是 finally 语句，最终 finally 语句依旧会执行！！！&quot;) 推荐阅读 Python Exception Handling - Try, Except and Finally 菜鸟教程：Python 异常处理 廖雪峰：错误、调试和测试]]></content>
      <categories>
        <category>coding</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>异常处理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux 使用笔记（2）修改文件权限 chmod]]></title>
    <url>%2F2017%2Flinux-operations-notes-chmod%2F</url>
    <content type="text"><![CDATA[修改口令密码 passwd 修改文件权限 chmod 1chmod [选项] [参数] 1234567891011# 运算选项符号+ # 权限操作符，添加某些权限- # 权限操作符，数字代号为“0”，取消某些权限= # 权限操作符，设置文件的权限为给定的权限# 权限设定符号 r # 权限设定（英文），数字代号为“4”，表示可读权限w # 权限设定（英文），数字代号为“2”，表示可写权限x # 权限设定（英文），数字代号为“1”，表示可执行权限X # 权限设定，如果目标文件是可执行文件或目录，可给其设置可执行权限--version # 显示版本信息 举例， 1chmod -rw-r--r-- file.txt # 取消 file.txt 文件的写入权限 ls -l 第一列就是文件权限信息，每个文件（任意类型的文件或者文件夹）的属性都用 10 个字符表示，按照 1-3-3-3 分段理解。比如，-rw-r--r--，第一个字符 - 表示这是一个文件（如果是 d 则是文件夹），第二到第四个字符 rw- 表示文件所有者的权限，意思是可读（r）可写（w），第五到第七个字符表示文件所有者所在组只拥有读（r）的权限，第八到第十个字符表示其他人只拥有（r）的权限。 权限的设置也可以简写为数字，比如上例中 -rw-r--r-- 就可以改写为 1234# rw- // 4+2+0=6# r-- // 4+0+0=4# r-- // 4+0+0=4chmod 644 xxxx.txt]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>文件权限</tag>
        <tag>chmod</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Git 使用笔记（1）：基本操作命令]]></title>
    <url>%2F2017%2Fgit-notes-common-operation-command%2F</url>
    <content type="text"><![CDATA[1. 单人作业单人的代码管理，理解一下几个命令就足够了 1234git clone &lt;path/to/repository&gt; # 版本库的地址git add &lt;filename&gt; # 将文件改动保存到缓存区git commit -m &quot;代码提交信息&quot;git push origin master # 将改动推送到版本库 2. 多人协作123456git checkout -b &lt;branchname&gt; # 新建一个分支，分支没有 # merge 成功前只有自己可见git push origin &lt;brachname&gt; # 将项目改动提交到分支git checkout master # 切换到主分支git checkout -d &lt;branchname&gt; # 删除分支git merge &lt;branchname&gt; # 合并分支 1git pull # 更新本地仓库至最新改动 3. 版本控制查看提交历史 12git loggit reflog 版本回退至 commit id 1git reset --hard &lt;commit_id&gt; 回退至上一个版本 1git reset --hard HEAD^ 推荐阅读 git - 简易指南 廖雪峰：Git教程]]></content>
      <categories>
        <category>教程</category>
      </categories>
      <tags>
        <tag>Git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PyCharm IDE 使用笔记（1）：新建虚拟环境和更换源]]></title>
    <url>%2F2017%2Fpycharm-ide-notes-virtualenv-and-manage-repository%2F</url>
    <content type="text"><![CDATA[1. Python 新建虚拟环境1.1 安装 virtualenv安装 virtualenv 12# install virtualenvpip install virtualenv 在你的项目下新建一个虚拟的 Python 环境，该环境下的 Python 的“纯净版”的 Python，没有安装任何库，所以可以为不同项目自定义 Python 的运行环境。 12$ cd your-project-path$ virtualenv your-project-name 在 PyCharm 的 project interpreter 中找到项目路径下的 Python 虚拟环境文件，并添加；或者点击齿轮状的图标，通过 create virtualenv添加虚拟环境。然后到程序的主界面，找到 Edit Configuration 并设置 Python interpreter 为刚刚添加好的虚拟环境文件。最后，在 中打开运行终端，命令行头部会显示虚拟环境的别名，再安装相应的库。至此，一个完全独立的 Python 虚拟环境就建立好了。 1.2 conda 建立虚拟环境123456# 创建虚拟环境conda create -n [env_name] python=X.X # 激活虚拟环境source activate [env_name]# 终止虚拟环境source deactivate [env_name] cd 到 anaconda 的 env 文件夹，可以查看目前已经创建的虚拟环境 123456# 查看某个虚拟环境下已经安装好的包conda list -n [env_name]# 给虚拟环境安装包conda install -n [env_name] [package_name]# 查看conda现有的虚拟环境列表conda info --env 如果不想再使用某个虚拟环境，直接删除即可 1conda remove -n [env_name] --all 1.3 激活、退出虚拟环境1234# 实际是激活虚拟环境下的 activate 脚本source [your/virtualenv/path/activate]# 退出虚拟环境deactivate 1.4 创建特定版本的虚拟环境比如有的库只在 python2 或者某个版本下才能运行，这时也可以用 virtualenv 来创建，比如下面这条命令就是新建一个 2.7 的虚拟环境。 1virtualenv -p /usr/bin/python2.7 my_project 1.5 删除虚拟环境要删除一个虚拟环境，只需要删除它的文件夹就 OK 了。 2. 更换安装源同样也在的 project interpreter 中进行设置，如下图 国内比较快的源有： 豆瓣(douban) https://pypi.douban.com/simple 阿里云 https://mirrors.aliyun.com/pypi/simple 中国科技大学 https://pypi.mirrors.ustc.edu.cn/simple 清华大学 https://pypi.tuna.tsinghua.edu.cn/simple 推荐阅读1.Create virtual environments for python with conda]]></content>
      <categories>
        <category>教程</category>
      </categories>
      <tags>
        <tag>PyCharm</tag>
        <tag>IDE</tag>
        <tag>虚拟环境</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java | 自动类型转换和强制类型转换]]></title>
    <url>%2F2017%2Fauto-conversion-and-narrow-conversion-in-Java%2F</url>
    <content type="text"><![CDATA[在 Java 中，不同的数值类型能够表示的数值范围不相同，不同的数值类型经常需要互相转换，有自动转换和强制转换两种方式。 数值类型的表示范围可以理解为盛放液体的容器，容量小的容器可以随意将液体倒入大容量容器。反过来则不行，多余的液体会溢出。 强制类型转换分为两种情形，比如 byte(233) 和 byte(300) 两者的计算略有差别。 233 是 4 字节 32 位的 int 型，二进制表示为 00000000 00000000 0000000 11101001，byte 型是 1 字节 8 位，属于强制类型转换。强制转换的过程中，Java 直接将前 24 位截断，只保留后面 8 位 11101001。 11101001 最左一位是 1，表示是负数，而负数是以补码的形式存在的，因此还需要换算成原码。最左一位固定不动，其他 7 位取反，即 10010111，换算成十进制即为 -23。 300 是 4 字节 32 位的 int 型，二进制表示为 00000000 00000000 00000001 00101100，转 byte 属于强制类型转换。Java 将前 24 位截断，只保留后面 8 位 00101100。 最左一位是 0，表示是正数，故不需要反码——&gt;原码的过程，换算成十进制即为 44。 123456789101112public class test&#123; /*自动类型转换和强制类型转换*/ public static void main(String[] args) &#123; byte i = (byte)233; System.out.println("(byte)233 result is: " + i); byte j = (byte)300; System.out.println("(byte)300 result is: " + j); &#125;&#125;]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>类型转换</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习中的正则化]]></title>
    <url>%2F2017%2Fregularization-in-machine-learning%2F</url>
    <content type="text"><![CDATA[通常，我们为了追求模型在训练集上的表现，在模型中加入了过多的参数，它通过“死记硬背”式的方式学到了很多东西。但是，往往数据中也是存在噪声的，模型把这些噪声也学到了，导致当它去预测新数据的时候预测能力不佳。 The general principle is we want both a simple and predictive model. The tradeoff between the two is also referred as bias-variance tradeoff in machine learning. 通常我们需要一个简单并且预测能力强的模型，这两者之间的权衡通常就是指机器学习中偏差方差的权衡。 如何达到这个目的？正则化！ 关于偏差和方差，更多：如何解释方差与偏差的区别？ 了解正则化之前，有必要弄明白范数，不太严谨地解释： 0 范数，向量中非零元素的个数。 1 范数，为绝对值之和。$||\beta||$ 2 范数，就是通常意义上的模。${||\beta||}^2$ 正则化的目的是为了控制模型的复杂度，即通常所说的过拟合，实际上是 bias 和 variance 之间的一个 tradeoff，往往模型的 bias 很低，但是一接触到新的数据就表现不太好（泛化能力不佳）。既然，我们需要一个简单并且预测能力强的模型，正则化如何达到这个目的呢？L1 正则化是可以减少特征的数量，L2 正则化则是可以降低特征的权重。 这两者有什么不同呢？两者优化得到的解有差别，从图中来看，L2 正则化的最优解，即预测错误（损失函数）等高线与惩罚值等高线（正则化项）的相交点，最优解的点比较随机，此时 $\beta_1$ 和 $\beta_1$ 的值不一定为零。 观察 L1 正则化，两条等高线的相交点出现在坐标轴处，这就意味着特征必有一个为零（该例中 $\beta_1$ 等于零）最优解出现的地方往往出现在某个轴。 特征多了不好，模型的泛化能力不强。特征少了也不行，模型的预测能力太弱。 推荐阅读 https://dsp.stackexchange.com/questions/17416/l-0-norm-minimization-in-compressive-sensing Introduction to Boosted Trees：Objective Function : Training Loss + Regularization]]></content>
      <categories>
        <category>Machine-Learning</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>Machine-Learning</tag>
        <tag>正则化</tag>
        <tag>regularization</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Sublime Text 3 快捷键操作笔记]]></title>
    <url>%2F2017%2Fsublime-text-3-hot-keys%2F</url>
    <content type="text"><![CDATA[1. 文件操作插入注释：Ctrl+Shift+/ 跳到第 N 行：Ctrl+G 删除光标所在行：Ctrl+X 选择光标所在行：Ctrl+L 打开文件侧栏：Ctrl+K+B 选择侧栏中已打开文件：Alt+数字 新建文件：Ctrl+N 跳到花括号：Ctrl+M 2. 快速选择shift+右键移动：选择多个内容 shift+↑：向上选中多行 shift+↓：向下选中多行 Shift+←：向左选中文本 Shift+→：向右选中文本 Ctrl+Shift+↑：将光标所在行和上一行代码互换（将光标所在行插入到上一行之前）。 Ctrl+Shift+↓：将光标所在行和下一行代码互换（将光标所在行插入到下一行之后）。 3. 文本编辑Ctrl+Z 撤销 Ctrl+Y 恢复撤销 Ctrl+F2 设置书签 Ctrl+Shift+D 复制光标所在整行，插入到下一行 4. 视窗双栏操作：Alt+Shift+2 快速切换左右两边窗口：ctrl+数字]]></content>
      <categories>
        <category>教程</category>
      </categories>
      <tags>
        <tag>sublime</tag>
        <tag>编辑器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习算法系列（14）梯度提升决策树]]></title>
    <url>%2F2017%2Fmachine-learning-algorithm-series-gbdt%2F</url>
    <content type="text"><![CDATA[1. GBDT与AdaBoost的联系与区别在前面的文章中，我们了解到了一种提升方法 —— AdaBoost，它是一类能够从训练数据中学习一组弱分类器，并将弱分类器组合成强分类器的算法，通俗一点解释就是“三个臭皮匠顶个诸葛亮”。GBDT跟AdaBoost一样都属于集成学习中的提升方法（另一种是Bagging），特殊之处在于GBDT的弱学习器限定为了决策树（分类树或回归树），提升树可以看做是多颗决策树的线性组合。不过AdaBoost算是年代比较久远的集成学习算法，现在基本上很少用到了。 除了弱学习器的不同，另外，它们两者在迭代上有所区别。AdaBoost利用的是上一轮学习器的误差率来更新数据的权重，在每一轮的迭代中误分类的数据和分类误差率小的模型会获得更高的权重，理论上AdaBoost的学习器可以选择任何分类模型，只是由于树形模型比较简单且容易产生随机性所以通常被用于基分类器。而GBDT的基学习分类器限定只能是CART回归树，顾名思义，它可以解决回归和分类问题，GBDT的迭代思路是使用前向分布算法，基本思想是根据上一轮学习器损失函数的负梯度来训练本轮的学习器，本轮学习器$f_{t}(x)$的优化的目标是将上一轮学习器的损失函数L优化得到一个最小值。如果这样解释还不太清楚，那我举一个例子，比如现在给你k次机会猜测我的真实年龄，并且下一次猜测的数字需要跟上一次猜测的数字相加才能成为下一次猜测的最终结果，第一次你猜测20岁，我说小了，然后第二次你说加10岁，我说大了，然后第三次你说减5岁，我说小了…… 而如何拟合GBDT的损失函数则是一个GBDT需要考虑的关键问题。 总结一下Adaboost 与 GBDT的区别： Adaboost 是基于样本权重更新，误分类的样本和误差率低的学习器可以获得更高的权重 Gradient Boost 是基于残差减小的梯度方向进行更新 理论上AdaBoost的分类器可以选择任何分类模型，GBDT的学习器则限定于分类回归树 可以这样说，如果限定提升树的基分类器为分类回归树（CART），那么此时的GBDT跟AdaBoost就没有差别了。 2. 回归问题的提升树算法下面讨论回归问题的提升树，其实相比分类问题，回归问题更好计算，没有 AdaBoost 那一套复杂的权重更新过程，每一次的迭代只需要拟合残差，每一棵新树的建立都是基于残差的负梯度方向。残差是什么？残差是实际值与预测值之差： r = y_i - f(x_i)打个比方，假设一个人真实年龄是30岁，第一颗回归树预测的结果是27岁，残差是3，下一颗树的目标就是假定预测一个年龄等于3岁的人，如果第二棵树预测的结果是2岁，那么第一颗和第二棵树相加的结果29就是两棵树的预测结果。 回归问题的提升树有两个值得关注的点： 一、寻找使得残差最小的切分节点 二、平方损失误差 $L$ 达到要求后即可停止。 3. 梯度提升（Gradient Boosting）上面一节中，可以知道，平方损失函数的情形很简单，但是对于更一般的损失函数，优化就没有那么简单了，于是我们需要在提升树算法中引入梯度下降的优化算法，即所谓的GBDT。GBDT的损失函数，如果是对于分类算法，其损失函数有对数损失和指数损失，如果是回归算法，其损失函数一般有平方损失、绝对值损失、Huber损失函数（对异常值的鲁棒性比较强）等等。梯度下降优化算法的关键是计算其负梯度在当前模型的值，并将该值作为近似的残差，最后根据残差拟合一个回归树。 -\left[ \frac{\partial L(y^{(i)}, f(x^{(i)}))} {\partial f(x^{(i)})} \right]_{f(x) = f_{k-1}(x)} \approx r_{m,i} 第2（c）步的$c_{mj}$是各个叶子节点最佳负梯度的拟合值，第2（d）步将该拟合值更新到下一轮的学习器中。 3. sklearn GBDT可调节的参数按照分类和回归问题，sklearn有GradientBoostingClassifier为GBDT的分类类， 而GradientBoostingRegressor为GBDT的回归类，两者的参数类型差不多相同。 loss：选择损失函数，分类问题的损失函数可以选择对数损失函数（deviance）和指数损失函数（exponential），回归问题的损失函数可以选择均方差损失函数（ls）、绝对损失函数（lad）、huber损失函数（huber）和分位数损失函数（quantile） learning_rate：学习率，默认0.1，取值范围(0,1]，文档提示这个参数经常需要跟n_estimators一块调节 n_estimators：学习器的迭代次数，一般可以把这个参数设置得大一点以获得更好的效果，默认是100 subsample：采样率决定数据中不放回抽样一定比率的样本拟合基学习器，抽样能减小方差防止过拟合，但是同时也会增加偏差 max_depth：决策树的最大深度，默认为3，max_depth限制了数的节点个数，如果数据集的特征和数据量比较大可以把这个值调大一点 min_samples_split：内部节点再划分所需要的最小样本数，可以取int或者float，如果在划分的时候子树的节点数小于这个参数便不会继续划分 min_samples_leaf：叶子节点所需要的最少的样本数，默认等于1，可以取int或者float min_weight_fraction_leaf：叶子节点最小的样本权重和，如果样本中有比较多的缺失值可以引进这个参数 max_leaf_nodes：最大叶子节点数，默认为None，可以防止模型过拟合 min_impurity_split：节点划分的最小不纯度，如果节点的不纯度小于它，那么该节点不再生成子节点 GBDT的优点和局限性优点： 由于GBDT的弱学习器是决策树，这就决定了我们在应用模型的时候无需对数据进行归一化等预处理 泛化能力强 局限性： 不适合处理文本分类的任务，在高维稀疏的数据集上表现也一般 因为GBDT是加法模型，下一个模型是基于上一个模型的，所以它的训练过程是串行的 推荐阅读 机器学习中的算法(1)-决策树模型组合之随机森林与GBDT chentianqi: XGBoost 与 Boosted Tree GBDT原理与Sklearn源码分析-分类篇 - SCUT_Sam - CSDN博客 GBDT原理与Sklearn源码分析-回归篇 - SCUT_Sam - CSDN博客]]></content>
      <categories>
        <category>Machine-Learning</category>
      </categories>
      <tags>
        <tag>决策树</tag>
        <tag>机器学习算法系列</tag>
        <tag>Machine-Learning</tag>
        <tag>梯度提升</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java | 变量]]></title>
    <url>%2F2017%2Fvariables-in-Java%2F</url>
    <content type="text"><![CDATA[变量本质是内存中的一块空间，数值类型、变量名、字面值（数据），在 Java 中变量应该注意以下几点： 字面值数据类型必须和变量声明的类型是一致的，比如说 int age = true 这种用法就是不当的 声明和变量赋值可以放在一块 访问变量有两种方法：set 和 get 变量在没有赋值之前是无法访问的，强行访问会出错 在同一个作用域中，变量名不能重叠，如果一个变量叫做 a，那么在一个花括号{}内就不能再声明一个名为 a 的变量 在 Java 中，变量可以分为八种类型，以下是它们的分类及占用空间大小： 数值型 整型 byte 1 字节 8 位 short 2 字节 16 位 int 4 字节 36 位（用得最多） long 8 字节 64 位 字符型 char 2 字节 16 位 浮点型 float 4 字节 32 位 double 8 字节 64 位（默认是 double 型） boolean 型 boolean 一个字节等于八个比特位，一个比特位表示一个二进制位：1/0 在 Java 中使用变量是需要注意的点，整理的有点杂乱，防止遗忘： char y = &#39;ab&#39;; 编译将会出现错误，因为 ab 是字符串而非字符 ASCII 最多支持 256 种类型的编码，覆盖英文字符绰绰有余，后来随着计算机的发展，又出现了一种编码方式，由 ISO 制定，支持西欧语言，但是不支持中文。随着计算机在亚洲的普及，后来又逐渐出现了支持简体中文的编码方式：GB2312、GBK、GB18030，繁体中文 big5（大五码），再到后来出现了 Unicode 编码方式，统一了全球的字符编码 一个汉字占用两个字节，16 个比特位 在声明 long 类型数据时，一定要记得在字面值后面加上 L，比如 long x = 2147483648，就会出现编译错误，因为 2147483648 被当做了 int 类型来处理。要解决这个问题，只需要在 2147483648 后加上一个 L 就行了，long x = 2147483648L 整数型字面值以 0 开头的，表示数字是八进制形式；整数型字面值以 0x 开头的，表示数字是十六进制；以 0b 开头的表示数字是二进制 负数在计算机里是以补码的形式存在，因此还需要换算为原码 大容量转换成小容量是需要添加强制类型转换符的]]></content>
      <categories>
        <category>coding</category>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>变量</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java | public class 与 class 的区别]]></title>
    <url>%2F2017%2Fthe-difference-between-public-class-and-class-in-Java%2F</url>
    <content type="text"><![CDATA[一个 java 源文件当中可以定义多个 class 一个 java 源文件当中 public 的 class 不是必须的 一个 class会定义生成一个 xxx.class 字节码文件 一个 java 源文件当中定义公开的类的话，只能有一个，并且该类名称必须和 java 源文件名称一致。 每一个 class 当中都可以编写 main 方法，都可以设定程序的入口，想执行 B.class 中的 main 方法：java B，想执行 X.class 当中的 main 方法：java X 注意：当在命令窗口中执行 java Hello，那么要求 Hello.class 当中必须有主方法。没有主方法会出现运行 阶段的错误： 123D:\course\JavaProjects\02-JavaSE\day02&gt;java Hello错误: 在类 Hello 中找不到主方法, 请将主方法定义为: public static void main(String[] args)]]></content>
      <categories>
        <category>coding</category>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>public class</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[常用 DOS 命令]]></title>
    <url>%2F2017%2Fdos-command%2F</url>
    <content type="text"><![CDATA[cls：清屏 dir：列出当前目录下所有文件 cd 相对路径/绝对路径 cd ..： 返回上级目录 cd \： 返回根目录 切换盘符：d: e: f: exit：退出 cmd]]></content>
      <categories>
        <category>教程</category>
      </categories>
      <tags>
        <tag>DOS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[从毛选（第一卷）中谈个人与组织成长]]></title>
    <url>%2F2017%2Ftalking-about-personal-and-organization-growth%2F</url>
    <content type="text"><![CDATA[这篇笔记是我在从北京回湖南的火车上完成的，阅读和读书笔记花费时间约为 5 个小时，挺值得一读的。]]></content>
      <categories>
        <category>思想王国</category>
      </categories>
      <tags>
        <tag>读书</tag>
        <tag>阅读</tag>
        <tag>个人成长</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[matplotlib 无法显示中文]]></title>
    <url>%2F2017%2Fmatplotlib-unable-use-Chinese%2F</url>
    <content type="text"><![CDATA[直接使用本地的 ttc 格式字体源，本例中使用的是 Windows 自带的微软雅黑字体，当然你也可以更换成其他的字体，但是前提是你预先下载好该字体的源文件。 123456789101112import matplotlib.font_manager as fmmyfont = fm.FontProperties(fname='C:/Windows/Fonts/msyh.ttc')%matplotlib inline# 记得在中文前加上 u 字符import matplotlib.pyplot as pltplt.title(u'测试中文', fontproperties=myfont)plt.xlabel(u'测试中文', fontproperties=myfont)plt.ylabel(u'测试中文', fontproperties=myfont)plt.legend()plt.grid(True)plt.show() 运行上述代码，matplotlib 无法显示中文的问题就解决了。]]></content>
      <categories>
        <category>教程</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>matplotlib显示中文</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【译】自然语言处理中的深度学习：优势与挑战]]></title>
    <url>%2F2017%2FDeep-Learning-for-Natural-Language-Processing%2F</url>
    <content type="text"><![CDATA[本文翻译自李航老师发表在 National Science Review 上关于自然语言处理中的深度学习文章，该文讨论了目前存在的优势与挑战。 1. 引言深度学习指学习和使用“深度”人工神经网络的机器学习技术，比如深度神经网络（DNN）、卷积神经网络（CNN）和循环神经网络（RNN）。近来，深度学习成功地应用在 NLP 中并取得了很多重要的进展。这篇文章总结了深度学习在 NLP 中取得的进展，最后讨论它的优势和面临的挑战。 我们认为在 NLP 中主要有五个任务，包括分类、匹配（matching）、翻译、结构化预测（Structured prediction）和序列决策过程（sequential decision process）。对于前四个任务，研究发现深度学习方法的表现已经超过或者可以说远远超过传统的手段。 端到端训练（end-to-end training）和表示学习两者都是使得深度学习在 NLP 中成为强大的工具的关键特征。然而，深度学习并不是万能的。在多轮对话（multi-turndialogue）中推理和决策的能力十分关键，但是深度学习并不擅长。另外，如何结合符号处理（symbolic processing ）和神经处理（neural processing），如何处理长尾现象等等问题也都是深度学习在 NLP 中的挑战。 2. NLP 流程我们认为， NLP 中有五大任务，即分类、匹配、翻译、结构化预测和序列决策处理过程。NLP 中的大多数问题都可以归结为以上五类任务，如表 1。在这些任务中，尽管有着不同的复杂度，单词、短语、句子甚至文档通常被当做 标记序列（sequence of tokens，即字符串 strings）来同等对待。事实上，句子是最常使用的处理单元。 据观察，深度学习能够帮助提升前四个任务的效果，并成为了这些任务中的前沿技术。 表 2 中展示的例子深度学习已经超过了传统方法。在所有的 NLP 任务中，机器翻译取得进展是最显著的。比如神经机器翻译，使用了深度学习后的机器翻译显著超过了传统的统计机器学习方法。前沿的神经翻译系统应用了包含 RNNs 的 sequence-to-sequence 学习模型。 深度学习也首次使得某些应用成为了可能。比如，深度学习在图像检索（image retrieval）就得到了成功的应用，在图像检索中，请求和图像都被 CNN 转化成为了向量表示的形式。另外，深度学习也在 generationbased 自然语言对话中得到了应用， 在多轮对话中的关键议题，即第五个任务，比如序列决策处理，比如马尔科夫决策过程。深度学习在这个任务中到底能够发挥多大的作用，目前还没有得到广泛的验证。 3. 优势与挑战的确，在 NLP 中应用深度学习，优势与挑战并存，总结如表 3 3.1 优势我们认为，在所有的优势中，端到端训练和表示学习是深度学习区别于传统机器学习方法关键，并且这两者让深度学习成为了 NLP 中一个强大的机器。 通常，在深度学习的应用中施展端到端训练，这是因为 DNN 模型在数据中提供了丰富的表示性（representability）和信息，它们能够在模型中高效编码（encoded）。比如，在神经机器翻译中，模型完全是由并行语料库中自动建立起来的，并且通常不需要人为的干预。对比需要特征工程的传统的统计机器翻译方法，深度学习优势明显。 在深度学习中，数据的表示有多种不同的形式，比如，文本和图像，都能够以实数值向量的形式学习。数据表示的形式使得信息处理能够在多个模式（multiple modality）中实现称为可能。比如，在 图像检索中，可以通过请求（文本）匹配最相关的图像，因为它们本质上都是向量（注：正因为都是数值型向量，那么它们便具备了统计意义）。 3.2 挑战深度学习遭遇的挑战很常见，比如，缺乏理论基础、解释性模型、需要大规模数据和强大的计算资源。在 NLP 中遭遇的挑战不太一样，比如，处理长尾现象困难、无法直接处理符号、推理决策方面效率低下。 由于自然语言的数据通常服从幂率分布（power law distribution），词汇量会一般会随着数据量的增大而增大。这意味着无论训练数据有多么大，永远存在训练数据无法覆盖到的词汇。如何处理长尾问题成为了摆在深度学习面前的重大挑战。仅仅依靠深度学习，这个问题可能很难解决。 与深度学习使用到的向量数据（real-valued vectors）有所不同，语言数据本质上是符号数据。目前，语言中的符号数据转化为向量数据然后输入到神经网络中，神经网络的输出再转化为符号数据。事实上，NLP 中大量的知识是以符号的形式存在，包括语义知识（如语法），词汇知识（lexical knowledge，如 WordNet）和世界知识（如维基百科）。目前，深度学习还没有好好利用起来这些知识。符号表示易于解释和操作，而另一方面，向量表示在模糊（ambiguity）和噪音条件下表现稳健。如何组合符号数据和向量数据，如何充分利用两种数据类型的优势在 NLP 中仍然是一个开放性问题。 NLP 中有复杂的任务，仅仅依靠深度学习使不够的。比如，多轮对话就是一个非常复杂的处理过程。它涉及到语言理解、语言生成、对话管理、知识库访问（knowledge base access）和推理。对话管理可以形式化为序列决策过程，增强学习还可以起到关键作用。显然，深度学习和增强学习的组合对处理这些任务有相当大的潜力，远远超出了深度学习本身。 总的来说，在 NLP 中，深度学习仍然面临着许多挑战。当结合了其他技术（增强学习、推理、知识）之后，深度学习才有可能进一步推动这个领域（NLP）的发展。 原文链接：https://academic.oup.com/nsr/article/doi/10.1093/nsr/nwx110/4107792/Deep-Learning-for-Natural-Language-Processing]]></content>
      <categories>
        <category>Machine-Learning</category>
      </categories>
      <tags>
        <tag>翻译</tag>
        <tag>深度学习</tag>
        <tag>自然语言处理</tag>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【译】数据科学家降维技术测试 40 题]]></title>
    <url>%2F2017%2Fquestions-dimensionality-reduction-data-scientist%2F</url>
    <content type="text"><![CDATA[原文链接：https://www.analyticsvidhya.com/blog/2017/03/questions-dimensionality-reduction-data-scientist/ 前言 你有没有遇到过拥有几百列的数据集而对其建立预测模型不知所措？或者数据集的变量大量相关？在现实的工程问题中这些问题是很难避免的。 幸亏，降维技术帮了我们大忙，在数据科学中，降维是一门很重要的技术。对于任何数据科学家来说，降维技术是必备的。为了检测我们在降维技术中的知识，我们组织了这次技能测试。这些问题包括主成分分析、t-SNE 和 LDA。 Q &amp; A 假如你在机器学习问题中有 1000 个输入特征和一个目标特征，基于输入特征和目标特征之间的关系，要求你选出100个最重要的输出特征，请问这是一个降维的例子吗？ 是 否 【判断题】在应用降维算法时没有必要设置目标变量【对】 解析：LDA 是有监督降维算法 数据集中有 4 个变量 A、B、C、D，执行下列步骤 step1：使用以上变量创建两个新的变量，$E=A+3B$ 和 $F = B + 5*C + D$ step2：只用 E 和 F 两个变量建立随机森林模型 问：以上步骤是否属于降维手段的一种 对 错 解析：因为在第一个步骤中使得数据降低到只有两个维度 下列哪种方法降维的效果更好？ A. 去除缺失值多的列（如果数据集中某一列缺失值太多，可以肯定的是一定要去掉） B. 去除高方差的列 C. 去除数据趋势不同的列 D. 以上都不是 【判断题】降维算法是建立模型时减少运算时间的一种可行方法。【对】 下列哪种算法不能用于数据的降维？ A. t-SNE B. PCA C. LDA False D. 以上都不是 【判断题】PCA 可用于将数据映射和可视化到低维空间。【对】 有的时候有必要将数据集在低维空间中可视化出来，我们可以用前两个主成分并画出它们的散点图 PCA 是非常受欢迎的降维算法，下列对于PCA的说法正确的是？【全部都是】 PCA 是无监督方法 寻找原始数据中方差最大的方向（原理解释参考：机器学习算法系列（10）主成分分析（PCA）） 主成分最大个数小于等于特征个数 所有的主成分相互正交（协方差矩阵是对称的） 假设用降维算法给数据预处理，将数据降到只有 k 个维度之后，然后用这些 PCA 映射作为特征，下列陈述正确的是？ A. k 越大则正则化效果越强 B. k 越大则正则化效果越弱（k 越大意味着要保留数据中的更多特征，所以正则化效果越弱） 考虑一台计算能力不强的计算机，下列哪种降维场景中，t-SNE 比 PCA 表现得要好？ A. 数据集有 100 万 entries，300 个特征 B. 数据集有 10 万 entries，310 个特征 C. 数据集有 1 万 entries，8 个特征 D. 数据集有 1 万 entries，200 个特征 t-SNE 有着$O(n^2)$的时间和空间复杂度，考虑计算能力有限的系统资源，则必须选择 c，因为它只有 8 个特征，并且在这样一个小特征的数据集降维重要信息的丢失是很少的 对于 t-SNE 的损失函数，下列陈述正确的是？ A. 不对称 B. 对称 C. 与SNE的损失函数一样 SNE的损失函数本质上是不对称的，故使用梯度下降算法很难收敛。损失函数是不是对称是 SNE 与 t-SNE 最大的区别 假设在处理文本数据中使用词嵌入（Word2vec），得到一个 1000 个维度的词向量。现在，你想要降低它的维度，这样最近邻空间的单词具有同样的含义。在这种情况下，你会选择哪个算法？ t-SNE PCA LDA 【判断题】t-SNE 学习的是非参映射【对】 对 PCA 和 t-SNE 说法正确的是 t-SNE 是线性的，PCA 是非线性的 t-SNE 和 PCA 都是线性的 t-SNE 和 PCA 都是非线性的 t-SNE 是非线性的，PCA 是线性的 在 t-SNE 算法中，哪个高维参数可以调参？ 维度的个数 Smooth measure of effective number of neighbours 最大迭代次数 以上所有 t-SNE 与 PCA 相比，下列说法正确的是？ 数据集很大的时候，t-SNE 可能产生不了较好的结果 不管数据集大或者小，t-SNE 总能产生更好的结果 针对小规模数据，PCA 总是比 t-SNE 表现得更好 以上都不是 $x_i$和$x_j$是高维数据中的两个不同的点，$y_i$和$y_j$是$x_i$和$x_j$在低维空间的映射 $x_i$和$x_j$的相似度等于条件概率$p(j|i)$ $y_i$和$y_j$的相似度等于条件概率$q(j|i)$ $p(j|i)=0$，$q(j|i)=0$ $p(j|i) \lt q(j|i)$ $p(j|i) = q(j|i)$ $p(j|i) \gt q(j|i)$ 关于 LDA 正确的是？ LDA 的目标在于最大化不同类之间的距离且最小化相同类之间的距离 LDA 的目标在于同时最小化相同和不相同类之间的距离 LDA 的目标在于最小化不同类之间的距离且最大化相同类之间的距离 LDA 的目标在于同时最小化相同和不相同类之间的距离 哪种情形下，LDA 会失败？ 如果 discriminatory information 不在数据的方差中而在均值中 如果 discriminatory information 不在数据的均值中而在方差中 如果 discriminatory information 都在数据的方差和均值中 都不是 下列关于 PCA 和 LDA 的比较,正确的是？ LDA 和 PCA 都是线性转换的手段 LDA 是监督学习而 PCA 是无监督 PCA 最大化数据的方差，而 LDA 最大化不同类别之间的间隔 当特征值都大致相等时？ PCA 表现很好 PCA 表现不好 不确定 当特征向量都相同时，在这种情况下你不可能选择主成分因为主成分都是相等的 满足下列哪种条件，PCA 会表现得更好？ 数据集有线性结构 如果数据集在弧面而不是平面 变量都在同一个单位空间 低维空间中使用 PCA 得到的特征？ 特征仍有解释性 特征将失去解释性 特征一定携带目前数据的所有信息 特征可能没有携带目前数据的所有信息 给定以下高度和重量的散点图， 0 45 60 90 下列哪个选项是正确的？ PCA 需要初始化参数 PCA 不需要初始化参数 PCA可能会陷入局部最小化问题 PCA 不会陷入局部最小化问题 下图是两个特征的散点图，PCA 和 LDA 的方向，哪种方法能够得到比较好的分类结果？ 用 PCA 建立分类算法 用 LDA 建立分类算法 不确定 如果任务的目标是划分点集，PCA 映射会得不偿失 针对图像数据运用 PCA 时，下列哪个选项是正确的？ 检测变形的物体会很有效率 It is invariant to affine transforms 可以用于lossy图像压缩 not invariant to shadows 在何种情形下，SVD 和 PCA 产生一样的结果 数据中位数为 0 数据均值为 0 都是一样的 如果数据拥有零均值，在使用SVD的时候首先你得center the data 考虑二维空间下的三个数据点，数据的第一个主成分是？ [$\sqrt2/2, \sqrt2/2$]√ [$1/\sqrt3, 1/\sqrt3$] [$-\sqrt2/2, \sqrt2/2$]√ [$-1/\sqrt3, -1/\sqrt3$] 如果用主成分把原始数据投影到一维子空间，它的坐标是多少？ $(-\sqrt2,0,\sqrt2)$√ $(\sqrt2,0,\sqrt2)$ $(\sqrt2,0,-\sqrt2)$ $(-\sqrt2,0,-\sqrt2)$ 根据29-31对于你从$(-\sqrt2,0,\sqrt2)$得到的数据，如果你要将它们呈现在二维空间中，损失会是多大？ 0% 10% 30% 40% 在 LDA 中，最理想的是找到划分两个类别的线。在给定图像中哪个映射是最好的？ LD1 LD2 PCA 是一门很好的降维技术，因为它易于理解并且被广泛应用。观察f(M)是如何随着M的移动而变化的，见下图。问，上面两幅图中 PCA 的表现更佳？ 左图 右图 下列哪个选项是正确的？ LDA 尝试找出数据类之间的不同，PCA 则不是 LDA 和 PCA 两者都尝试找出数据类的不同 应用 PCA 之后，下列选项哪个是前两个主成分？ (0.5,0.5,0.5,0.5),(0.71,0.71,0,0) (0.5,0.5,0.5,0.5),(0,0,-0.71,-0.71) (0.5,0.5,0.5,0.5),(0.5,0.5,-0.5,-0.5) (0.5,0.5,0.5,0.5),(-0.5,-0.5,0.5,0.5) 下列哪个选项给出了 LR 和 LDA 之间的差别？ 如果类别都被很好地切割了，LR 的参数估计可能会不太稳定 如果样本集数据太小并且每个类别数据特征都服从正态分布。在这种情况下，LDA 比 LR 要更稳定 下列两种补偿需要考虑 PCA？ 垂直补偿（vertical offset） 垂直面补偿（perpendicular offset） 如果你在处理10类别分类问题，LDA 最多可以生成多少个discriminate向量？ 20 9 21 10 根据下图，使用 PCA 和最近邻方法构造预测是否为“Hoover”的分类器，需要哪些数据预处理？ 将图片中的塔放置在图片中央 将所有图片处理至相同尺寸 下图中最优主成分数目为多少？ 7 30（方差最大主成分数目最少） 40]]></content>
      <categories>
        <category>Machine-Learning</category>
      </categories>
      <tags>
        <tag>翻译</tag>
        <tag>主成分分析</tag>
        <tag>降维</tag>
        <tag>PCA</tag>
        <tag>LDA</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java 的参数传递方式]]></title>
    <url>%2F2017%2FJava-parameter-transfer-mechanism%2F</url>
    <content type="text"><![CDATA[Java 的方法（method）有两个特点： 方法不能独立存在，必须依赖类和对象 方法不能被独立执行，只能以“类.方法”或“对象.方法”的方式执行 方法若没有 static 关键字的修饰，则方法属于类的实例 简单一点理解：如果说“骑单车”是一种方法，那么脱离了人或熊，没有了对象的“骑单车”也就没有意义，也不可以单独执行，因为力的作用是相互的，没有施力物体，单车不可能自己动，总不可能是一个鬼在“骑单车”吧==!。 Java 里方法的传递方式只有一种：值传递，就是将实际参数（实参）的副本传入到方法中，而实际参数的本身不会受到影响。实际参数是在 main 方法中定义的初始变量，如 123456public static void main(String[] args)&#123; // a, b 均为实际参数 int a = 6; int b = 9;&#125; 在 Java 中基本类型的参数传递比较好理解，有点难理解的是引用类型的参数传递。 1234567891011121314151617181920212223242526272829303132class DataWrap&#123; // 定义两个成员变量 a 和 b int a; int b;&#125;public class ReferenceTransferTest&#123; public static void swap(DataWrap dw) &#123; // swap 方法实现 a 和 b 的值交换 int tmp = dw.a; dw.a = dw.b; dw.b = tmp; System.out.println("swap 方法里，a 成员变量的值是" + dw.a + "；b 成员 变量的值是" + dw.b); &#125; public static void main(String[] args) &#123; // 新建 DataWrap 对象 dw DataWrap dw = new DataWrap; // 赋值 dw.a = 6; dw.b = 9; // 调用 swap 方法 swap(dw); System.out.println("交换结束后，a 成员变量的值是" + dw.a + "；b 成员 变量的值是" + dw.b); &#125;&#125; 如何理解 Java 的引用参数传递机制？ 首先，新建 DataWrap 对象 dw 并赋值，这时 main 方法和 swap 方法同时对堆内存中的 dw 对象有一个指针，即它们两个方法实际引用的是同一个内存区域；然后，main 方法中调用 swap 方法，完成值的交换，这时 dw 所在的内存区域 a 和 b 的值已经完成交换；最后，交换结束之后，dw.a 等于 9，dw.b 等于 6。需要注意的是，在调用 swap 方法时，传入到 swap 的并不是 dw 对象本身，而仅仅是它的副本。 形参个数可变的方法 当调用方法的时候，可以传入多个字符串作为参数值，在 Java 中有两种传入多个参数的方法。 1234\\形参个数可变方式 public static void method(int price, String... brand);\\数组形式public static void method(int price, String[] brand); 两种方式分别调用的方式为： 12method(100, "BMW", "QQ", "BYD");method(100, new String[]&#123;"BMW", "QQ", "BYD"&#125;); 从上面两种调用方式来看，第一种更加简洁易懂，推荐使用第一种。 在 Java 中，允许同一个类中定义多个同名方法，区分它们的地方在于形参列表不同就行了，这种被称为方法重载。 static 关键字static 关键字修饰的成员属于类本身，而不属于该类的实例。所以，用实例去调用 static 修饰的成员变量和方法是不行的，会出现“无法从静态上下文中引用非静态方法…”的错误。解决此问题的办法是重新建立一个对象。 123456789101112131415public class CallPhone&#123; public void call() &#123; System.out.println("正在打电话..."); &#125; public static void main(String[] args) &#123; //新建对象访问非静态方法 CallPhone p = new CallPhone(); p.call(); // this.call(); // System.out.println("正在打电话给X..."); &#125; &#125; 在 CallPhone p = new CallPhone(); 这行代码中创建了一个实例也就是对象，实际上生成了两个东西：一个是变量 p，一个是对象 CallPhone，其中变量存储在栈内存中，对象存储在堆内存中，对象与变量之间通过引用的方式建立联系。]]></content>
      <categories>
        <category>coding</category>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>参数传递</tag>
        <tag>static</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java 面向对象：类与对象的关系]]></title>
    <url>%2F2017%2FJava-the-relationship-between-class-and-object%2F</url>
    <content type="text"><![CDATA[类与对象的关系 构造器 创建对象 变量 静态方法 1. 类与对象的关系在 Java 中，程序的最小单位是类（class）。 类是对象的抽象，对象是类的具体。 类是一般化的对象，对象是特殊化的类。 比如说犬是一个类，犬有许多不同的品种，有哈士奇、金毛、田园犬，它们就是一个个的对象。 12345678// Java 定义类的语法格式// 修饰符可以是 public、final、abstract修饰符 class 类名&#123; 构造器 成员变量 方法&#125; 2. 构造器构造器是一个特殊的方法，构造器的作用是用来产生对象的。在创建一个对象的时候，至少要调用一个构造器。构造器的名称必须与类同名，一个类可以有多个构造器。 123456// 定义构造器，构造器名必须与类名相同// 形参：String name public puppy(String name)&#123; System.out.println("Input name is:" + name);&#125; 3. 创建对象123456// 创建具体的对象public static void main(String[] args)&#123; // 调用 puppy 构造器，返回一个实例 puppy mypuppy = new puppy("Sweet");&#125; 4. 变量变量必须有类型和名称，比如 int count，Java 中变量的命名规范，也可以说是标识符（变量、方法、类名）： 字母、下划线或 $ 开头，不能以数字开头 除了首位，其他位置可以用数字 避免使用保留字 只能包含美元符号，不能使用 @、# 等其他特殊字符 当然在对变量进行命名的时候一定要考虑可读性，比如 LearnJava 一看就知道是什么意思，而 TodayJava 看起来就比较令人费解。 局部变量：在方法、构造方法或者语句块中定义的变量被称为局部变量。变量声明和初始化都是在方法中，方法结束后，变量就会自动销毁。局部变量的修饰符只能是 final，可以省略。使用局部变量之前，必须先用 new 关键字创建实例才可以使用。 成员变量：成员变量是定义在类中，方法体之外的变量。这种变量在创建对象的时候实例化。成员变量可以被类中方法、构造方法和特定类的语句块访问。 123// Java 定义成员变量的格式// 修饰符可以是 public protected private 修饰符 类型 成员变量名 [= 默认值] 12345678910public class puppy&#123; // 定义成员变量 String name; int puppyweight; // 方法 public void ... &#123; &#125;&#125; 类变量：也叫静态变量，类变量也声明在类中，方法体之外，但必须声明为static类型。 如果一个方法没有返回值，则必须使用 void 来声明没有返回值。 123456// 定义方法 1：设定体重// 方法没有返回值，则必须使用 void 来声明没有返回值public void setWeight(int weight)&#123; puppyweight = weight;&#125; 5. 静态方法12345// 定义方法的语法格式[修饰符] 方法返回值类型 方法名(形参列表)&#123;&#125; 通常把 static 修饰的成员变量和方法称为静态变量（类变量）和静态方法（类方法），而相对应的则是实例变量和实例方法。 静态方法只能访问静态成员，实例方法可以访问静态和实例成员。之所以不允许静态方法访问实例成员变量，是因为实例成员变量是属于某个对象的，而静态方法在执行时，并不一定存在对象。 Java 方法需要注意的几点 修饰符：public static 返回值类型：分为无返回值（void）和有返回值，可以返回 Java 的任意数据类型 形参列表：也被称为形式参数列表，简称形参，形参是局部变量。与形参列表对应的实参列表，调用方法时传入方法的实参 方法名命名规则：首个单词保持小写，其余单词首字母大写，一般是动词为主 方法不能嵌套 方法的调用方式：“类名.方法”。方法在调用时，如果方法在本类中，“类名.”可以省略掉，如果方法不在本类中，则不能省略 return 语句使用注意事项：一、必须返回与“返回值类型”相同类型的数据；二、return 一旦被执行，后面的语句全部都执行不到，也就是在同一个域中 return 后面永远不要编写代码 完整代码123456789101112131415161718192021222324252627282930313233343536public class puppy&#123; // 定义成员变量 String name; int puppyweight; // 定义构造器，构造器名必须与类名相同 public puppy(String name) &#123; System.out.println("Input name is:" + name); &#125; // 定义方法 1：设定体重 // 方法没有返回值，则必须使用 void 来声明没有返回值，否则必须含有有效的 return 语句 public void setWeight(int weight) &#123; puppyweight = weight; &#125; // 定义方法 2：获取体重 public int getWeight() &#123; System.out.println("Puppy's weight is:" + puppyweight); return puppyweight; &#125; // 创建具体的对象 public static void main(String[] args) &#123; // 调用 puppy 构造器，返回一个实例 puppy mypuppy = new puppy("Sweet"); // 调用对象的方法 1 和 2 mypuppy.setWeight(12); mypuppy.getWeight(); &#125;&#125; 推荐阅读 http://www.cnblogs.com/shenliang123/archive/2011/10/27/2226923.html]]></content>
      <categories>
        <category>coding</category>
        <category>Java</category>
      </categories>
      <tags>
        <tag>面向对象编程</tag>
        <tag>Java</tag>
        <tag>学习笔记</tag>
        <tag>静态方法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[评估机器学习模型：指标解释]]></title>
    <url>%2F2017%2Fevaluate-machine-learning-models%2F</url>
    <content type="text"><![CDATA[1. 机器学习模型开发和评估的流程统计学习方法的基本假设是同类数据具有一定的统计规律性，它的研究对象是数据，从数据出发，提取数据特征，抽象数据模型，发现数据中的知识，又回到数据的分析与预测中。统计学习方法（一般指的就是机器学习）的三个要素： 模型 策略 策略能够帮助我们选择最优模型，通过损失函数来度量模型的好坏 计算损失函数的期望是模型关于联合分布下平均意义上的损失，由于联合分布未知，期望损失是无法直接计算的 在大数定律下，当数据量趋近无穷大时，经验损失趋向于期望损失，所以，我们希望借助于用经验损失来拟合期望损失 经验损失的计算通常会遇到过拟合的问题 算法 如何保证找到全局最优解？ 统计学习方法的步骤： 得到一个有限的训练数据集合 确定包含所有可能的模型的假设空间，即学习模型的集合 确定模型选择的准则，即学习的策略 实现求解最优模型的算法，即学习的算法 通过学习方法选择最优模型 利用学习的最优模型对新数据进行预测或分析 2. 分类模型的评估指标2.1 准确率Acc = 正确分类样本数量 / 总样本数量2.2 混淆矩阵 2.3 ROC 曲线ROC 曲线是一种研究机器学习泛化性能的工具，它用到了两个重要的指标：真正例率（TPR）和假正例率（FPR）。在测试数据上，机器学习模型会得出一个预测值或概率值，然后我们用分类阈值（threshold）$x$ 与预测值或概率值进行比较，比如大于 $x$ 判为正例，小于 $x$ 判为负例。 TPR = TP / (TP + FN)FPR = FP / (FP + TN)当分类阈值最大时，没有一个点划为正例，此时 TP 和 FP 都等于零，真正例率和假正例率都为零，对应 ROC 曲线的 A 点；最理想的情况是 B 点，此时的真正例率为 1 而假正例率为 0；分类阈值最小时，所有的样本都划为正例，故 TN 和 FN 都都等于零，则真正例率和假正例率都为 1。 AUC（area under curve），即曲线下的面积，AUC 是度量 ROC 曲线的一种方法，好的模型 ROC 曲线下的面积会很大，因此 AUC 越大越好。 2.4 查准率和召回率疾病诊断为例，查准率指的是诊断结果中有多少是真正患病的，召回率指的是有多少真正患病者被诊断出来了，F-1 值则是查准率和召回率的调和平均值。查准率和召回率是评估分类模型中最常见的一组指标，在工程实践中通常用不同阈值与模型得出的预测值或概率值进行比较，然后比较这几组查准率和召回率的优劣。 Precision = TP / (TP + FP)Recall = TP / (TP + FN)F1-score = \frac{2 * Precision * Recall}{Precision + Recall} 3. 回归模型的评估指标3.1 均方差回归模型中用得最多的度量指标是均方差（mean square error） \frac{1}{n}\sum{(y_i - \hat{y}_i)}^23.2 Root Mean Squared Log Error\text{RMSLE} = \sqrt{ \frac{1}{n} \sum^{n}_{j=1}{( \log(y_j + 1) - \log( \hat{y}_j + 1 ) )^{2}}}3.3 灵敏度和特异度灵敏度（Sensitivity），跟真正例率的定义一样，指真正例中被模型发现的比率，所以灵敏度又可以称为召回率。 特异度（specificity），指真负例中被模型发现的比率，也可以看做是负样本的召回率。 3.4 R-Squared在线性回归以及广义线性回归中，R-squared 误差的大小意味着模型的拟合度的好坏。R-squared 误差取值范围为 0 到 1，这个值越接近 1 说明模型的拟合度越好。 TSS：Total Square Sum / 总离差平方和 RSS：Residual Square Sum / 残差平方和 ESS：Explain Square Sum / 解释平方和 R-Squared = 1 - RSS / TSSR-Squared = 1 - \frac{\sum(y_i-\hat{y_i})^2}{\sum{(y_i-\bar{y_i})}^2}推荐阅读 https://zh.wikipedia.org/wiki/ROC曲线 https://github.com/yanyachen/MLmetrics https://en.wikipedia.org/wiki/Sensitivity_and_specificity em.hzu.edu.cn/uploadfile/20051017163341988.ppt http://www.investopedia.com/terms/r/r-squared.asp]]></content>
      <categories>
        <category>Machine-Learning</category>
      </categories>
      <tags>
        <tag>评估模型</tag>
        <tag>ROC曲线</tag>
        <tag>准确率</tag>
        <tag>召回率</tag>
        <tag>统计学习方法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java 错误 编码GBK的不可映射字符]]></title>
    <url>%2F2017%2FJava-GBK-error%2F</url>
    <content type="text"><![CDATA[1. 错误: 编码GBK的不可映射字符当 Java 代码或注释中出现中文，如果在编译时不添加编码方式，会出现错误: 编码GBK的不可映射字符的报错信息，加上以下条件便可解决： 1javac -encoding utf-8 ****.java 2. parseInt() 函数parseInt() 函数可以解析一个字符串，并返回一个整数 3. print() 与 println() 的区别这两种输出方法的区别在于 println() 除了输出要求输出的内容外，还会在后面加上一个回车符；而 print() 只是输出要求输出的内容，后面的内容紧接着前面的输出。 4. Java 写的三个小程序 打印九九乘法表 模拟扔硬币 模拟掷骰子 123456789101112131415161718192021222324252627/*打印九九乘法表*/public class jiujiu&#123; public static void main(String[] args) &#123; for (int i = 1; i &lt; 10; i++) &#123; for (int j = 1; j &lt;= i; j++) &#123; if (i != j) &#123; int k = i * j; System.out.print(i + "*" + j + "=" + k + "," + "\t"); &#125; // 最后一项后面没有逗号 else &#123; int k = i * j; System.out.print(i + "*" + j + "=" + k + "\t"); &#125; &#125; System.out.println(); &#125; &#125;&#125; 12345678910111213141516171819202122232425/*扔硬币的小程序*/import java.util.Random;public class Flips&#123; public static void main(String[] args) &#123; int heads = 0; int tails = 0; int T = Integer.parseInt(args[0]); for (int i = 0; i &lt; T; i++) &#123; double p = Math.random(); if (p &lt;= 0.5) &#123; heads++; &#125; else &#123; tails++; &#125; System.out.println("正面：" + heads + "反面：" + tails); &#125; &#125;&#125; 123456789101112131415/*模拟掷骰子*/import java.util.Random ;public class RollDice&#123; public static void main(String[] args) &#123; int T = Integer.parseInt(args[0]); for (int i = 0; i &lt; T; i++) &#123;//生成 0 到 1 之间的随机数并强制转换为 int 型 int s = (int)(Math.random() * 6) + 1; System.out.println(s); &#125; &#125;&#125;]]></content>
      <categories>
        <category>coding</category>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>GBK不可映射</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java 开发环境配置]]></title>
    <url>%2F2017%2Fjava-set-develop-environment%2F</url>
    <content type="text"><![CDATA[Java 语言最初是由 Sun 公司团队发明的一种面向对象的编程语言，Java 最开始不叫 Java，而是 oka（橡树）。相对于 C++，Java 比较简单，更加容易理解，Java 可以在不同的平台执行，比如同一个 Java 程序既可以在 Windows 系统下运行，又可以在 Linux 下执行。因为不同的操作系统内核不同，所以 Java 不能与操作系统直接接触，所以开发者发明了虚拟机 JVM 的办法来解决这个问题。不同的操作系统下有不同版本的 JVM。 1. 下载 jdkjdk 下载链接http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html 下载之后按照系统提示安装，当然也可以自定义安装路径，但是建议路径中不要包含空格。 2. 设置环境变量环境变量分为用户变量和系统变量，用户变量只在当前用户下生效，系统变量则对全局用户生效。建议添加用户变量，在 Path 环境变量中添加 jdk 的安装路径 E:\java\jdk1.8.0_131\bin。 完成上述步骤后，在 cmd 命令行窗口运行 java 和 javac，如果没有返回异常则安装成功。 3. 运行第一个 Java 程序在编辑器中键入一下代码，并用 javac HelloWorld.java 编译，编译完成后此目录下会多出一个 HelloWorld.class 的类文件，这是编译的结果。再运行 java HelloWorld，即可打印出程序运行的结果。 1234567public class HelloWorld&#123; public static void main(String[] args) &#123; System.out.println("Hello, World!"); &#125;&#125; 4. Java 的工作方式那么运行 Java 代码时，计算机里面发生了什么事情呢？]]></content>
      <categories>
        <category>coding</category>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Hello World</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[北京，后会有期]]></title>
    <url>%2F2017%2F2017-09-08-beijing-see-you-again%2F</url>
    <content type="text"><![CDATA[这一年过得很快，经历了许多人许多事。每人都得给自己一个交待，学会向前看，学会告别 一、一旦心里有了答案，路便就在脚下这是我的第一份工作，而在这之前是没有任何的经验，所以我一直保持着职场菜鸟和学生的心态，毕竟这样的身份能够给自己带来不少的安全感。校园，能够为你遮风避雨的温室，可是待久了你便会觉得无所适从，总会质疑自己能不能承受突然暴晒的阳光。我害怕重蹈覆辙，不想重复几年前的“悲剧”，我知道，我必须得出发了。 首先，找实习的过程确确实实费了不少周折。渐渐明白，关键还是在于自己想不想做。心里一旦有了答案，路便已经在脚下铺好。 没有找工作的经验，其实在去年我就找了一个模板，把并不丰富的经历逐字逐句码了一个简历，修改得还算漂漂亮亮。简历做好了但始终没有投出去的勇气，四月份办完了家里的一点事回到学校，下载了几个找实习找工作的 APP，每天看看 APP 的职位更新并投投简历，大半个月的时间过去了，除了几个回绝并没有一点回应。以至于我开始怀疑自己是不是选择错了或者是根本不能胜任我所期待的工作，后来才明白，机会怎么会往你头上砸！？得自己去争取啊！慢慢我开始停掉手头的工作。 整个 5 月我都在找实习的 offer，修改简历，给 HR 写求职信，给 leader 写邮件，找校友内推……反正能用的手段都用上了。从一开始石沉大海到慢慢有公司给我安排视频、电话面试的机会，再到后来给我 offer。就这么找了差不多一个月，北京的两家公司给了我 offer，幸好都是自己理想的岗位。也许是缘分吧，最终选择了值得买，也就是传说中的色魔张大妈（广告：欢迎加入张大妈，超好的工作环境和福利，no official politics，给你最大的成长空间）。对于职业，我笃信，做自己喜欢做的事情才是最重要的，毕竟做人嘛，开心最重要。 第一次来北京，在北京也极少有同学或熟人，对北京的印象还停留在影视或文学作品的层面，比如《阳光灿烂的日子》、《蜗居》、《北京爱情故事》，又或者是《血色浪漫》类似的小说。想象中，北京是逐梦者的天堂或者地狱，有人过得很纸醉金迷，有人为了生存难以为继；又或者大部分的北漂者都只能屈居于地下室。Z162 快到北京西站的时候，透过火车车窗，蓝天白云，没有想象中的那么堵，河里的水干净得有些不真实，说实话我一直怀疑这是不是抽的地下水。 二、北京，没有想象中那么差 在我的印象中，naive 地认为北京人都瞧不起外地人。下了火车站，也不东张西望跟着人群进了出站口，故作冷静。正好遇到早高峰，地铁进站口黑压压的上班族涌过来竟然有点窒息感。乱七八糟的的指示牌指着各个方向，操着各色口音的人问你去不去长城、十三陵……这跟南方完全不同，你让我如何分辨？我也不太敢问，生怕浓厚的湖南口音会暴露自己的地域。但后来似乎是我想多的，其实北京人民还是挺热心的，一位志愿者告诉我从哪里进站，买了票然后上了地铁，从九号线换到十号线，在角门东地铁站下了地铁。太阳挺大，但一点都不热，丝毫没有南方阳光带来的灼伤感，即使暴露在阳光下也很难出汗，这是我对北京的第一印象！空气十分干燥，以至于刚来的前两个星期经常流鼻血。 三、北京，没有想象中那么好一直以来我都不太喜欢麻烦别人，但是初入异地，没有当地好朋友的帮助将寸步难行。来北京最头疼的莫过于解决租房问题，北京的黑中介和高房价早有耳闻，防不胜防。幸好有小学同学雄哥热情的帮助，辛苦他背着老板一大早坐两个多小时从昌平赶到南三环，用工科生严谨的态度帮我计算比较了几个不同的租房方案。能够把别人的事情当做自己的事情，而且负责任地做好，这让我非常诧异感动。由于相距太远，比较遗憾的是一直没机会请他吃顿饭。 刚来北京也没闲着，北京是一座古城，坐地铁、骑着摩拜和小蓝单车也逛了不少地方：故宫、天坛、颐和园、国博、圆明园、国贸、鸟巢、水立方、长安街、簋街……。北京没有想象中那么好，我原本以为应该是遍地高楼大厦，到这才发现这里的建筑除了国贸一带，一般都不会超过四五十米，而且很老很旧。有的地方卫生状况堪忧、马路特别宽、长安街上可以踢足球…… 四、职场初体验作为一个非 CS 科班出生的菜鸟初次步入互联网的行业，而且还是从事技术开发（非常感谢 leader 对我信任与宽容），缺乏信心与经验是让我没有底气的罪魁祸首。还记得第一天来公司报到，填完资料后 HR 便带我见部门 leader，略微生涩，记得刚见易总是叫的“易老师好！”，挺尴尬的。然后易总领着我跟事业部的同事都认识了一遍（人太多了），领设备器材，调试机器，配置工作环境。忙完了上午一轮，本来有点紧张已经有点累了，没休息马上就到下午的工作时间，完成了一些杂七杂八的事情后坐在工位也不敢乱动。平时在学校一般最晚六点吃饭，工作了便不一样，七八点多大家还在上班，这让学生党如何受得了！ 上班之后才发现校园与工作完全是两回事，工作中要求需求能够快速上线实现，并且有各种指标来度量你的工作效果，项目可能需要经过时长一年无数次修修补补的迭代，熟悉业务逻辑、洞察数据的能力往往决定了问题的解决速度。小组的另外一位开发，也就是我的师傅靖哥，他负责算法调优和模型开发，也是一位健身狂人。靖哥对我的帮助很多，帮我“解救”不少调不通的代码，给予了我这个啥也不懂的菜鸟最大的理解。我跟靖哥一样，都对技术和机器学习比较感兴趣，喜欢研究算法背后的原理，所以经常会互相分享一些比较有意思的 paper 和博客资源，并且会琢磨这怎么去实现。对于工作中的错误与问题，“没事，大家都这么过来的”。 在值得买实习期间每天晚上回去的比较晚，一般十点多以后，十一点多也经常是家常便饭。附近没有找到合适的自习室，所以周六周天一般也会来公司敲代码、看书、研究 paper、写笔记，后来小组的 PM 媛姐开玩笑说我都上了公司“勤奋榜” top 3 了。其实，大家工作都很努力负责，像志伟、“接口王”还有强哥，即使是自己的工作干完了都还要抽出时间来学习新的知识技能，都想着怎么提升自己的技术能力。 实习的时间不算太长，3 个月多一点时间一划而过，短短的时间内有幸参与了小组的两个项目。第一个项目让我明白熟悉业务逻辑的重要性，对数据不了解、思路不清晰是不可能把模型做好的。第二个文本分类的项目中大部分时间都是一个人在做，之前并没有完完整整独自实现一个方案的经验，压力也挺大的，记得有一段时间晚上做梦迷迷糊糊都在脑子里写着代码。我怕自己解决不了问题，第二天的站会没有什么东西可讲，不过确实提升了个人独立解决问题的能力。还好还出什么大乱子，非常感谢易总给了我这么多试错的机会！ 几个月时间里，除了工作能力得到提升以及对自己的职业角色定位有了更加清晰的认识，最令人欣慰的是坚定了自己的兴趣爱好。每当有同事问我为什么选择做这个行业，我总能发自内心地回答：“主要是自己感兴趣”、“这个东西挺有意思的”。另外，我还发现自己迷上了写作和阅读，周六周天要是不想出去走走，捧个 kindle 看看自己想看的书或者是去公司写博客一样也能够让我感受到充实与满足。生活啊，就是一个不断发现自我的过程。 在公司最后一个星期的时间主要就是准备工作的交接，梳理项目，写文档和代码注释，整理数据。完事发现这段时间还是做了不少的工作。PM 媛姐让走之前我在事业部的例行分享会上做项目的 presentation，并十分贴心给我写了大纲。由于 BI 团队的技术和 PM 都会参加，她特意嘱咐我要把 presentation 做得有深度一点。这让我这个初出茅庐的菜鸟倍感压力，花了不少时间整理做过的实验和代码，把之前做过的机器学习笔记复习了一遍，并用 LaTeX 认认真真写了一个逻辑和样式都还不错的 beamer，也算是给自己的一个交待吧。例行分享的氛围挺不错，进展还算顺利，不管是技术还是产品都挺感兴趣的，很开心分享完了之后得到了不少同事的肯定。记得考研独自二战那会，听冬吴相对论比较多，其中有一次关于正心诚意的主题节目给我的印象特别深刻，你若用超乎敬意的诚意去对待一件事情，something will happen，这世界就怕认真二字。 五、Make things happen在快两个月的时间里尝试了几个不同的方案却没有一个让人完全满意的答案，因为一个项目的上线得需要经过层层评估还得依靠技术主动去推动，贸然只会造成资源的浪费，这也让我明白了能够把方案“做出来”和“做好”是两码事。放心吧，路还长着呢。 后记有幸在千里之外结识了你们这帮朋友，我会记得那些赤脚骑着共享单车在长安街狂飙的日子，我会记得大半夜喝完酒回来有人会为你开门，我会记得周五晚上和靖哥 debug 到 11 点的时光，以及每天晚上十点半走出公司后的那口干爽粗糙的空气。 再见了，北京。 再见了，一去不返的时光。]]></content>
      <categories>
        <category>思想王国</category>
      </categories>
      <tags>
        <tag>实习</tag>
        <tag>工作总结</tag>
        <tag>北京</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux 使用笔记（1）]]></title>
    <url>%2F2017%2F2017-08-18-linux-operations-notes%2F</url>
    <content type="text"><![CDATA[1. scp 跨机远程拷贝将 10-9-185-82 的机器/titles_classification/libin目录上所有文件拷贝至10.9.90.211机器/root/titles_classification目录下 1root@10-9-185-82:~/titles_classification/libin# scp ./* root@10.9.90.211:/root/titles_classification 2. 清屏命令在windows 的 DOS 操作界面里面，清屏的命令是 cls， clear 这个命令将会刷新屏幕，本质上只是让终端显示页向后翻了一页，如果向上滚动屏幕还可以看到之前的操作信息。一般都会用这个命令。 reset 这个命令将完全刷新终端屏幕，之前的终端输入操作信息将都会被清空，这样虽然比较清爽，但整个命令过程速度有点慢，使用较少。 另外介绍一个用别名来使用清屏命令的方法，如下： 12[root@localhost ~]$ alias cls=&apos;clear&apos;[root@localhost ~]$ cls 执行以上命令后，以后你就可以直接输入 cls 命令来实现和 clear 一样的清屏命令了。 3. 目录切换 找到文件/目录位置：cd 切换到上一个工作目录： cd - 切换到home目录： cd or cd ~ 显示当前路径: pwd 更改当前工作路径为path: $cd path 4. 拷贝文件将/root/titles_classification/下所有文件拷贝至/root/machinelearning/libin目录下 1[root@10-9-90-211 machinelearning]# cp /root/titles_classification/* /root/machinelearning/libin 5. 删除文件删除文件，系统会事先询问是否删除 1rm filename 强行删除文件，这时系统不会事先询问是否删除 1rm -rf filename 6. 杀掉 root 用户下所有 Python 进程1ps -ef |grep python |awk &apos;&#123;print $2&#125;&apos;|xargs kill -9 7. 多重视窗管理程序12345678/*创建名为 screen_name 的屏幕*/screen -S screenname/*回到 screenname 屏幕*/screen -r screen_name/*将指定的 screen 作业离线*/screen -d screen_name/*Kill detached session*/screen -S some_name -X quit 会话共享 还有一种比较好玩的会话恢复，可以实现会话共享。假设你在和朋友在不同地点以相同用户登录一台机器，然后你创建一个 screen 会话，你朋友可以在他的终端上命令： 1[root@ml ~]# screen -x screen_name 这个命令会将你朋友的终端 Attach 到你的 Screen 会话上，并且你的终端不会被 Detach。这样你就可以和朋友共享同一个会话了，如果你们当前又处于同一个窗口，那就相当于坐在同一个显示器前面，你的操作会同步演示给你朋友，你朋友的操作也会同步演示给你。当然，如果你们切换到这个会话的不同窗口中去，那还是可以分别进行不同的操作的。]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>笔记</tag>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习算法系列（13）理解卷积神经网络]]></title>
    <url>%2F2017%2F2017-08-13-machine-learning-algorithm-series-understanding-cnn%2F</url>
    <content type="text"><![CDATA[写在前面没想到博客能存活到现在，全凭着一股兴趣，不知不觉这个系列已经写到第 13 篇了。一直以来，我都是抱着初学者的心态来写机器学习算法，限于专业以及其他因素，圈子内有同样兴趣的伙伴少之又少，所以对机器学习的理解大多来自独自阅读论文和观看教学视频，个人的理解难免出现低级错误。再加上本人又非科班出身，遇到难以理解的地方往往“求告无门”，只能翻来覆去的啃，铺天盖地地搜，难免陷入主观的境地。 本文主要写的是笔者对 CNN 原理的一些个人理解，在 CNN 如何进行学习以及 dropout 没有做太多涉及，笔者接触卷积神经网络（Convolutional Neural Network，以下称 CNN）的时间不长，不论是看的论文还是做的 project 都远远不够，出现错误在所难免，理解上出现的偏差我是要负主要责任的，欢迎大家帮我找出文章中纰漏！ Regular Neural Nets don’t scale well to full images. 在之前的一篇文章中：机器学习算法系列（8）神经网络与 BP 算法，我们了解到对于一般神经网络，它们的基本组成结构分为输入层、隐藏层和输出层三个部分，隐藏层分布的各个神经元彼此独立，他们各自从输入层接受输入并进行计算，汇总之后再传送至输出层；还有反向传导算法的实现过程。 理论上，参数越多的模型复杂度越高，同时也能够完成更加复杂的学习任务，但是与此同时参数过多造成许多不足。通常，一般神经网络会面临参数剧增的问题，比如神经网络的输入为一个 [255, 255, 3] 的矩阵，完全展开后即为 172,125 维的向量，假设隐藏层有 1,000 个神经元，那么仅仅从输入层到第一层隐藏层参数的个数就达到了 255×255×3×1000=172,125,000 个！是不是有点恐怖？如果再向神经网络添加几个隐藏层，参数数量上升的速度会非常快，从而在计算上将变得十分耗时（近年来深度学习大热，与计算机计算能力大幅提升的背景离不开），并且参数数量过多还容易导致模型的过拟合。而 CNN 通过参数共享（shared weights）、下采样（sub-sampling）等手段能够较好地克服了一般神经网络中参数剧增的缺陷。 1. CNN 结构的解释1.1 卷积、池化、全连接在理解 CNN 的结构之前，有必要了解几个基本的概念：卷积（Convolution）、池化（Pooling）和全连接（Fully-Connected），将这三种不同的层按照一定的结构连接起来便组成了卷积神经网络的基本架构。 卷积 “自然图像有其固有特性，也就是说，图像的一部分的统计特性与其他部分是一样的”，也就是说图像或者是向量化后的文字同样具备一定的统计特性，即同样也可以进行特征提取、组合。在 CNN 中，卷积运算便是这么一个过程，不同于拥有显示特征的数据，CNN 对数据的特征提取是通过 filter 来进行的，每个 filter 都有着采集不同的特征的任务，因此 filter 中参数的大小是不相同的。 以识别图像中的小鸟为例，有的 filter 可能专门找像鸟嘴的部分，有的则专门找寻找像鸟的尾巴的部分，并且 feature map 会共享（sharing）来自 filter 的参数。 池化 池化的目的是为了减轻计算量的压力，池化还有一个作用，它能够使模型具备识别平移、转换、缩放图片的功能@杨培文。比如对于一个 32×32 的图像，如果经过 100 个 5×5 filter 卷积提取得到 100 个（32-5+1）×（32-5+1）=784维的卷积特征，那么对于每个输入都有 78,400 维度的卷积特征向量，这个给计算带来了巨大的压力。 为了解决这个问题，池化的方法比较简单，可以理解它是一个采样的过程，对 filter 过滤出来的 feature map，我们可以将它分成几个小块，然后对每个小块采取取最大（max pooling）或取平均（average pooling）的操作，从而达到降维的目的。 然后 CNN 反复重复 Conv-&gt;Pooling 的这个过程，最后展开（flatten）连接到全连接层。这时参数个数便能够下降到可以接受的范围之内。 1.2 卷积层、池化层大小的计算无论是卷积到池化还是池化到卷积，两个过程的参数个数计算其实区别不大 输入：$W_1, H_1, D_1$ 四个超参数（hyper parameters） 滤波器（filter）的个数： $K$ 滤波器的大小： $F$ 步长（stride）： $S$ 零填充（zero padding）的个数： $P$ 输出： $W_2, H_2, D_2$ $W_2 = (W_1 - F + 2P)/S + 1 $ $H_2 = (H_1 - F + 2P)/S + 1$ $D_2 = K$ 如果输入层有$n​$个 channel，滤波器（filter）会考虑到 channel 的深度，那么滤波器的结构一般也是（$··n​$）的结构；filter 的个数决定了卷积层的深度$D​$和 feature map 的个数 如下图，如果输入的图像有 RGB 3 个 channel，相应 filter 的结构也为 $··3$ 零填充（zero padding）个数有助于适应 filter 和控制输出单元的大小 1.3 例子举个例子，赢得 2012 年 ImageNet 的 Krizhevsky et al. CNN 架构中接受的是 [227×227×3] 大小的图片（原文中是说224×224×3，但这样计算得到的卷积层大小不是一个整数，应该是作者使用了零填充但是却忘了在文中提及），一共有 (K = 96) 个大小为 [11×11×3] 的滤波器，F = 11，步长 S = 4， 那么卷积层计算 $(227 - 11+0)/4 + 1 = 55$，卷积层的大小便为 [55×55×96]。 2. 经典的 LeNet-5若要提起 CNN，就不得不提起 Yann LeCun 用 CNN 进行手写数字识别任务的论文，LeCun 这篇经典论文的插图在各种文章中被引用了无数遍，相信你也不是第一次在这里看到这幅图片 如上图所示，不包括输入层（Input layer），LeNet-5 网络一共有 7 层，分别是 3 个 卷积层（Convolution layer），2 个采样层（Subsampling layer），1 个全连接层（Fully-Connect layer）和 1 个输出层（Output layer）。 LeNet-5 的输入层是 32×32 的手写数字图像的像素矩阵，输出层是长度为 10 的向量，分别对应 10 个数字相应的概率。$Cx$ 表示卷积层，$Sx$ 表示采样层，$Fx$ 表示全连接层（$x$ 表示层的索引）。Lenet-5 经过两轮的卷积（Convolution）、采样（Subsampling）重复操作，32×32 的输入被整合为 16 个 10×10 的矩阵，最后经过两个全连接层到达输出层。 C1 C1 卷积层有 6 个 feature map，这 6 个 feature map 分别由 6 个 5×5 的 filter 过滤得来。算上偏置项（bias），从输入层到卷积层中间有（5×5+1）×6=156 个可以训练的参数（trainable parameter）（对比一般神经网络，参数数量根本不是一个数量级）和 （28×28×6）×（5×5+1）=122,304 个连接。 S2-&gt;C3 S2 采样层共有 12 个参数和 5880 个连接。 C3 卷积层有 16 个 feature map，上面讲过有多少个 filter 就有多少个 feature map。显然，S2 采样层到 C3 卷积层之间有 16 个 filter，与 C1 卷积层一样，每个 filter 的大小为 5×5。然而，C3 卷积层的参数个数计算稍微有点绕，LeCun 在原文中的解释如下图： 原文大意是：前 6 个 feature map 从采样层相邻的 3 个子集获得输入，接下来的 6 个 feature map 又从采样层相邻的 4 个子集获得输入，剩余的 feature map 也依照这种办法生成。为什么要采用这种方法来生成 feature map 呢？LeCun 解释有两个方面的原因，首先这种不完全的连接（non-complete connection）能够保证连接数能被控制在一定的范围之内，再者便是它能够强行打破网络的对称性（symmetry in the network），不同的 feature map 能较好地提取不同的特征。 通过下面这幅图以前 6 个 feature map 从采样层相邻的 3 个子集获得输入为例，只不过这里单个生成 feature map 的 filter 是由三个 5×5 的矩阵联合组成的。 从 S2 到 C3 总共有 1,516(456+606+303+151) 个可训练的参数，每个 filter 连接的都是 10×10 的 feature map，那么全连接数即为 151,600 个。 (5*5*3+1)*6=456\\ (5*5*4+1)*6=606\\ (5*5*4+1)*3=303\\ (5*5*6+1)*1=151 S4-&gt;C5 S4 是有 16 个 5×5 大小feature map 组成的采样层，由上面的算法，一共有 32 个可训练的参数和 2,000 个连接。 C5 卷积层拥有 120 个 feature map，因为 C5 卷积层的 feature map 是 1×1 的，所以 S4 与 C5 之间的连接是全连接，将 S4 铺平展开为一个 400（5×5×16）×1 的矩阵，则可训练参数的个数为 400×120+120=48,120 个。 终于到 F6 全连接层了，全连接层有 84 个单元，与 C5 卷积层 fully connected 之后，它们之间一共有 10,164 个参数。 以手写数字识别为例，CNN 的输出有十个单元，对应 F6 ，不难得出全连接层到输出层有840个连接，840个可训练的参数。 以上就是 LeNet-5 的卷积神经网络的完整结构解释，网络中共计 60,840 个训练参数，340,908 个连接，参数和连接数统计表格总结如下： 卷积-采样 训练参数 连接数 input-&gt;C1 156 1,122,304 C1-&gt;S2 12 5,880 S2-&gt;C3 1,516 151,600 C3-&gt;S4 32 2,000 S4-&gt;C5 48,120 48,120 C5-&gt;F6 10,164 10,164 F6-&gt;output 840 840 总计 60,840 340,908 3. CNN 是如何学习的？在接触 CNN 的过程中，似乎感觉其解释性不够强，给人一种“黑盒”的印象。简单讲讲 CNN 是如何进行学习的？以数字识别为例，专门寻找数字 8 弯弯曲曲特征的 filter 过滤生成了 feature map，把 feature map 的每一个元素相加得到 $a^k$，表示 filter 被刺激的程度，CNN 的任务便是寻找一张能够使 $a^k$ 最大的 image x。 a^k = \sum_{i=1}^{n}\sum_{i=1}^{n}a_{ij}^kx^*=arg\ maxa^x(gradient\ ascent)4. CNN 在文本分类中的表现CNN 非常强大，在文本、图像分类上尤为突出，即使没有做任何数据预处理的操作，简单的模型也能得到很好的效果。 …..Despite little tuning of hyperparameters, this simple model achieves excellent results on multiple benchmarks…… 简单说一下 Kim, Y. (2014). 的这篇文章，作者在实验时直接用了 Mikolov 从 Google News 1000 亿个单词中预训练好的词向量，通过 CNN 来做句子层面的分类任务，并得到了非常理想的结果。思路如下：假设词向量 $x_i$ 为 $k$ 维向量，（k 等于句子的最大长度，这样才能保证输入的矩阵维度一致），$x_i$ 对应着句子中的第 $i$ 个词，如此一来，一个长度为 $n$ 的句子便可以表示为 $n×k$ 的矩阵，这么一来句子便可以进行向量计算了！（不得不佩服 Mikolov，他是怎么想到 word2vec 这个办法的） 接下来的步骤便是依葫芦画瓢了，卷积提取特征，从 feature map 中选择一个最大值，进入全连接层，最后得到输出。实验结果如下，CNN 在 7 个语料库上有 4 个的准确率超过了传统方法。 5. Summary如何理解 CNN？从人的经验角度来看，正如我们人识别动物，人们不会把动物仔仔细细看个遍才做出判断，而是看到小鸟有尖尖的嘴巴、羽毛、流线型的身体等等几个特征就能够分辨是不是小鸟。卷积神经网络的工作原理也类似，神经元不会像一般神经网络一样去傻傻地去遍历整张图片来提取特征，而是只需要看一小部分就行了，每个神经元都有自己独特的任务，如下图所示，神经元主要的任务是识别小鸟的嘴巴。 有人可能会问，如果小鸟的嘴巴没有在图片中央而是在右上角，是不是又需要一个专门识别右上角小鸟嘴巴的神经元呢？答案是否，相同特征可能会出现在图像中不同位置，在 CNN 中，这些发现相同特征的神经元彼此之间是共享参数的，实际上它们做的是同一件事情。 随着 CNN 层数的叠加，越往后面提取的特征越发抽象难以理解，还有如果特征太多了怎么办？那我们就取一小部分就好了。 池化的直观理解：去除图片的一部分有时候并不会影响对整个图片的理解。 在 CNN 中防止模型出现过拟合一般有 data augmentation 和 dropout 两种手段，前者是人工地增加数据量，后者是随机地让某些神经元失效，以达到减少参数量的效果。 事实上，CNN 与深度学习、AI 总是形影不离，因此在我的印象里 CNN 一定是那种非常复杂高深的机器学习方法；另外，笔者第一次接触 CNN 是在学习 fast.ai（只知道讲课的老师非常认真，现在不知道怎么样了）提供的课程，当时因为文档不完善再加上自己又是一个不折不扣菜鸟，光配置 aws 就吃了不少苦头，第一节“猫狗大战”的任务都没有完成就放弃了，于是便形成了认为 CNN 一定非常难以理解的感觉，甚至于有点恐惧。但实际上，“柳暗花明又一村”，CNN 与普通全连接的神经网络区别不大，唯一的区别是 CNN 仅仅只连接到了输入的局部，并且众多的神经元彼此之间共享参数。 PS：如果有时间，接下来我会写一写如何用 CNN 实现文本分类的文章。 推荐阅读 http://cs231n.github.io/convolutional-networks/ http://scs.ryerson.ca/~aharley/vis/conv/ http://speech.ee.ntu.edu.tw/~tlkagk/courses/ML_2016/Lecture/CNN%20(v2).pdf.pdf) http://speech.ee.ntu.edu.tw/~tlkagk/courses_ML16.html http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/ http://www.aclweb.org/anthology/D14-1181 https://adeshpande3.github.io/adeshpande3.github.io/A-Beginner%27s-Guide-To-Understanding-Convolutional-Neural-Networks/ https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf ufldl:卷积特征提取 CNN资源合集：来自@爱可可老师的微博]]></content>
      <categories>
        <category>Machine-Learning</category>
      </categories>
      <tags>
        <tag>机器学习算法系列</tag>
        <tag>Machine-Learning</tag>
        <tag>深度学习</tag>
        <tag>卷积神经网络</tag>
        <tag>CNN</tag>
        <tag>卷积</tag>
        <tag>池化</tag>
        <tag>pooling</tag>
        <tag>convolution</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[思想王国 | 一九八四]]></title>
    <url>%2F2017%2FGeorge-Orwell-Nineteen-Eighty-Four%2F</url>
    <content type="text"><![CDATA[]]></content>
      <categories>
        <category>思想王国</category>
      </categories>
      <tags>
        <tag>一九八四</tag>
        <tag>George Orwell</tag>
        <tag>1984</tag>
        <tag>乔治奥威尔</tag>
        <tag>英国</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python | 乘上 Time Machine]]></title>
    <url>%2F2017%2F2017-07-28-Python-time-machine%2F</url>
    <content type="text"><![CDATA[本篇文章主要介绍在使用 Python 过程中涉及到“时间”的一些处理笔记，包括获取格式化后的当前时间、实时观察任务完成的进度等等。 一、获取当前系统时间 第一种：导入 datetime 模块 123import datetime&gt;&gt;&gt; datetime.datetime.now()datetime.datetime(2017, 7, 5, 13, 40, 18, 759000) 当然这种获取办法得到的时间格式往往不符合我们的阅读习惯，可以通过strftime()函数来格式化当前时间，Python 中时间日期格式化符号有 1234567891011121314151617181920212223%y 两位数的年份表示（00-99）%Y 四位数的年份表示（000-9999）%m 月份（01-12）%d 月内中的一天（0-31）%H 24小时制小时数（0-23）%I 12小时制小时数（01-12） %M 分钟数（00=59）%S 秒（00-59）%a 本地简化星期名称%A 本地完整星期名称%b 本地简化的月份名称%B 本地完整的月份名称%c 本地相应的日期表示和时间表示%j 年内的一天（001-366）%p 本地A.M.或P.M.的等价符%U 一年中的星期数（00-53）星期天为星期的开始%w 星期（0-6），星期天为星期的开始%W 一年中的星期数（00-53）星期一为星期的开始%x 本地相应的日期表示%X 本地相应的时间表示%Z 当前时区的名称%% %号本身 比如，格式化当前时间为年-月-日 12&gt;&gt;&gt; datetime.now().strftime("%Y %B %d")'2017 July 28' 如果想比较完整的本地日期和时间表示 12&gt;&gt;&gt; datetime.now().strftime("%c")'Fri Jul 28 18:59:02 2017' 第二种：导入 time 模块 time当前时间的 Unix 时间戳（注：Unix 时间戳是从1970年1月1日（UTC/GMT的午夜）开始所经过的秒数） 123456import time&gt;&gt;&gt; time.time()1501238427.6173134# 将时间戳精确到秒&gt;&gt;&gt; int(time.time())1501238482 你也可以将 Unix 时间戳格式化为易读的格式 123import time&gt;&gt;&gt; time.strftime("%Y:%m:%d %H:%M:%S")'2017:07:05 13:40:04' 获得当前时间前后某一时刻，比如在执行实时任务中，有时需要规定在每秒时间间隔内抓取一次数据，可以通过timedelta来实现。下面的例子中，end_time比begin_time是要大一秒的 12345begin_time = datetime.datetime.now()end_time = begin_time + datetime.timedelta(seconds=1)# 格式化当前时间begin_time = begin_time.strftime("%Y-%m-%d %H:%M:%S")end_time = end_time.strftime("%Y-%m-%d %H:%M:%S") 二、延迟进程调用time模块有一个推迟调用线程time.sleep()的功能特别实用，括号内的数字表示进程挂起的时间(秒)，比如 1234567#!/usr/bin/pythonimport timeprint "Start : %s" % time.ctime()time.sleep(5)print "End : %s" % time.ctime()Start : Wed Jul 05 14:02:11 2017End : Wed Jul 05 14:02:16 2017 三、计算程序运行时间12345678#!/usr/bin/python#coding: utf-8import timestart = time.clock()# 中间是你的程序elapsed = (time.clock() - start)print("Time used:",elapsed) 四、显示任务完成进度有的时候运行程序执行循环计算会等待较长的时间，这时如果有一个实时的可视化进度条那就再好不过了。在这里要介绍 Python 中两个非常有趣的库，分别是 tqdm 和 pyprind，它们可以帮助我们了解任务完成的状态。 tqdm 123from tqdm import tqdmfor i tqdm(range(10)): sleep(i) pyprind 然后是 pyprind，相比 tqdm，pyprind 允许自定义的空间就大了很多，进度可以有进度条和百分比两种形式，程序运行完成还可以得到 CPU 和内存使用情况报告等等。 12345&gt;&gt;&gt; for i in pyprind.prog_bar(range(10)): time.sleep(1)...`ETA`表示距离程序结束的时间0% [######## ] 100% | ETA: 00:00:02 1234&gt;&gt;&gt; for i in pyprind.prog_percent(range(10)): time.sleep(1)···[ 50 %] Time elapsed: 00:00:05 | ETA: 00:00:05 1234567bar = pyprind.ProgBar(10, monitor=True, title='Test')for i in range(10): time.sleep(10) # your computation here bar.update() # print report for future referenceprint(bar) 等待程序运行完了之后，pyprind 会打印出程序开始、结束时间以及 CPU 和内存占用的情况，一目了然。 123456789Test0% [##########] 100% | ETA: 00:00:00Total time elapsed: 00:01:40Title: Job_1 Started: 07/28/2017 19:59:49 Finished: 07/28/2017 20:01:29 Total time elapsed: 00:01:40 CPU %: 0.00 Memory %: 0.33 关于 pyprind 还有很多好玩的玩法，有兴趣的读者可以参考这个链接 推荐阅读 Python datetime PyPrind demo]]></content>
      <categories>
        <category>coding</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>pyprind</tag>
        <tag>tqdm</tag>
        <tag>进度条</tag>
        <tag>时间</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习算法系列（12）奇异值分解——以图像压缩为例]]></title>
    <url>%2F2017%2Fmachine-learning-algorithm-series-svd%2F</url>
    <content type="text"><![CDATA[严格地说来，本文涉及的主题并非属于机器学习算法，只是机器学习中的一种降维方法，笔者为了方便，仍旧将其归于机器学习算法系列。在解释奇异值分解的时候，笔者回避了难以理解的矩阵空间变换，而是从数学的角度用简单明了的公式说明。 一、前言在之前一篇文章中有提到过一种降维方法：机器学习算法系列（10）主成分分析（PCA），在这篇文章中我会介绍另外一种在机器学习中非常著名的降维技术：奇异值分解（Singular Value Decomposition，以下简称 SVD）是机器学习中提取信息或者也叫降维的一种方法，SVD 广泛应用于数据压缩和图像去噪。 其实，奇异值分解等价于主成分分析，核心都是求解$A A^{T}$的特征值以及对应的单位特征向量。 举一个简单易懂的例子：比如，每当学习一门新的学科，想象学科中的各种概念和知识构成了一个巨大的矩阵中的每个元素。为了理解它，你阅读了大量的文献，记了许多笔记，反反复复地理解，然后你明白了哪些部分是无关紧要的，哪些是必考的核心概念，一番去粗取精之后，原本厚厚的教材便可以浓缩为几页笔记。简单地来说，SVD 本质上就是这么一个过程，它能够帮助我们去除矩阵中的冗余信息，只保留最能够代表原矩阵的部分。 那么问题来了，如何进行奇异值分解呢？在这之前，听过一次培训班的笔者在庸师的讲解下，被各种 null space 、Strang’s Diagram 留下了“阴影”，以为 SVD 乃一大难度极高的“绝学”，以至于后来一直没敢去碰。 其实， 最近重拾 SVD ，查找了许多资料，其中来自火光摇曳翻译的一篇文章：奇异值分解（We Recommend a Singular Value Decomposition）在中文世界中广为流传，这篇译文加上原文：We Recommend a Singular Value Decomposition前前后后看了不下30遍，然而笨拙如我，除了记住了“22矩阵奇异值分解的几何实质是：对于任意22矩阵，总能找到某个正交网格到另一个正交网格的转换与矩阵变换相对应”这句话，依旧还是没能弄明白 SVD 到底要干啥。罢了，只能老老实实回去啃论文，Dan Kalman, A Singularly Valuable Decomposition: The SVD of a Matrix, The College Mathematics Journal 27 (1996), 2-23. 这篇论文的第2页和第3页，反复读几遍你会对 SVD 与 EVD 两者有更深的理解。然后，直到遇见统计之都的这篇文章：奇异值分解和图像压缩，看到了公式$\eqref{svd}$才豁然开朗，仿佛一语惊醒梦中人！什么鬼旋转、伸缩、再旋转，这才是 SVD 的精髓啊！！！ 二、奇异值分解：从特征值分解的角度回顾线性代数，我们知道，给定实对称矩阵$A_{n*n}$，对它进行特征分解（eigenvalue decomposition，以下简称 EVD）十分简单，只需要求解$ AV=\lambda V $即可，$V$是它的特征向量，$\lambda$是特征值。忘记这部分的读者可以翻翻之前的教材或者看看麻省理工公开课：线性代数，里边有一节专门讲奇异值分解，时隔多年，老爷子去年貌似又更新了 在理解 SVD 前，我们必须抛弃以往在大学学习线性代数时留下的只对或只能方阵做特征分解的思想 考虑任意的$m*n$矩阵$A$，则存在对角矩阵$\Sigma$，正交矩阵$U$和$V$（也叫左右奇异矩阵），有 \begin{equation}\begin{split}A=U \Sigma V^{T}\label{1}\end{split}\end{equation} 奇异值分解的目的就是为了找到符合条件的$\Sigma$，$U$和$V$，使得上式$\eqref{1}$成立。但是，现实中的矩阵大多不是对称矩阵，无法直接求解，所以，奇异值和奇异矩阵的求解需要转换一下思维。 不妨令$A^{T} A =(V\Sigma U^{T}) U \Sigma V^{T}$，由于$U$是正交矩阵，故$U^{T}U$为单位矩阵。所以，有 A^{T}A = V(\Sigma ^T\Sigma)V^T \\ A A^{T} = U(\Sigma ^T\Sigma)U^T这样一来，非对称阵的境地就得到了化解，奇异值分解巧妙地转化为特征值分解了。对任意矩阵$A$的 SVD， 右奇异向量是$A^{T} A$的特征向量，左奇异向量是$A A^{T}$的特征向量，奇异值为$\Sigma ^T\Sigma$上对角线非零元素的平方根。 正如上图下划线部分内容，如果$A$本身就是对称矩阵，对$A$的 SVD，它的左右奇异向量等价于$A$的特征向量，它的奇异值等价于$A$的特征值。所以，对任意的$A$，实对称矩阵$A^T A$的 SVD 和 EVD 殊途同归。 讲了这么多，SVD 的求解应该解释得差不多了，不过读者可能还是不明白 SVD 到底是干啥的，你不是说 SVD 可以压缩数据吗？那你是怎么压缩的呢？下面就来从数学的角度来讲讲数据压缩的原理 $A$的左右奇异向量如果用列向量的形式表示 U=(u_1,u_2,\ldots,u_n) \\ V=(v_1,v_2,\ldots,v_n)$\Sigma$的对角元素为$\sigma_i$，那么$A$可以分解为一系列的线性组合 \begin{equation}\begin{split}A = \sigma_1u_1v_1^{T} + \sigma_2u_2v_2^{T}+···+\sigma_nu_nv_n^{T} \\label{svd}\end{split}\end{equation} 又因为每项$\sigma_iu_iv_i^{T}$都表示一个$m*n$的矩阵$A_i$。这样一来，我们便可以认为矩阵$A$是$n$个这样矩阵的和。特征值$\sigma_i$越大表示$A_i$对矩阵的贡献越大，所以控制等式右边多项式的个数便可以达到压缩数据的目的！ 三、用 Python 实现 SVD在 Python 下通过 numpy 下的np.linalg.svd()函数可轻松实现 SVD 1234567891011121314151617%matplotlib inlineimport matplotlib.pyplot as pltimport numpy as npimport timefrom PIL import Image# 读取图片数据img = Image.open('gakki.png')imggray = img.convert('LA')plt.figure(figsize=(9, 6))plt.imshow(img)# 图片转化为 numpy 数组imgmat = np.array(list(imggray.getdata(band=0)), float)imgmat.shape = (imggray.size[1], imggray.size[0])imgmat = np.matrix(imgmat)plt.figure(figsize=(9,6))plt.imshow(imgmat, cmap='gray') 123456789# 奇异值分解得左右奇异矩阵和奇异值矩阵U, sigma, V = np.linalg.svd(imgmat)# 逐步增加奇异值的个数for i in range(2, 40, 8): reconstimg = np.matrix(U[:, :i]) * np.diag(sigma[:i]) * np.matrix(V[:i, :]) plt.imshow(reconstimg, cmap='gray') title = "n = %s" % i plt.title(title) plt.show() 下面是图片从压缩到逐步接近原图的过程，可以看出，当只取公式$\eqref{svd}$的前2项时，从图像中几乎不能分辨出 gakki 酱，只能看到一个隐隐约约的轮廓。当奇异值得数目增加到 10 个，似乎能够猜到这是谁了，只是图片仍旧十分模糊。随着奇异值数目的增加，图片越来越清晰。 当取了前 51 个奇异值的时候，压缩后的图像与原图基本上看不出什么差别了，这也侧面说明了在 SVD 中，奇异值的分布是很不均衡，绝大多数奇异值集中在一极。以 gakki 的图片为例，总共有 670 个奇异值，奇异值的大小下降特别快，最大的奇异值和第二大的奇异值就相差了一个数量级。 如果您有什么问题或建议，欢迎留言！要是能够帮助分享一下那就再好不过啦！ 参考链接 统计之都： 奇异值分解和图像压缩 Singular Value Decomposition (SVD) tutorial 奇异值的物理意义是什么？ - 回答作者: 郑宁 We Recommend a Singular Value Decomposition 示例图片来源 Dan Kalman, A Singularly Valuable Decomposition: The SVD of a Matrix, The College Mathematics Journal 27 (1996), 2-23.]]></content>
      <categories>
        <category>Machine-Learning</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>Machine-Learning</tag>
        <tag>降维</tag>
        <tag>新垣结衣</tag>
        <tag>gakki</tag>
        <tag>奇异值分解</tag>
        <tag>svd</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pandas 处理数据的一点经验]]></title>
    <url>%2F2017%2Fpandas-preprocessing-data%2F</url>
    <content type="text"><![CDATA[pandas 是 Python 中一个非常强大的数据处理包，几乎所有的数据预处理都可以通过它来完成。 pandas 中有两种类型的数据结构，分别是 DataFrame 和 Series。Series是一个一维的类似的数组对象，包含一个数组的数据（任何NumPy的数据类型）和一个与数组关联的数据标签，被叫做 索引 。Series 有点类似于字典，它的每一个索引（index）唯一对应着一个值（value） 12345678910111213# 创建 Series 数据In [4]: obj = Series([4, 7, -5, 3])In [5]: objOut[5]:0 41 72 -53 3# 值和索引In [6]: obj.valuesOut[6]: array([ 4, 7, -5, 3])In [7]: obj.indexOut[7]: Int64Index([0, 1, 2, 3]) DataFrame 表示一个表格，类似电子表格的数据结构，包含一个经过排序的列表集，它们每一个都可以有不同的类型值（数字，字符串，布尔等等）。 1234567891011121314data = &#123;'state': ['Ohio', 'Ohio', 'Ohio', 'Nevada', 'Nevada'], 'year': [2000, 2001, 2002, 2001, 2002], 'pop': [1.5, 1.7, 3.6, 2.4, 2.9]&#125;frame = DataFrame(data)``` ![](http://raw.githubusercontent.com/swordspoet/i/img/778d5ca9ly1fh6quml61ij23v92ky1l2.jpg)# 一、显示全部的列名 #DataFrame 中包含了许多列，pandas 会用省略号来代替显示全部的行和列，可以通过一个小技巧来显示```pythonDataFrame.columns.tolist() 二、连接 MySQL连接 MySQL 的前提是 MySQL 已经安装，并且 Python 中也安装了 MySQLdb 包，满足两个条件之后，便可以通过编写好的 sql 语句直接从数据库中读取数据了，并且读取的数据还可以保存为 DataFrame 格式。 1234db_conn = MySQLdb.connect('localhost', 'root', '312624', 'baoliao', charset="utf8")# 截取从2017-02-01至2017-04-12年之间的用户爆料的单品数据sql = "select * from sampleDB where baoliao_sampleDB_for_model where baoliao_time&gt;='2017-02-01' and baoliao_time&lt;'2017-04-12'"sample = pd.read_sql(sql02, db_conn,) and is_shangjia_baoliao='用户爆料' and is_danpin='单品'" 三、统计各列下缺失值数1DataFrame.isnull().sum() 四、统计各列下每个值的频数1DataFrame.value_counts() 五、四舍五入1round(value, decimals) 对某个特征值下的全部数据进行四舍五入操作， 1data[''] = [round(value, 2) for value in data['']] 六、排序12# 对单列数据进行升序排序DataFrame.sort([""], ascending=True) 七、处理非连续值非连续值可以将取值进行唯一编码 123456cat = pd.Categorical(["1", "2", "c", "d"]).codescatOut[19]: array([0, 1, 2, 3], dtype=int8)cat = pd.Categorical(["1", "2", "c", "c"]).codescatOut[21]: array([0, 1, 2, 2], dtype=int8) 八、处理缺失值 pandas.to_numeric(arg, errors=’raise’, downcast=None) errors 的参数设置有三种情形： 如果是 ‘raise’，无效的数据将以例外的形式提示 如果是 ‘coerce’，无效的数据将设置为 NaN 如果是 ‘ignore’，无效的解析会被 pandas 直接忽略并返回到输入中 downcast 的参数设置也有三种情形： ‘integer’ or ‘signed’: smallest signed int dtype (min.: np.int8) ‘unsigned’: smallest unsigned int dtype (min.: np.uint8) ‘float’: smallest float dtype (min.: np.float32) 123456789101112131415161718192021222324252627282930&gt;&gt;&gt; import pandas as pd&gt;&gt;&gt; s = pd.Series(['1.0', '2', -3])&gt;&gt;&gt; pd.to_numeric(s)0 1.01 2.02 -3.0dtype: float64&gt;&gt;&gt; pd.to_numeric(s, downcast='float')0 1.01 2.02 -3.0dtype: float32&gt;&gt;&gt; pd.to_numeric(s, downcast='signed')0 11 22 -3dtype: int8&gt;&gt;&gt; s = pd.Series(['apple', '1.0', '2', -3])&gt;&gt;&gt; pd.to_numeric(s, errors='ignore')0 apple1 1.02 23 -3dtype: object&gt;&gt;&gt; pd.to_numeric(s, errors='coerce')0 NaN1 1.02 2.03 -3.0dtype: float64 对列中的缺失数据值填充 12# 零值填充DataFrame.fillna(0, inplace=True) 九、转换数据类型在 pandas 中将 object 类型数据转换成 float 型，通过convert_objects(convert_numeric=True)便可实现，参考链接：stackoverflow: Convert pandas.Series from dtype object to float, and errors to nans 12345678910111213141516171819In [2]: a = pd.Series([1,2,3,4,'.'])In [3]: aOut[3]: 0 11 22 33 44 .dtype: objectIn [30]: pd.Series([1,2,3,4,'.']).convert_objects(convert_numeric=True)Out[30]: 0 11 22 33 44 NaNdtype: float64 十、列的操作：增删改12345678910# 插入列df.insert(idx, col_name, value)# 删除某列del data['']df.drop(['col_name'], axis=1)# 删除某行df.drop(['row_name'], axis=0)# 修改列名df.columns = ['', '', '']df.rename(columns=&#123;'a': A', 'b': 'B'&#125;) 十一、去重复值1df.drop_duplicates(['col_name']) 参考链接 oreilly: 处理缺失数据 Python for Data Analysis: pandas入门]]></content>
      <categories>
        <category>coding</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>pandas</tag>
        <tag>缺失值处理</tag>
        <tag>数据预处理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何解释 Python 面向对象编程？]]></title>
    <url>%2F2017%2Fhow-to-explain-oop-in-python%2F</url>
    <content type="text"><![CDATA[面向对象编程（Object Oriented Programming，简称OOP）把对象作为程序的基本单元，一个对象包含了数据和操作数据的函数。 什么是“对象”？在 Python 中所有的数据类型都可以视为对象，然后每个对象都会有多个属性。比如，将“人”视为一个对象，姓名、身高、体重、性别等等都可以作为描述对象的属性。 1对象 = 属性1 + 属性2 + ··· + 属性n 创建类（class）按照我的理解，类（class）是一种可以将实体进行快速抽象的方法，在类里会强制规定各种属性。通过类方法，我们可以快速创建对象。 1234567891011class People(object): # '__init__' 的第一个参数永远是 self，self 参数 # 表示创建的实例本身 def __init__(self, name, height, weight, gender): self.name = name self.height = height self.weight = weight self.gender = gender def print_people(self): print "Name: %s, Height: %s, Weight: %s, Gender: %s" % (self.name, self.height, self.weight, self.gender) 然后就可以将数据传入到 People 类了，其实讲到现在类方法实际上跟函数没什么差别。 12345libin = People('libin', 175, 66, 'male')# 调用操作也十分简单libin.print_people()&gt;&gt;&gt; Name: libin, Height: 66, Weight: 175, Gender: male 访问属性访问、添加、删除、修改类的属性 12345678910# 访问类的'name'属性getattr(libin, 'name')&gt;&gt;&gt; libin# 检查类中是否存在一个属性，返回True或Falsehasattr(libin, 'name')&gt;&gt;&gt; True# 设置属性：设置类libin的weight为70setattr(libin, 'weight', 70)# 删除属性：删除类libin的height属性delattr(libin, 'height')]]></content>
      <categories>
        <category>coding</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>面向对象编程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python | pandas.concat：连接数据的“万能胶”]]></title>
    <url>%2F2017%2Fpython-pandas-concat%2F</url>
    <content type="text"><![CDATA[在 pandas 下，可以对多种不同类型的数据进行粘结，包括 DataFrame、Series，甚至字典都可以合在一起，可以说是“万能胶”了。 在 Python 中 Pandas 提供了许多组合数据的手段，下面主要介绍 concat 的用法。 现有3个 DataFrame 分别是：df1、df2、df3： 1234567891011121314151617df1 = pd.DataFrame(&#123;'A': ['A0', 'A1', 'A2', 'A3'], 'B': ['B0', 'B1', 'B2', 'B3'], 'C': ['C0', 'C1', 'C2', 'C3'], 'D': ['D0', 'D1', 'D2', 'D3']&#125;, index=[0, 1, 2, 3]) df2 = pd.DataFrame(&#123;'A': ['A4', 'A5', 'A6', 'A7'], 'B': ['B4', 'B5', 'B6', 'B7'], 'C': ['C4', 'C5', 'C6', 'C7'], 'D': ['D4', 'D5', 'D6', 'D7']&#125;, index=[4, 5, 6, 7]) df3 = pd.DataFrame(&#123;'A': ['A8', 'A9', 'A10', 'A11'], 'B': ['B8', 'B9', 'B10', 'B11'], 'C': ['C8', 'C9', 'C10', 'C11'], 'D': ['D8', 'D9', 'D10', 'D11']&#125;, index=[8, 9, 10, 11]) 使用 Pandas 将3个 DataFrame 组合起来也是特别简单的，只需 1result = pd.concat([df1, df2, df3]) 假如将 DataFrame 组合起来后，又有从 result 中提取 DataFrame 的需要呢？只需要在组合的时候添加 keys 参数就好了。 12frames = [df1, df2, df3]result = pd.concat(frames, keys=['x', 'y', 'z']) 如果需要提取 df2， 1234567In [7]: result.loc['y']Out[7]: A B C D4 A4 B4 C4 D45 A5 B5 C5 D56 A6 B6 C6 D67 A7 B7 C7 D7 在拼接 DataFrame 时，通过设置参数join你还可以决定只拼接哪些轴，比如，有 df4， 1234df4 = pd.DataFrame(&#123;'B': ['B2', 'B3', 'B6', 'B7'], 'D': ['D2', 'D3', 'D6', 'D7'], 'F': ['F2', 'F3', 'F6', 'F7']&#125;, index=[2, 3, 6, 7]) 在默认情况下，pandas 会将两个 DataFrame 完全粘结起来，这种连接方式保证了信息的零丢失， 12# 默认join='outer'result = pd.concat([df1, df4], axis=1) 当参数设置为join=&#39;inner&#39;，可以理解为从两个 DataFrame 中取交集， 1result = pd.concat([df1, df4], axis=1, join='inner') 当然，Series 也可以与 DataFrame 连接，比如， 12s1 = pd.Series(['X0', 'X1', 'X2', 'X3'], name='X')result = pd.concat([df1, s1], axis=1) 如果不喜欢 Series “讨厌”的名字，设置参数ignore_index=True便可以去掉重新索引， 1result = pd.concat([df1, s1], axis=1, ignore_index=True)]]></content>
      <categories>
        <category>coding</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>pandas</tag>
        <tag>concat</tag>
        <tag>数据连结</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习算法系列（11）Adaboost 算法及其参数解释]]></title>
    <url>%2F2017%2Fmachine-learning-algorithm-series-adaboost%2F</url>
    <content type="text"><![CDATA[在 Adaboost 中会有很多次的迭代计算，每一次的迭代计算得到一个基本分类器，根据已有的基本分类器，Adaboost 算法会提高那些在前一轮被弱分类器误分类的样本的权值，并降低那些被正确分类的样本的权值，对于“差生”重点关注。提升方法通过重复修改训练数据的权重分布，构建一系列的弱分类器，然后依据特定的组合算法对这些弱分类器进行线性组合，从而得到最终的强分类器。 Adaboost 是一个“笨蛋”，一开始它也许不怎么聪明，但是它非常乐于改进并接受他人的意见，所以最后它会变得很强大。 偏差指的是模型的期望预测值与真实结果的偏离程度，Boosting 方法在训练的过程中非常关注自己在每次的训练中所犯下的错误，所以“从偏差-方差分解的角度看，Boosting 主要关注降低偏差，因此 Boosting 能给予泛化性能相对弱的学习器构建很强的集成”。 一、背景在机器学习中，集成学习（Ensemble learning）目标是构建并组合多个学习器（Weak learner）来完成学习任务，组合学习器中的单个学习器“单兵作战”的效果可能并不理想，然而多个这样的学习器组合起来效果就很可观了。有一句俗话，“三个臭皮匠顶个诸葛亮”，意思是说几个资质平庸的人联合起来也有能有大智慧，集成学习的道理也是如此：能力不够，人头来凑。 学习一个简单粗糙的弱学习器比学习一个准确率很高的学习器要简单得多，比如预测股市的涨或者跌，没有任何知识背景的外行瞎蒙也有50%的准确率。然而通过特定组合算法的学习，这样几个弱学习器最终学习器整体的性能却能够得到显著提高，这个提高的过程也被称为提升（boosting）。提升是集成学习的一类，可以理解为“集思广益”，即团结力量大。这篇文章要介绍的便是集成学习中一个非常著名的算法—— Adaboost 算法。 二、Adaboost 算法Adaboost 算法的原理比较简单，在 Adaboost 算法中，有两个难点问题需要弄明白： 如何在每一轮训练中更新数据的权值$D_m$？ 如何将弱学习器组合成一个强学习器？ Adaboost 算法的基本思路大致有3个步骤，首先 Adaboost 给予训练集中的每个样本均匀的权重$w{1i}=\frac{1}{N}$，即每个训练样本在$h{1}(x)$分类器中所起得作用均一样；然后 Adaboost 反复学习$h{1}(x)$分类器，分别更新数据集权重和$h{m}(x)$分类器的系数；最后组合$m$个弱学习器得到最终分类器。 初始化训练数据$T$的权重分布$D_m$；D_1 = (w_{11}, w_{12},...,w_{1N}), w_{1i}=\frac{1}{N} 基于初始化训练数据集进行学习，得到弱学习器$h_{1}(x)$； 对$m=1,2,..M$，计算弱学习器$h{m}(x)$在训练数据集上的误差率$e_m$和弱学习器$h{m}(x)$的系数$\alpha$； e_m = P(h_{m}(x) \neq y_i) \begin{equation}\begin{split} \alpha_m = \frac{1}{2}ln(\frac{1-e_m}{e_m}) \end{split}\end{equation} 更新训练数据集的权值分布，迭代到第m次时； D_{m+1} = (w_{m+1,1}, w_{m+1,2},...,w_{m+1,N}) \begin{equation}\begin{split} w{m+1,i} = \frac{w{m,i}}{Zm}exp(-\alpha{m}yih{m}(x_i))\ \label{dataweightupdate} \end{split}\end{equation} Z_m = \sum_{i=1}^{N}exp(-\alpha_{m}y_ih_{m}(x_i)） 构建弱学习器的线性组合，得到最终分类器$G(x)$ f(x) = \sum_{i=1}^{N}\alpha_{m}h_{m}(x_i)H(x) = sign(f(x)) 三、Adaboost 算法参数的解释不打算继续深究的读者可以就此打住了，毕竟大多数人只是用用模型，背后复杂的数学推导根本没有必要。然而，本着知其然更要知其所以然的精神，我们还是有必要对 Adaboost 算法中涉及到的两个点——弱学习器系数和数据集权值分布，做一个简单的梳理，亦呼应了上一节开头提的两个问题。 1. 分类器权重$\alpha_m$更新原理Adaboost 算法的优化目标是最小化指数损失函数 \begin{equation}\begin{split}E &amp;= exp^{-f(x)H(x)} \&amp;=exp^{-f(x)\alpha_mh_m(x)},f(x)h_m(x)只有1和-1两种情形,同时对应两种概率 \&amp;=exp^{-\alpha_m}(1-e_m)+exp^{\alpha_m}e_m\\label{weightupdate}\end{split}\end{equation} 令$\eqref{weightupdate}$式为零可得分类器权重更新公式 \alpha_m = \frac{1}{2}ln(\frac{1-e_m}{e_m})2. 数据集权重$\bar{w}_{m+1,i}$更新原理求解在每一轮样本的权值更新，由下面两个式子 f_m(x) = f_{m-1}(x) + \alpha H_m(x)\bar{w}_{m,i} = exp(-y_if_{m-1}(x_i))得 \frac{ln(\bar{w}_{m+1,i})}{-y_i} = \frac{ln(\bar{w}_{m,i})}{-y_i} - \alpha_m H_m(x)经过化简可得样本权值的更新公式，对应算法中的$\ref{dataweightupdate}$，唯一的区别在与少了一个规范化因子，但是这并不妨碍与$\ref{dataweightupdate1}$等价 \begin{equation}\begin{split}\bar{w}{m+1,i} = \bar{w}{m,i}·exp(-y_i \alpha_m H_m(x)) \\label{dataweightupdate1}\end{split}\end{equation} 3. 分类误差率$e_m$e_m = \sum_{i=1}^{N} w_{m,i} I(h_{m}(x_i) \neq y_i)Adaboost 算法的笔记暂时告一段落了，如果读者还有不太明白的地方，可以参照《统计学习方法》第140页的“Adaboost 例子”来辅助理解，文末提供的参考5给出了该例更为详细的讲解，相信对理解 Adaboost 算法的原理会有一定的帮助。 笔者是头一次接触集成学习（Ensemble learning），在这之前对集成学习的概念很是模糊，因为先前接触到的都是依据单个算法做成的分类器，所以一直弄不明白的是多个弱分类器是如何组成称为一个新的并且性能更强大的分类器。知其然还要知其所以然，笔者一直认为不管是直接调用各种强大的库，还是调一调参数以期提高机器学习的应用水平，背后的数学原理还是有必要去理解一下的，不然总有一种不踏实的感觉。 参考 统计学习方法.李航 机器学习.周志华 Explaining AdaBoost Adaboost - 新的角度理解权值更新策略 Adaboost 算法的原理与推导（读书笔记）]]></content>
      <categories>
        <category>Machine-Learning</category>
      </categories>
      <tags>
        <tag>机器学习算法系列</tag>
        <tag>Machine-Learning</tag>
        <tag>集成学习</tag>
        <tag>Adaboost</tag>
        <tag>弱学习器</tag>
        <tag>boosting</tag>
        <tag>提升方法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL 学习笔记（二）数据类型]]></title>
    <url>%2F2017%2Fmysql-study-note-2%2F</url>
    <content type="text"><![CDATA[MySQL 中所谓的数据类型，是从系统的角度出发，为了方便对数据进行统一的分类，能够使用统一的方式进行管理，更好地利用有限的空间的一种手段。在 SQL 中数据类型分成了三大类：数值类型、字符串类型和时间日期类型。 数值型数据都是数据，系统将数据型分为整数型和小数型。 一、整数型在 UTF-8 编码环境下，一个英文字符等于一个字节，一个中文字符（含繁体）等于三个字节。 在 SQL 中要更多考虑如何节省磁盘空间，所以系统将整型又细分成了5类 tinyint：迷你整型，1个字节存储，表示的状态最多为255种 smallint：小整型，2个字节存储，表示的状态最多为65535种 mediumint：中整型，3个字节存储 int、integer：标准整型，使用4个字节存储（常用） bigint：大整形，使用8个字节存储 创建一张整型表 SQL 中的数值类型全部都是默认有符号，区分正负，有的时候需要给数据类型限定：int unsigned，无符号，从零开始。 1234567-- 创建整型表create table my_int(int_1 tinyint,int_2 smallint,int_3 int,int_4 bigint)charset utf8; 12345678910-- 插入数据insert into my_int values(100,100,100,100); -- 有效数据insert into my_int values('a','b','199','f'); -- 无效数据insert into my_int values(255,10000,100000,1000000); -- 错误：255超出tinyint的范围-- 给表增加一个无符号类型alter table my_int add int_5 tinyint unsigned; -- 添加无符号类型-- 再次插入数据insert into my_int values(127,10000,100000,1000000,255); 1234-- 指定字段显示宽度alter table my_int add int_6 tinyint(1) unsigned; -- 指定显示宽度为1alter table my_int add int_7 tinyint(2) zerofill; -- 显示宽度为2,0填充insert into my_int values(1,1,1,1,1,1,1) 查看表结构的时候后，发现每个字段的数据类型之后都会自带一个括号，代表显示宽度，显示宽度没有特别的含义，只是默认的告诉用户可以显示的形式而已，不会改变数据本身的数值大小。显示宽度的意义在于当数据不够显示宽度的时候，会自动让数据变成对应的显示宽度。零填充（zerofill参数）会自动导致数值添加无符号（unsigned）属性，保证数据格式。 二、 小数型小数型，带有小数点或者范围超出整型的数值类型。SQL 中将小数型分为浮点型和定点型。浮点型：小数点浮动，精度有限；定点型：小数点固定，精度固定，不会丢失精度。 浮点型（float）浮点型数据是一种精度型数据，超出指定范围之后会丢失精度（会自动四舍五入），分为float单精度（占4个字节）和double双精度（占用8个字节）。 123456-- 创建浮点数据表create table my_float(f1 float,f2 float(10,2),f3 float(6,2) -- 6位在精度范围之内)charset utf8; 123456789-- 插入数据insert into my_float values(1000.10,1000.10,1000.10);insert into my_float values(1234567890,12345678.90,1234.56); insert into my_float values(3e38,3.01e7,1234.56);insert into my_float values(9999999999,99999999.99,9999.99); -- 超出长度插入数据insert into my_float values(123456,1234.12345678,123.9876543); -- 小数部分超出没有问题，自动四舍五入insert into my_float values(123456,1234.12,12345.56); -- 整数部分超出 浮点数如果不写精度和标度，则会按照实际的精度值显示；float(M,D)则表示标注了精度和标度，M 代表数据总长度（精度），D 代表小数部分长度（标度），整数部分长度为M-D。浮点型数据的插入，整型部分是不能超出长度的；但是小数部分是可以超出长度的，超出范围的小数的部分系统会自动四舍五入并保留合法的小数部分长度。 定点型（decimal）绝对保证整数部分不会被四舍五入，不会丢失精度，小数部分有可能会丢失精度。定点型在不指定精度时，默认的整数位为10，默认的小数位为0。定点数的整数部分一定不能超出长度，小数部分的长度可以随意，超出的部分会被系统自动截断。12345-- 创建定点数表create table my_decimal(f1 float(10,2),d1 decimal(10,2))charset utf8; 123-- 插入数据insert into my_decimal values(12345678.90,12345678.90); -- 有效数据insert into my_decimal values(1234.123456,1234.1234567); -- 小数部分超出没事 12insert into my_decimal values(99999999.99,99999999.99); -- 有效数据insert into my_decimal values(99999999.99,99999999.999); -- 进位超出范围 三、时间日期类型 datatime：年月日时分秒，YYYY-MM-DD HH:MM:SS，显示宽度固定为19个字符，表示的范围从1000到9999年 date：年月日，datetime中的date部分 time：时分秒，指某个区间之内 timestamp：时间戳，在更新数据的时候会自动更新为当前系统时间 year：年份，在4位格式中，允许的值是1901-2155之间的年份 1234567891011121314151617181920-- 创建时间日期表create table my_date(d1 datetime,d2 date,d3 time,d4 timestamp,d5 year)charset utf8;-- 插入数据insert into my_date values('2017-05-25 21:28:54','2017-05-25 ','21:28:54','2017-05-25 21:28:54',2017);-- 时间改为负数insert into my_date values('2017-05-25 21:28:54','2017-05-25 ','-21:28:54','2017-05-25 21:28:54',2017);insert into my_date values('2017-05-25 21:28:54','2017-05-25 ','-211:28:54','2017-05-25 21:28:54',2017);insert into my_date values('2017-05-25 21:28:54','2017-05-25 ','-2 11:28:54','2017-05-25 21:28:54',2017);-- year可以使用两位或者四位insert into my_date values('2017-05-25 21:28:54','2017-05-25 ','21:28:54','2017-05-25 21:28:54',69);insert into my_date values('2017-05-25 21:28:54','2017-05-25 ','21:28:54','2017-05-25 21:28:54',70); 四、字符串类型在 SQL 中，将字符串类型分成了6类：char,varchar,text,blob,enum和set。 定长字符型（char）char 磁盘在定义结构的时候，就已经确定了最终的数据的存储长度。char(L)，L代表长度，可以存储的长度，单位为字符，最大长度可以为255。比如在 utf8 环境下，中文字段char(4)需要4*3=12个字节。 变长字符串（varchar）varchar，变长字符串，在分配空间的时候，按照最大的分配空间，但是实际上用了多少，是根据具体的数据来确定的。varchar(L)，L 表示字符长度，理论长度是65536个字节，会多出1到2个字节来确定存储的实际长度。varchar(10)：确实存了 10 个汉字，utf8环境，10*3+1=31个字节，好处是可以节省存储空间。 如果字符串实际上长度超过255，则既不使用定长也不使用变长，使用文本字符串text。 定长与变长的实际存储空间（UTF8），假设实际存储数据是abcd，在char(4)下占用43个字节，在varchar(4)下占用43+1个字节。假设实际存储数据是a，在char(4)下占用43（长度固定）（长度是可以变化的），varchar(4)下占用13+1个字节。假设实际存储数据是abcde，在char(4)和varchar(4)下是无法存储成功的，因为数据全部超出了长度4。 如何选择定长或者变长字符串？？？ 数据基本上长度都一样，如身份证号码、手机号码、电话号码，一般选择定长型字符串，如果选用定长字符串，查找的效率比较高，但是比较浪费磁盘空间。 如果不同数据的长度有变化，比如姓名、地址等的长度就会有两个或者三个，一般选用变长型字符串。变长型字符串则比较节省磁盘空间，但是效率比较低。 文本型字符串（text）通常说超过255个字符就会使用文本字符串，也就是数据量非常大。文本字符串根据存储的数据格式进行分类：text（存储文字）和blob（二进制数据，通常不用）。 12345-- text占用10个字节长度create table my_text(name varchar(21841) not null, -- 21841 * 3 + 2 = 65525content text not null -- 10 )charset utf8; 枚举类型（enum）枚举类型（enum）是将所有可能出现的结果都设计好，实际上存储的数据必须是规定好数据中的一个。按照元素出现的顺序，从1开始编号。枚举在进行数据规范的时候，系统会自动建立一个数字会枚举元素的对应关系，然后在进行数据插入的时候，系统自动将字符转换成对应的数字存储，然后在进行数据提取的时候，系统自动将数值转换成对应的字符串显示（这也是效率比较低的原因）。 枚举的使用方式： 123# 定义：enum（可能出现的元素列表） # 存储数据，只能存储定义好的数据enum('男','女','不男不女','保密'); 1234-- 创建枚举表create table my_enum(gender enum('男','女','保密'))charset utf8; 123-- 加入数据insert into my_enum values('男'),('保密'); -- 有效数据insert into my_enum values('male'); -- 无效数据 枚举数据类型的作用之一是规范数据格式，数据只能是规定的数据中的一个，作用之二是节省存储空间（枚举通常有一个别名：单选框），在 MySQL 中系统会自动转换数据格式的 枚举实际存储的是数值，所以在插入时可以直接插入数值。证明字段存储的数据是数值：将数据加上0，如果能够正常返回则证明字段存储的是数值。 集合字符串（set）集合字符串与枚举十分相似，两者实际存储的都是数值，而不是字符串（集合是多选）集合中每一个元素都是对应一个二进制位。 123# 集合使用方式# 定义：set(元素列表)# 元素列表可以是一个或者多个，使用逗号分隔 123-- 插入数据insert into my_set values('篮球,足球,乒乓球');insert into my_set values(3); 每一个元素都是对应一个二进制位，被选中为1没有则为0，最后反过来；集合中插入数据时与插入的顺序没有关系；集合的强大在于能够规范数据和节省空间。 五、MySQL 记录长度MySQL 中规定任何一条记录最长不能超过65535个字节，utf8下varchar的实际最多有21844个字符，gbk下最多32766个字符。 MySQL 记录中如果有任何一个字段允许为空，那么系统会自动从整个记录中保留一个字节来存储null，如果想释放null这个字节，必须保证所有的字段都不能为空。所以我们可以通过这种方式来占用utf8或gbk下全部的65535个字符。 列属性真正约束字段的是数据类型，但是数据类型的约束十分单一，需要有一些额外的约束来更加保证数据的合法性。 空属性 NULL（默认）和NOT NULL（不为空），尽可能在数据库汇总要保证所有的数据不能为空，空数据没有意义并且不能参与运算。 列描述 列描述comment用来给数据库管理员了解列属性，没有实际意义，是专门来描述字段，会根据表创建语句保存 默认值 某一种数据会经常性地出现某个具体的值，可以在一开始就指定好]]></content>
      <categories>
        <category>coding</category>
      </categories>
      <tags>
        <tag>数据库,SQL,MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL 学习笔记（一）基本操作]]></title>
    <url>%2F2017%2Fmysql-study-note-1%2F</url>
    <content type="text"><![CDATA[一、关键字说明数据库：database数据库系统：DBSDBMS：数据库管理系统DBA：数据库管理员 行、记录（record）：都是指表中的一条记录，行是从结构角度出发，记录是从记录角度出发 列、字段（field）：数据 二、结构化查询语言（数据以查询为主：99%是在进行查询操作）SQL 分为三个部分： DDL：数据定义语言，用来维护数据的结构（数据库、表），代表指令：create、drop、alter DML：数据操作语言，用来对数据进行操作，代表指令：insert、delete、update DCL：数据控制语言，主要是负责权限管理（用户），代表指令：grant、revoke等 SQL 是关系型数据库的操作指令，SQL 是一种约束，但不强制，不同的数据库可能内部会有细微的区别。 三、 MySQL 数据库MySQL 是一种c/s（客户端/服务器）结构的软件：客户端/服务器，若想访问服务器必须通过客户端（服务器一直运行，客户端在需要使用的时候运行） 客户端与服务器之家的交互方式 客户端连接认证：连接服务器，认证身份：mysql -u root -p 客户端发送SQL指令 服务器接收SQL指令：处理SQL指令返回操作结果 客户端接收结果：显示结果show databases; 断开连接（释放资源：服务器并发限制）：exit、quit、\q MySQL 服务器对象：由于编译是不可逆的，所以无法完全了解服务器内部内容，只能粗略分析服务器的内部结构。可以将服务器内部对象分成四层：操作系统（3306端口）=》数据库（DBMS）=》数据表（Table）=》字段（filed）。 四、 SQL 基本操作基本操作：CRUD（增删改查），根据操作对象分为三类：库操作、表操作、数据操作。 库操作对数据库的增删改查 新增数据库 123456# 库选项：用来约束数据库，分为两个选项# 字符集设定：charset/character set（数据存储的编码格式）# 校对集设定：collate# 其中数据库名字不能用关键字# 如果一定要使用关键字或保留字，必须使用反引号`Create database 数据库名字 [库选项]; SQL 语句报错只会提示大概错误位置，不会说明错误的原因（静默模式）。中文名的数据库是可以创建的，但是前提是保证服务器能够识别（建议不用）。 查看数据库 查看指定部分的数据库：模糊查询，%匹配多个字符，_匹配单个字符 查看数据库的创建语句：show create database [数据库名字] 数据库在执行SQL语句之前会优化SQL：系统保存的结果是优化的结果 更新数据库 数据库名字不可以修改（不安全），数据库的修改仅限库选项：字符集和校对集（校对集依赖于字符集，但是不要随便改）。 删除数据库 数据库删除了之后，发生了什么？ 在数据库中看不到对应的数据库 在对应的数据库存储的文件夹内，数据库名字的文件夹也被删除，数据表也被删除了 不要随意删除数据库，应该先进行备份后操作（删除不可逆的）。 表操作表与字段是密不可分的。 新增数据表 任何一个表的设计都必须指定数据库，一般有显式和隐式两种方案，显式的采用数据库名.表名办法；隐式的先进入某个数据库环境，然后将创建的表自动归属到某个指定的数据库。 12345678create table [if not exists] 数据库名.表名( -- 显视地将表放置在数据库下字段名字 数据类型,字段名字 数据类型 -- 最后一行不需要逗号)[表选项]; # if not exists 检查功能：如果表名不存在数据表才会被创建 # 字符集：具体字符集 # 存储引擎：engine （innodb 和 mysian） 1234567891011121314151617 -- 新增数据表（显式的）create table if not exists mydatabase.student(name varchar(10),gender varchar(10),number varchar(10),age int )charset utf8;-- 新建数据表（隐式的）-- 进入数据库use mydatabase;-- 创建表create table class(name varchar(10),room varchar(10))charset utf8; 创建SQL 执行指令之后，发生了什么？ 指定数据库下存在对应的表 在数据库对应的文件夹下，会产生对应表的结构文件 查看数据表 12-- 查看数据库show databases; 数据库能查看的方式，表都可以查看 查看所有表 12-- 查看所有表show tables; 查看表中的字段信息： desc、 describe 、show column from 表名 1234-- 查看表结构desc class;describe class;show columns from class; 修改数据表 表本身存在，还包含字段，表的的修改分为两个部分：修改表本身和修改字段。 修改表本身：表名和表选项 修改表名：rename table 旧表名 to 新表名 12-- 重命名表：student表 -&gt; my_student（取数据库名字的前两个字母）rename table student to my_student; 修改表选项：字符集、校对集和存储引擎 12-- 修改表选项：字符集alter table my_student charset = GBK; 修改字段：新增、修改、重命名、删除 12345678新增字段：alter table 表名 add[column] 字段名 数据类型 [列属性][位置];位置：字段名可以存放在表中的任意位置 first：第一位置 after：在哪个字段之后修改字段：修改通常是修改属性或者数据类型：alter table 表名重命名字段：alter table 表名 change 旧字段 数据类型 [属性][位置]; 1234-- 给学生表增加一个id放到第一位置alter table my_studentadd column id intfirst; -- mysql 会自动寻找分好作为语句结束符 123-- 将学生表中的number学号字段变成固定长度，且放到第二位置alter table my_studentmodify number char(10) after id; 123-- 修改学生表中的gender字段为sexalter table my_studentchange gender sex varchar(10); 删除字段 alter table 表名 drop 字段名;字段名下的数据都会被删除 删除数据表 drop table 表名1，表名2...; 当删除数据表之后发生了什么？ 表空间中，没有了指定的表（数据也没有了） 数据库对应的文件夹下，表对应的文件也会被删除 删除有危险，操作需谨慎（操作不可逆） 数据操作 新增数据 方案1： 给全表段插入数据，不需要指定字段列表：要求数据的值出现的顺序必须与表中设计的字段出现的顺序一致：凡是非数值数据，都需要使用引号（建议是单引号）包裹 1insert into 表名 value（值列表） -- 可以一次性插入多条记录 123-- 插入数据insert into my_student values(1,'itcast0001','Jim','male'), (2,'itcast0002','Hanmeimei','female'); 方案2： 给部分字段插入数据，需要选定字段列表，字段列表出现的顺序与字段的顺序无关，但是值列表的顺序必须与选定的字段的顺序一致 1inssert into 表名（字段列表）values（值列表）[,(值列表)]; 查看表中的数据 1select */字段列表 from 表名 [where条件]; 查看指定字段，指定条件的数据 12-- 查看指定字段，指定条件数据select id,number,sex,name from my_student where id = 1; -- 查看满足ID为1的学生信息 更新数据 12-- 更新数据update my_student set sex = 'female' where name = 'Jim'; update 表名 set 字段 = 值 [where 条件] 不是说执行了SQL 语句就是成功了，需要看数据字段是否有改变 删除数据 删除是不可逆的，需谨慎删除：delete from 表名 where 12-- 删除数据delete from my_student where sex = 'male'; 五、中文数据问题中文数据问题本质是字符集问题，计算机只识别二进制，人类更多的是识别符号，所以需要有一个二进制与字符的对应关系（字符集） 场景：客户端向服务器插入中文数据，但是没有成功 原因：中文字符在当前在当前编码（字符集）下对应的二进制转十六进制：两个汉字=》四个字节（GBK），服务器没有识别对应的四个字节：服务器认为数据是utf-8，在utf-8编码下一个汉字有三个字节，读取三个字节转换成汉字失败，剩余的再读一个字节不够。 查看服务器到底识别哪些字符集，支持39种数据集，基本上什么字符集都可以支持 问题根源是客户端数据是gbk而服务器认为是utf8，解决方案是改变服务器默认接收字符集为gbk，set 变量 = 值，会话级别，只对当前会话有效，关闭之后重启还是一样 设置服务器对客户端的字符集的认识，可以使用快捷方式： 1set names gbk; 连接层是字符集转变的中间者，如果统一了效率更高，不统一也没问题。 六、校对集问题校对集：数据比较的方式，一共有197种，不过校对集用的频率不多。 校对集有三种格式 _bin:binary，二进制比较，取出二进制位，一位一位比较，区分大小写 _cs: case sensitive，大小写敏感，区分大小写 _ci: case insensitive，大小写不敏感，不区分大小写 只有对数据产生比较的时候校对集才会产生作用。 12345678910111213141516-- 使用不同的校对集创建表create table my_collate_bin(name char(1))charset utf8 collate utf8_bin;create table my_collate_ci(name char(1))charset utf8 collate utf8_general_ci;-- 插入数据insert into my_collate_bin values('a'),('A'),('b'),('B');insert into my_collate_ci values('a'),('A'),('b'),('B');-- 比较：根据某个字段进行排序：order by 字段名 [asc/desc]select * from my_collate_bin order by name;select * from my_collate_ci order by name; 校对集必须再数据之前生成好，如果生成好了则对数据无效。 七、Web 乱码问题动态网站由浏览器、Apache服务器、数据库服务器三个部分组成，三个部分都有自己的字符集（中文），数据在这三者之间的传递很容易导致乱码。 如何解决？？？]]></content>
      <categories>
        <category>coding</category>
      </categories>
      <tags>
        <tag>SQL</tag>
        <tag>数据库</tag>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何解释方差与偏差的区别？]]></title>
    <url>%2F2017%2Fhow-to-explain-the-difference-between-bias-and-variance%2F</url>
    <content type="text"><![CDATA[将来训练上出现偏差，等于…你…你也有责任吧！ 从统计学的意义上来说，方差（variance）是衡量数据的离散程度，而偏差（bias）则是反映数据与真实情形的偏离程度。 假设有一个样本$x$，$y$是测试样本的真实标记，$f(x)$是模型根据样本训练得出的预测输出结果，预测输出结果的期望为$\bar{f}(x)$，则有 预测结果在样本集上的分散程度（偏离期望）方差为， var(x)=[f(x)-\bar{f}(x)]^2预测输出结果与真实标记的差别称为偏差， bias(x)=[y-\bar{f}(x)]^2下面这个图可以很好地解释偏差与误差之间的区别，圆圈中间的红点代表数据的真实标记，蓝点表示模型的预测结果。很明显，在高方差情形下，模型的预测结果很分散，反之则很集中；在高偏差情形下，模型预测结果的正确率极低，反之则很高。 可以说，方差和偏差分别评价了两个不同因素（数据、模型）在机器学习中的表现，也可以这样来理解：方差代表了数据扰动所造成的影响，增大样本容量通常可以减轻数据扰动带来的影响；偏差则刻画了算法或模型的拟合能力，它与数据本身关系不太大，通过特征工程、调节参数、选择模型等手段可以解决偏差问题。随着训练程度的增大，学习器拟合性能越来越强，偏差会越来越小，而数据的任何扰动则会使已经完善的学习器产生较大方差。]]></content>
      <categories>
        <category>Machine-Learning</category>
      </categories>
      <tags>
        <tag>统计学</tag>
        <tag>方差</tag>
        <tag>偏差</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习算法系列（10）主成分分析（PCA）]]></title>
    <url>%2F2017%2Fmachine-learning-algorithm-series-pca%2F</url>
    <content type="text"><![CDATA[主成分分析（Principal Components Analysis，PCA）是一种无监督降维技术，它广泛应用于电视信号传输、图像压缩等领域。当面临的数据维数很高的时候，我们很难发现隐藏在数据中的模式和有用的信息，并且给建模带来不便，PCA 是一种常见的解决这类问题的手段。PCA 的目的在于寻找一个能够对所有样本进行恰当表达的超平面，这个超平面具有两个性质：最近重构性（样本点到这个超平面的距离都足够近）和最大可分性（样本点在这个超平面上的投影能够尽可能分开）。 以上说的可能有点抽象，举个例子，比如晚上你在路灯下行走，当你走到路灯的正底下或者仅仅偏离正底下一点点，光凭一丁点阴影是没有办法判断你的性别的。当你继续往前走，灯光把影子越拉越长，阴影中包含的信息逐渐多了起来，比如胖瘦、头发、衣着等等，此时判断性别就相对简单多了。我举这个例子的用意在于，如果找到一个好的投射坐标系（这是关键！），就能用最小的成本保留原始数据最多的信息。 1. 主成分分析原理在 PCA 中，数据从原来的坐标系转换到新的坐标系，新坐标系的选择是由数据本身决定的。第一个新坐标轴选择的是原始数据中方差最大的方向，第二个新坐标轴的选择和第一个坐标轴正交且具有最大方差的方向。 为什么要找原始数据中方差最大的方向？在信号处理中认为信号具有较大的方差，噪声有较小的方差，信噪比就是信号与噪声的方差比，越大越好。如下图，数据投射到红色向量（方差为 0.206）上显然能够更好地将数据隔开，并且能够保留原数据集的最大信息。 PCA的优化目标是什么？假设对数据已经进行了去均值处理$\sum{x_i}=0$，且新坐标系为$W$，并且将维度降低，得到低维坐标中的投影$W^Tx$。因为我们需要尽可能地将样本点分开，所以需要寻找方差最大的方向$W$，由此 PCA 的优化目标 \begin{equation}max\quad tr(W^TXX^TW) \s.t.\quad W^TW=I\end{equation} 然后对上式采用拉格朗日乘子法， XX^TW=\lambda W是不是很熟悉？这不就是对协方差矩阵$XX^T$求特征值和特征向量嘛！$W$可能包含多个向量，主成分的个数对应着取$W$中多少个特征向量，需要留下多少的信息取决于自身的决定。 2. 预备数学知识在接触到 PCA 之前，大学学过的线性代数和概率统计的知识有点忘了，在学习的时候顺带把相关的知识复习一下。 2.1 协方差协方差在概率统计中用于衡量两个变量之间的总体误差，方差是协方差的一种特殊情况。 cov(X,Y)=\frac{\sum_{i=1}^{n}(X_i-\bar{X})(Y_i-\bar{Y})}{n-1}var(X)=\frac{\sum_{i=1}^{n}(X_i-\bar{X})(X_i-\bar{X})}{n-1}如果两个变量的变化趋势一致，也就是说如果其中一个大于自身的期望值，另外一个也大于自身的期望值，那么两个变量之间的协方差就是正值。 如果两个变量的变化趋势相反，即其中一个大于自身的期望值，另外一个却小于自身的期望值，那么两个变量之间的协方差就是负值。 如果两个变量相互独立，那么它们之间的协方差就是零。 上面讨论的情形均是针对两个变量，如果有多个变量的存在呢？比如有$x,y,z$三个变量，那么便需要计算三对协方差，为了方便，我们将协方差放进一个矩阵，称为协方差矩阵（covariance matrix）。 2.2 特征值和特征向量关于特征值和特征向量的基础知识可参照线性代数第五版（高等教育出版社）“方阵的特征值和特征向量”p117。 3. PCA 基本步骤了解了一些基本的数学知识之后，便可以开始着手PCA的工作了 假设我们需要对如下数据做降维处理，需求是将其从2维降到1维，该如何处理呢？ \begin{array}{c|lcr} Data & \text{} & \text{} \\ \hline x& 2.5&0.5&2.2&1.9&3.1&2.3&2&1&1.5&1.1 \\ y&2.4&0.7&2.9&2.2&3.0&2.7&1.6&1.1&1.6&0.9& \\ \end{array}PCA 主要有以下几个步骤： 1.去除平均值，做中心化处理 \begin{array}{c|lcr} Data\ Adjust & \text{} & \text{} \\ \hline x& .695&-1.31&.39&.09&1.29&.49&.19&-.81&-.31&-.71 \\ y&.49&-1.21&.99&.29&1.09&.79&-.31&-.81&-.31&-1.01 \\ \end{array}2.计算协方差矩阵 3.计算协方差矩阵的特征值（eigenvalue）和特征向量（eigenvector） 4.将特征值从大到小排列，选择前k个特征值和其相对应的特征向量，并组成新的特征向量矩阵。在本例中只有两个特征值，所以k=1，选择1.28402771和相应的特征向量（Feature vector）$(-0.677873399,-0.735178656)^T$ 5.将数据集映射到新空间，生成新的数据集。调整后的数据集是 10×2 的矩阵，与特征向量（n×k）相乘之后，原始数据集由 n=2 维变成 k=1 维 New\ data=Data\ Adjust*Feature\ vector 降维给数据带来了两个影响： 由于维度的降低，较小特征值对应的原始数据的部分信息被舍弃了，数据的采样密度增大，所以对数据降维的目的达到 当数据受到噪声影响时，最小特征值对应的特征向量往往与噪声有关，将它们舍弃能在一定程度上起到降噪的效果 4. 基于 Python 实现 PCA将上面的例子用 Python 来实现一遍 123456789101112import numpy as npimport matplotlib.pyplot as plt# 导入数据，转换为矩阵x=np.array([2.5,0.5,2.2,1.9,3.1,2.3,2,1,1.5,1.1])y=np.array([2.4,0.7,2.9,2.2,3,2.7,1.6,1.1,1.6,0.9])# 去除平均值remove_mean_x = x - np.mean(x)remove_mean_y = y - np.mean(y)data=np.matrix([[remove_mean_x[i], remove_mean_y[i]] for i in range(len(remove_mean_x))])plt.plot(remove_mean_x, remove_mean_y, 'o')plt.show() 1234# 计算协方差矩阵cov = np.cov(remove_mean_x, remove_mean_y)# 计算特征值和特征向量eig_value, eig_vector = np.linalg.eig(cov) 计算得到的协方差和特征向量矩阵，协方差矩阵是对称阵，故得到的特征向量之间是相互正交的。 12345678# 协方差矩阵[[ 0.61655556 0.61544444] [ 0.61544444 0.71655556]]# 特征值[ 0.0490834 1.28402771]# 特征向量[[-0.73517866 -0.6778734 ] [ 0.6778734 -0.73517866]] 1234567# 选择特征向量eig_pairs = [(np.abs(eig_value[i]), eig_vector[:,i]) for i in range(len(eig_value))]eig_pairs.sort(reverse=True)feature=eig_pairs[0][1]# 将数据映射到新空间，得到降维后的数据rot_data = (np.dot(eig_vector, np.transpose(data))).Tnew_data = (np.dot(feature, np.transpose(data))).T 1234567891011# 降维后的数据[[-0.82797019] [ 1.77758033] [-0.99219749] [-0.27421042] [-1.67580142] [-0.9129491 ] [ 0.09910944] [ 1.14457216] [ 0.43804614] [ 1.22382056]] 蓝色线条为选择的特征向量，红点为移除平均值之后的数据集，星点为降维后得到的数据 1234567# 降维前后数据集的对比plt.plot(remove_mean_x, remove_mean_y, 'o', color='red')plt.plot([eig_vector[:, 1][0], 0], [eig_vector[:, 1][1], 0],color='blue')plt.plot([eig_vector[:, 0][0], 0], [eig_vector[:, 0][1], 0], color='blue')plt.plot(rot_data[:, 0], rot_data[:, 1], '^', color='blue')plt.plot(new_data[:,0],[1.2]*10,'*',color='black')plt.show() 推荐阅读 A tutorial on Principal Components Analysis Principal Component Analysis Explained Visually]]></content>
      <categories>
        <category>Machine-Learning</category>
      </categories>
      <tags>
        <tag>Machine-Learning</tag>
        <tag>主成分分析</tag>
        <tag>pca</tag>
        <tag>降维</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python-numpy.nonzero用法]]></title>
    <url>%2F2017%2Fpython-numpy-nonzero%2F</url>
    <content type="text"><![CDATA[numpy.nonzero的功能是返回数组中所有非零元素的索引，比如在聚类分析中有这么一段更新质心位置的代码，cluster是每一行数组所属质心的索引，质心一共有k个，如何分别得到每个质心下的全部索引呢？一行nonzero(index == cent)就可以轻松解决问题。 123456# 更新质心的位置，直到簇分配结果不再改变为止for cent in range(k): index = cluster[:, 0].A # 簇分配结果中的所有索引 value = nonzero(index == cent) sample_in_cent = dataset[value[0]] centroids[cent, :] = mean(sample_in_cent, axis=0) # 按列计算均值 举个例子加深理解 123456789&gt;&gt;&gt; x = np.eye(3)&gt;&gt;&gt; xarray([[ 1., 0., 0.], [ 0., 1., 0.], [ 0., 0., 1.]])# 数组x中共有3个非零元素，nonzero则分别返回非零元素的行、列坐标# 如第1行第1列，第2行第2列，第3行第3列&gt;&gt;&gt; np.nonzero(x)(array([0, 1, 2]), array([0, 1, 2])) 当然也可以使用numpy.nonzero直接获取数组中的所有非零元素值 12345678# 返回x中所有的非零元素&gt;&gt;&gt; x[np.nonzero(x)]array([ 1., 1., 1.])# 如果觉得返回的结果不方便阅读，还可以将它转置一下&gt;&gt;&gt; np.transpose(np.nonzero(x))array([[0, 0], [1, 1], [2, 2]]) 不仅仅是快速查找数组内的非零元素，numpy.nonzero还可以用来返回特定条件下的元素索引。比如给定一个数组a，查找数组a中符合大于3条件的元素，使用numpy.nonzero则会返回所有符合条件的元素索引。 123&gt;&gt;&gt; a = np.array([[1,2,3],[4,5,6],[7,8,9]])&gt;&gt;&gt; np.nonzero(a &gt; 3)(array([1, 1, 1, 2, 2, 2]), array([0, 1, 2, 0, 1, 2])) 也可以这样用 12&gt;&gt;&gt; (a &gt; 3).nonzero()(array([1, 1, 1, 2, 2, 2]), array([0, 1, 2, 0, 1, 2])) 参考 numpy.nonzero]]></content>
      <categories>
        <category>coding</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>numpy</tag>
        <tag>nonzero</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习算法系列（9）k均值]]></title>
    <url>%2F2017%2Fmachine-learning-algorithm-series-k-means%2F</url>
    <content type="text"><![CDATA[在用户增长分析过程中有时除了需要预测用户的行为，还需要把用户细分为差异比较大的群体，然后针对不同的群体采取相应的运营手段，即用户分群。在机器学习的算法中有一个专门的领域——聚类算法，聚类是一种了解数据内在结构的方法，它通过将数据集中的样本划分为若干不相交的子集，每个子集称为一个“簇”（cluster）。如何定义簇以及簇的数量则完全人工自行决定。在先前的文章中涉及到的算法都是监督学习，现在我们开始接触到机器学习中第一个无监督学习算法——k-均值。不同于以往的数据集，无监督学习所用到的数据集都是没有标签的，也就是事先并不知道数据的内在性质及规律。 k-均值的最终目的是最小化平方误差： E = \sum_{i=1}^{k}\sum_{x\in C_i}{||x-\mu_i||}_2^2一、k-means基本思路k-均值算法的基本思路： 随机生成k个随机质心 计算样本值到质心之间的欧式距离，将样本点划分到距离最近的质心 重新更新各个簇的质心位置 重复2-3步，直到质心位置不再变化 得出输出结果 二、基于Python的k均值算法实现12345678910111213from numpy import *# 导入文件file_name = "E:/Python/mla/Ch10/testSet.txt"def load_data(file_name): data_mat = [] fr = open(file_name) for line in fr.readlines(): # 将文本文档的每一行转换成浮点型并添加到矩阵中 current_line = line.strip().split('\t') data_mat.append([float(current_line[0]), float(current_line[1])]) dataset = mat(data_mat) return datasetdataset = load_data(file_name) 123# 计算向量间欧几里得距离def cal_dist(vec_a, vec_b): return sqrt(sum(power(vec_a - vec_b, 2))) 123456789# 生成k个随机质心（centroids）def rand_centroid(dataset, k): n = shape(dataset)[1] centroids = mat(zeros((k, n))) for j in range(n): min_j = min(dataset[:, j]) range_j = float(max(dataset[:, j]) - min_j) centroids[:, j] = min_j + range_j * random.rand(k, 1) return centroids 1234567891011121314151617181920212223242526272829303132# k均值聚类算法def k_means(dataset, k): m = shape(dataset)[0] # 获取数据的行数m # 簇分配结果矩阵'cluster'包含两列： # 第一列记录簇索引值，第二列存储误差 cluster = mat(zeros((m, 2))) centroids = rand_centroid(dataset, k) # 创建一个标志变量'cluster_changed' cluster_changed = True while cluster_changed: cluster_changed = False for i in range(m): min_dist = inf min_index = -1 # 分别计算'dataset'中第i个点与k个质心之间的欧几里得距离， # 并将距离最小的质心的索引赋给簇分配结果 for j in range(k): dist_i_j = cal_dist(centroids[j, :], dataset[i, :]) if dist_i_j &lt; min_dist: min_dist = dist_i_j min_index = j if cluster[i, 0] != min_index: cluster_changed = True cluster[i, :] = min_index, min_dist**2 print(centroids) # 更新质心的位置，直到簇分配结果不再改变为止 for cent in range(k): index = cluster[:, 0].A # 簇分配结果中的所有索引 value = nonzero(index == cent) sample_in_cent = dataset[value[0]] centroids[cent, :] = mean(sample_in_cent, axis=0) # 按列计算均值 return centroids, cluster 经过四次迭代之后算法收敛，得到4个质心，四个质心以及原始数据的散点图在下图给出。 12345678910111213141516[[ 1.71514331 4.38373482] [-2.07697989 -2.34923269] [ 1.37232864 -1.69241204] [-4.4940993 -0.98212537]][[ 1.21336621 3.14825539] [-2.98849186 -3.14785829] [ 2.8692781 -2.54779119] [-3.56936853 0.80979859]][[ 1.98283629 3.1465235 ] [-3.38237045 -2.9473363 ] [ 2.80293085 -2.7315146 ] [-2.768021 2.65028438]][[ 2.6265299 3.10868015] [-3.38237045 -2.9473363 ] [ 2.80293085 -2.7315146 ] [-2.46154315 2.78737555]] 1234567891011def showCluster(dataset, k, centroids, cluster): numSamples, dim = dataset.shape mark = ['or', 'ob', 'og', 'ok'] for i in range(numSamples): markIndex = int(cluster[i, 0]) plt.plot(dataset[i, 0], dataset[i, 1], mark[markIndex]) mark = ['*r', '*b', '*g', '*k'] # 画出质心 for i in range(k): plt.plot(centroids[i, 0], centroids[i, 1], mark[i], markersize = 12) plt.show() 三、如何选择合适的K？与其说为k均值选择合适的聚类数k，倒不如说检验聚类的效果如何，因为k均值是无监督类的分析方法，所以在运用K均值算法的时候比较头疼的是我们事先无法知道最优的聚类个数。假定即使你事先确定了聚类的个数，而实际上也不一定能对应上，并且聚类的结果很大程度上依赖于分析人员喂给聚类模型的变量。在用Spark做k均值聚类时候通常有两个评估指标： 簇内误差平方之和：WSSSE（Within Set Sum of Squared Errors），Scala Spark计算损失的接口，computeCost，根据Spark文档的解释：sum of squared distances of points to their nearest center，即簇内误差平方之和是点距离它所归属的簇中心点的距离平方和，该值越小越好，表明簇类内部的样本距离越接近。 轮廓值(Silhouette score)：是聚类效果好坏的一种评价方式，轮廓系数的值是介于[-1,1]，越趋近于1代表内聚度和分离度都相对较优。轮廓系数的计算公式：$s_i = (b_i – a_i)/max(a_i,b_i)$，其中$a_i$表示簇内的聚集程度，对于第i个元素$x_i$，计算$x_i$与其同一个簇内的所有其他元素距离的平均值；选取$x_i$外的一个簇b，计算$x_i$与b中所有点的平均距离，遍历所有其他簇，找到最近的这个平均距离，记作$b_i$，用于量化簇之间分离度。计算所有x的轮廓系数，求出平均值即为当前聚类的整体轮廓系数。Spark的使用方法见：Clustering - Spark 2.4.0 Documentation 四、讨论k均值算法最大的优点就是运行速度快，在Spark上对10W条数据190个特征进行聚类分析，不到两分钟就可以跑完，速度还是非常快的，其实最主要的工作还是分析人员不断分析评估各种模型的好坏。但是它的劣势也是很明显的，比如它的区分度不是很好，对于稀疏数据的聚类，大多数的数据都集中在一块，无法剥离出来；到底K取多少需要分析人员不断尝试；另外，聚类分析需要用到什么变量也需要谨慎考虑，在聚类分析中我们应该使用与问题紧密相关的变量做聚类而不是使用所有的变量作为输入。 对异常值敏感，因为在计算各个簇质心位置时，异常点往往会使得结果变得扭曲 在对事物没有一定了解的前提下事先难以确定k值 算法的时间复杂度较高，在处理大数据时是相对可伸缩和有效的，通常需要使用合适规模的样本 只能发现球状簇，对不规则的簇效果一般 解决异常值敏感可采用k中心算法，k中心点算法中，每次迭代后的质点都是从聚类的样本点中选取，而选取的标准就是当该样本点成为新的质点后能提高类簇的聚类质量，使得类簇更紧凑。该算法使用绝对误差标准来定义一个类簇的紧凑程度。另外的优化手段还有凝聚的与分裂的层次聚类。 参考 腾讯QQ大数据：用户增长分析——用户分群分析 Clustering - Spark 2.4.0 Documentation]]></content>
      <categories>
        <category>Machine-Learning</category>
      </categories>
      <tags>
        <tag>机器学习算法系列</tag>
        <tag>Machine-Learning</tag>
        <tag>无监督学习</tag>
        <tag>聚类</tag>
        <tag>k均值</tag>
        <tag>kmeans</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[历年（1954-2016）政府工作报告文本分析及可视化]]></title>
    <url>%2F2017%2FChinese-government-report-text-analysis-over-the-years%2F</url>
    <content type="text"><![CDATA[本文的语料库（Corpus）来源于中华人民共和国中央人民政府网站 ，时间跨度为1954年到2016年，以文本挖掘和复杂网络理论为基础，针对该语料库做了一点简单的NLP工作。 本项目github地址：https://github.com/swordspoet/government_report_analysis **警告：本文版权归Thinking Realm作者李彬所有！！！严禁以任何形式抄袭或转载！！！违者必究！！！** 一、文本内容抓取网络爬虫（Web Crawler）技术是从网页中抽取可用数据的方式，广泛运用于大规模从网络中提取信息。本文使用Python，通过编写爬虫程序 ，以政府工作报告全文为检索目标，在中华人民共和国中央人民政府网站自动爬取了1954-2016年政府工作报告全文的文本数据，截止2016年12月27日，除掉缺失年份，其中剩余有效年份为48年。 爬虫代码如下： 12345678910111213141516171819202122232425262728293031#===================================================================# 抓取报告正文部分、词频统计#===================================================================# 建立一个空的字典用于存储网页数据，如标题、网页地址等；# 从历年国务院政府工作报告（1954年至2013年）http://www.gov.cn/test/2006-02/16/content_200719.htm中过滤出历年报告的链接并返回到'ReportLinks'url = 'http://www.gov.cn/guowuyuan/2006-02/16/content_2616810.htm'Report_Links = dict()# 过滤出页面中的网页链接def filterReportLinks(url): print '过滤1954-2016年政府工作报告网址······' html_doc = urllib2.urlopen(url).read() standard_html = BeautifulSoup(html_doc, "html.parser") for link in standard_html.find_all('a', href = re.compile(r'/content_')): url = link.get('href') year = link.get_text() Report_Links[year] = url return Report_Links# 提取网页（URL）文本def extract_text(url): """Extract html content.""" page_source = requests.get(url).content bs_source = BeautifulSoup(page_source) report_text = bs_source.find_all('p') text = '' for p in report_text: text += p.get_text() text += '\n' return text 二、报告文本预处理采用结巴分词系统对48年的全文数据进行分词处理。结巴分词系统在Python中有现成的组件，对文本（Text）进行分词处理时，需要去除停用词和标点符号，停用词是指那些对文本的影响可以忽略不计的，不包含有任何实质性内容的词语，如“的”、“得”、“地”，在这里我们直接从分词完毕后的文本中筛选出长度大于2的字符串就达到期望的效果，得到名为fil_seglist的变量，然后用空格拼接过滤掉停用词和标点符号的fil_seglist变量。如“必须全面深化改革，坚持和完善基本经济制度，建立现代产权制度”一段文本，经过预处理之后，处理之后的文本为“必须 全面 深化改革 坚持 完善 基本 经济 制度 建立 现代 产权制度”。 三、构建词同现网络词同现网络的构造思路比较简单，每一个词语对应网络中的一个节点（node），而同现词组之间的连接则成为连接节点的边（edge）。也就是说，如果在一个句子中，两个词在n阶Markov链的条件下存在同现关系，则可认为网络中的两个节点之间存在一个连接。实践表明，在自然语言处理中，n阶Markov链的n取2比较合适，因为句子中两个词相邻是最常见的，如“发展农业”的“发展”和“农业”，“补贴农村”的“补贴”和“农村”，n太大（两个词在一句话中距离太远）则会引入大量的无关词，导致同现网络准确性的降低，另一方面，又可以使模型的复杂度得到有效的控制。 构建词同现网络代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647#===================================================================# 同现词统计并封装在CSV文件中#===================================================================#针对每年报告文本结巴分词之后得到的seg_list进行“同现词”统计def build_co_network(seg_list): co_net = defaultdict(lambda: defaultdict(int)) for i in range(len(seg_list)-1): for j in range(i+1, i+2): w1 = seg_list[i] w2 = seg_list[j] if w1 != w2: co_net[w1][w2] += 1 return co_net#“同现词”统计之后，我们将统计的结果（Top 100）返回到“terms_max”中def get_co_terms(co_net): com_max = [] for t1 in co_net: t1_max_terms = sorted(co_net[t1].items(), key=operator.itemgetter(1), reverse=True) for t2, t2_count in t1_max_terms: com_max.append(((t1, t2), t2_count)) terms_max = sorted(com_max, key=operator.itemgetter(1), reverse=True) return terms_max#针对terms_max，将s2,s3,s4写入CSV文件中 def get_co_network(): for (year, link) in Report_Links.items(): #year = str(year) text = extract_text(link) seg_list = jiebait(text) co_net = build_co_network(seg_list) terms_max = get_co_terms(co_net) for s1 in terms_max: s2 = s1[0][0] s3 = s1[0][1] s4 = s1[1] s2 = s2.encode('utf-8') #s2和s3以Unicode编码存在，故转码为‘utf8’ s3 = s3.encode('utf-8') s4 = str(s4) print year, s4 terms = open('terms5.csv', 'ab') terms.write(year + '\t') terms.write(s2 + '\t') terms.write(s3 + '\t') terms.write(s4 + '\n') terms.close()get_co_network() 运行上述代码，本文对语料库分词之后得到的自定义字典进行同现词组的频率统计，得到了1954至2016年语料库中总计273592组共现词组的频率统计数据，包含19468个网络节点和170889条边。表 1截取了统计数据中1954年和2016年的前5行数据，其中Frequency表示某年（Year）中词1（Word1）与词2（Word2）在字典中前后相邻的次数统计，比如2016年政府工作报告中，“经济”和“发展”两个词前后相邻的频率为11次，“国内”和“生产总值”前后相邻出现的频率为10次。 Year Word1 Word2 Frequency 1954 等于 一九四九年 14 1954 我们 国家 14 1954 经济 建设 13 1954 全国 人民 10 1954 日内瓦 会议 10 … … … … 2016 经济 发展 11 2016 深入 推进 10 2016 国内 生产总值 10 2016 结构性 改革 10 2016 中国 特色 9 四、可视化分析在这一节里，我们主要围绕“农业”、“农村”、“农民”三个主题词进行文本挖掘的统计指标分析、可视化展示等工作。 主题词词频分析词频分析法是利用能够揭示或表达文本核心内容的关键词或主题词在某一研究领域文献中出现的频次高低来确定该领域研究热点和发展动向的文献计量方法，广泛应用于各学科领域。我们在语料库中选取“农业”、“农村”、“农民”3个主题词并对其进行词频统计分析，统计各年政府工作报告中主题词出现频率，具体见图1。 可以发现词频统计图的起伏涨落对应着同时期的国家对于三农战略或政策的变化：由于国内阶级斗争，政府工作会议曾停滞达10之久，从而20世纪60年代至80年代期间统计数据存在一个明显的缺口；在建国之初制定了“一化三改”过渡时期总路线，国家对重视农业十分重视，如在1954年政府工作报告的文本中，“农业”一词出现的频率就达154次，“农村”一词出现频率为19次，“农民”一词出现频率为61次；1990年至2000年国家对“三农”的重视程度处于比较平稳的状态，而2008年至2009年间爆发的全球金融危机使得政府在“三农”问题上的关注有所降低，在2009年的政府工作报告文本中，“农业”、“农村”、“农民”三个主题词出现频率分别只有3次、9次和5次，为改革开放以来的最低水平。 主题词网络统计特性分析平均路径长度、聚集系数和度分布为复杂网络的三大统计特性[29]，它们比较系统地反映了网络中的节点位置、连接状况、密度、节点间路径远近等各类结构特征。 Gephi是一款优秀的开源网络分析领域的数据可视化处理软件，本文主要通过Gephi 0.9.1软件对共现网络进行可视化及主要参数的计算工作。 Gephi软件有几个重要的计算参数，分别是度中心性（Degree centrality）、紧密中心性（Closeness centrality）、平均聚合系数（Average clustering coefficient）。度中心性指节点的度数, 适用于对局部网络节点的中心地位和影响力进行刻画，度中心性关注单个节点；紧密中心性用于度量网络中节点对于中心的紧密程度，是刻画节点通过网络到达其他节点难易程度的指标,节点的紧密度越高，则离其他节点越近，传递信息的难度也就越低，相比节点度指标更能反映网络的全局结构。 网络 节点数 边数 网络直径 平均度 平均最短路径 平均聚合系数 共现词组 19468 170889 16 17.566 3.278 0.210 小世界特性。节点网络平均最短路径为3.278，这说明在该词同现网络中的任意两个节点，只用少于4个节点就可以将他们连接起来，具有明显的小世界效应。也就是说，虽然汉语中有大量的词汇，但是人们在语言网络中，只需经过有限路径就可以从一个词迅速跳到另外一个词，并确保语义的完整，从而保证了交流表达的速度。 节点度分布。节点网络的度分布满足幂率分布，显示了无标度特性。节点度分布如图1所示，少数节点出现频繁，多数节点出现频率较低。 在共现词组列表中，以包含“农”字符串为检索条件，筛选出字段中含有“农”的词组，如“工农业”、“农村居民”、“农产品”等字段也统计在内，共计5990对。为了便于观察，将年份（Year）一列剔除，X轴设为Word1列，Y轴设为Word2列，Z轴设为Frequency列，得散点图如图 2所示，可以看出，频率计数最高的共现词组为“农业—生产”、“农业—发展”，其余大多数共现词组的频率低于2次。 我们将筛选出来的字段数据导入Gephi软件中，并利用Convert Excel and csv files to networks插件进行网络中心度计算。经计算，该网络中一共包含1832个节点和3098条边，网络的平均聚集系数为0.302，模块化系数为0.475（系数大于0.4则表明网络有良好的模块化结构），各节点模块化系数分布如图 3所示， 然后，我们选取“农业”、“农村”、“农民”三个主题词，选取度、紧密中心性、平均聚集系数三个指标，计算结果如 表 3所示，三个主题词节点的紧密中心性均在0.5左右，节点连接很紧密，说明主题词节点之间沟通良好，信息可以快速在节点之间传播。 主题词 度 紧密中心性 平均聚集系数 农业 685 0.552 0.002 农村 591 0.519 0.002 农民 418 0.493 0.003 基于Force Atlas算法生成的三农主题词关系图谱如图 4所示，可以看出关于主题词群体间界限十分明晰，在这些群体中，“农业”、“农村”、“农民”三个群体占据了核心位置，“农民工”、“农产品”、“农民收入”等分别占据次核心位置，并充当三个群体沟通连接的介中心（Between centrality），进一步印证了共现网络具有良好模块化的结论。 五、 结论本文从共生模式角度切入，构建词同现网络以分析农业、农村、农民三者之间的共生关系。共生模式主要有寄生、偏利共生、非对称性互惠共生和对称互惠共生几种，关于共生模式的判别，国内外的学者主要通过Logistic方程、共生度、共生界面等方法进行分析。 我们以连通主题词节点个数作为判别共生模式的主要指标，即共生度。如三个主题词分别通过“收入”、“经济”、“改革”、“城市”等中间节点充当物质、信息和能量传导的媒介、通道或载体，建立共生关系，节点数目的多少可以反映主题词之间的共生强度，从而判断共生模式就具备了逻辑上的可能性。从“农业-农村”、“农村-农民”和“农业-农民”三组主题词词组的共生关系的可视化图（见图 5、图 6、图 7）可以看出，大量中间点如发展、经济、城镇等充当沟通媒介，主题词之间联系相当紧密，由此我们可以判断农业、农村、农民三个主题词之间存在互惠共生的关系。但是紧密的程度却存在差别，农业和农村的沟通最为密切，共计有265个节点，与此同时，农业与农民之间有195个节点，农民与农村次之，有114个节点。由此可以进一步判断主题词之间处于非对称性互惠共生关系。 综上，根据模块化系数、紧密中心度和主题词关系图谱以及主题词关系子图谱，我们可以发现三农问题的组成部分之间存在非对称互惠共生的模式。 **警告：本文版权归Thinking Realm作者李彬所有！！！严禁以任何形式抄袭或转载！！！违者必究！！！**]]></content>
      <categories>
        <category>Machine-Learning</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>可视化</tag>
        <tag>文本挖掘</tag>
        <tag>文本分析</tag>
        <tag>词同现</tag>
        <tag>复杂网络</tag>
        <tag>爬虫</tag>
        <tag>自然语言处理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SQLite 数据库基础教程]]></title>
    <url>%2F2017%2Fsqlite3-basic-tutorial%2F</url>
    <content type="text"><![CDATA[目录 SQLite 创建数据库 SQLite Like 子句 SQLite Glob 子句 SQLite Limit 子句 SQLite Distinct 关键字 SQLite JOIN 子句 交叉连接 - CROSS JOIN 内连接 - INNER JOIN 外连接 - OUTER JOIN SQLite Having 子句 SQLite Delete 语句 一、SQLite 创建数据库SQLite 的 sqlite3 命令被用来创建新的 SQLite 数据库。您不需要任何特殊的权限即可创建一个数据。打开cmd，输入 12345sqlite3 student.db/*在当前目录下创建名为student的数据库，数据库创建成功后会弹出一个 sqlite&gt; 的提示符*/ SQLite 创建表的语法为 12CREATE TABLE table_name (); 这时数据库student.db还是空的，我们在数据库中新建三个表：course、sc和student，在当前目录下新建sql_scripts.txt文本文档，并将下列代码写入文本文档，然后在sqlite&gt;的提示符后输入.read data.txt加载，则三个表在student.db中新建完毕。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556PRAGMA foreign_keys=OFF;BEGIN TRANSACTION;/*创建course表*/CREATE TABLE course ( cno char(2) NOT NULL, --课程编号，不能为空 cname char(20), --课程名 cpno char(2), -- ccredit char(2), --课程学分 primary KEY (cno) --设置课程编号为主键);INSERT INTO "course" VALUES('1','数据库','5','4');INSERT INTO "course" VALUES('2','数学','','2');INSERT INTO "course" VALUES('3','信息系统','1','4');INSERT INTO "course" VALUES('4','操作系统','6','3');INSERT INTO "course" VALUES('5','数据结构','7','4');INSERT INTO "course" VALUES('6','数据处理','','2');INSERT INTO "course" VALUES('7','PASCAL语言','6','4');/*创建student表*/CREATE TABLE student(sno char(5) not null,sname char(20),ssex char(2),sage int,sdept char(20));INSERT INTO "student" VALUES('95001','李勇','男',20,'CS');INSERT INTO "student" VALUES('95002','刘晨','女',19,'IS');INSERT INTO "student" VALUES('95003','王敏','女',18,'MA');INSERT INTO "student" VALUES('95004','张立','男',19,'IS');INSERT INTO "student" VALUES('95005','王强','男',17,'IS');INSERT INTO "student" VALUES('95015','张三','男',20,'CS');INSERT INTO "student" VALUES('95019','李四','男',20,'CS');INSERT INTO "student" VALUES('95020','陈冬','男',18,'IS');/*创建sc表（学生课程成绩）*/CREATE TABLE sc(sno char(5), --学号cno char(2), --课程编号grade int, --成绩primary key(sno,cno) --主键设置为学号和课程编号);INSERT INTO "sc" VALUES('95001','1',65);INSERT INTO "sc" VALUES('95001','2',88);INSERT INTO "sc" VALUES('95001','3',57);INSERT INTO "sc" VALUES('95001','4',79);INSERT INTO "sc" VALUES('95001','5',45);INSERT INTO "sc" VALUES('95001','6',90);INSERT INTO "sc" VALUES('95001','7',81);INSERT INTO "sc" VALUES('95002','2',90);INSERT INTO "sc" VALUES('95002','3',80);INSERT INTO "sc" VALUES('95002','4',55);INSERT INTO "sc" VALUES('95003','2',0);INSERT INTO "sc" VALUES('95019','2',66);INSERT INTO "sc" VALUES('95020','1',NULL);COMMIT;/*COMMIT用来将事务写入数据库*/ 然后，使用SELECT查询语句可以查看各个表中的内容 1234567891011121314151617181920212223242526272829303132333435sqlite&gt; select * ...&gt; from student;95001|李勇|男|20|CS95002|刘晨|女|19|IS95003|王敏|女|18|MA95004|张立|男|19|IS95005|王强|男|17|IS95015|张三|男|20|CS95019|李四|男|20|CS95020|陈冬|男|18|ISsqlite&gt; select * ...&gt; from course;1|数据库|5|42|数学||23|信息系统|1|44|操作系统|6|35|数据结构|7|46|数据处理||27|PASCAL语言|6|4sqlite&gt; select * ...&gt; from sc;95001|1|6595001|2|8895001|3|5795001|4|7995001|5|4595001|6|9095001|7|8195002|2|9095002|3|8095002|4|5595003|2|095019|2|6695020|1|sqlite&gt; 二、QLite Like 子句SQLite 的 LIKE 运算符是用来匹配通配符指定模式的文本值。如果搜索表达式与模式表达式匹配，LIKE 运算符将返回真（true），也就是 1。这里有两个通配符与 LIKE 运算符一起使用： -百分号 （%）代表零个、一个或多个数字或字符 -下划线 （_）代表一个单一的数字或字符，这些符号可以被组合使用 比如，查找student表中，所有姓李的学生 12345sqlite&gt; SELECT sname ...&gt; from student ...&gt; WHERE sname LIKE '李%';李勇李四 语句 描述 WHERE SALARY LIKE ‘200%’ 查找以 200 开头的任意值 WHERE SALARY LIKE ‘%200%’ 查找任意位置包含 200 的任意值 WHERE SALARY LIKE ‘_00%’ 查找第二位和第三位为 00 的任意值 WHERE SALARY LIKE ‘2%%’ 查找以 2 开头，且长度至少为 3 个字符的任意值 WHERE SALARY LIKE ‘%2’ 查找以 2 结尾的任意值 WHERE SALARY LIKE ‘_2%3’ 查找第二位为 2，且以 3 结尾的任意值 WHERE SALARY LIKE ‘2___3’ 查找长度为 5 位数，且以 2 开头以 3 结尾的任意值 三、SQLite Glob 子句SQLite 的 GLOB 运算符是用来匹配通配符指定模式的文本值。如果搜索表达式与模式表达式匹配，GLOB 运算符将返回真（true），也就是 1。与 LIKE 运算符不同的是，GLOB 是大小写敏感的，对于下面的通配符，它遵循 UNIX 的语法。 -星号 （*）代表零个、一个或多个数字或字符 -问号 （?）代表一个单一的数字或字符 语句 描述 WHERE SALARY GLOB ‘200*’ 查找以 200 开头的任意值 WHERE SALARY GLOB ‘200‘ 查找任意位置包含 200 的任意值 WHERE SALARY GLOB ‘?00*’ 查找第二位和第三位为 00 的任意值 WHERE SALARY GLOB ‘2??’ 查找以 2 开头，且长度至少为 3 个字符的任意值 WHERE SALARY GLOB ‘*2’ 查找以 2 结尾的任意值 WHERE SALARY GLOB ‘?2*3’ 查找第二位为 2，且以 3 结尾的任意值 WHERE SALARY GLOB ‘2???3’ 查找长度为 5 位数，且以 2 开头以 3 结尾的任意值 四、SQLite Limit 子句SQLite 的 LIMIT 子句用于限制由 SELECT 语句返回的数据数量。带有 LIMIT 子句的 SELECT 语句的基本语法如下： 123SELECT column1, column2, columnN FROM table_nameLIMIT [no of rows] 下面是 LIMIT 子句与 OFFSET 子句一起使用时的语法： 123SELECT column1, column2, columnN FROM table_nameLIMIT [no of rows] OFFSET [row num] SQLite 引擎将返回从下一行开始直到给定的 OFFSET 为止的所有行，如下一个实例所示 123456789101112sqlite&gt; SELECT * ...&gt; FROM student ...&gt; LIMIT 3;95001|李勇|男|20|CS95002|刘晨|女|19|IS95003|王敏|女|18|MAsqlite&gt; SELECT grade ...&gt; FROM sc ...&gt; LIMIT 3 OFFSET 2;577945 五、SQLite Distinct 关键字SQLite 的 DISTINCT 关键字与 SELECT 语句一起使用，来消除所有重复的记录，并只获取唯一一次记录。 有可能出现一种情况，在一个表中有多个重复的记录。当提取这样的记录时，DISTINCT 关键字就显得特别有意义，它只获取唯一一次记录，而不是获取重复记录。 12345678910sqlite&gt; SELECT ssex ...&gt; FROM student;男女女男男男男男 用于消除重复记录的 DISTINCT 关键字的基本语法如下： 123SELECT DISTINCT column1, column2,.....columnN FROM table_nameWHERE [condition] 1234sqlite&gt; SELECT DISTINCT ssex ...&gt; FROM student;男女 六、SQLite JOIN 子句SQLite 的 Joins 子句用于结合两个或多个数据库中表的记录。JOIN 是一种通过共同值来结合两个表中字段的手段。 SQL 定义了三种主要类型的连接： 交叉连接 CROSS JOIN 内连接 INNER JOIN 外连接 OUTER JOIN 交叉连接 - CROSS JOIN交叉连接（CROSS JOIN）把第一个表的每一行与第二个表的每一行进行匹配。如果两个输入表分别有 x 和 y 列，则结果表有 x+y 列。由于交叉连接（CROSS JOIN）有可能产生非常大的表，使用时必须谨慎，只在适当的时候使用它们。 下面是交叉连接（CROSS JOIN）的语法： 1SELECT ... FROM table1 CROSS JOIN table2 ... 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960/*student表中的sname和course表中的cname cross join*/sqlite&gt; SELECT sname,cname ...&gt; FROM student ...&gt; CROSS JOIN course;李勇|数据库李勇|数学李勇|信息系统李勇|操作系统李勇|数据结构李勇|数据处理李勇|PASCAL语言刘晨|数据库刘晨|数学刘晨|信息系统刘晨|操作系统刘晨|数据结构刘晨|数据处理刘晨|PASCAL语言王敏|数据库王敏|数学王敏|信息系统王敏|操作系统王敏|数据结构王敏|数据处理王敏|PASCAL语言张立|数据库张立|数学张立|信息系统张立|操作系统张立|数据结构张立|数据处理张立|PASCAL语言王强|数据库王强|数学王强|信息系统王强|操作系统王强|数据结构王强|数据处理王强|PASCAL语言张三|数据库张三|数学张三|信息系统张三|操作系统张三|数据结构张三|数据处理张三|PASCAL语言李四|数据库李四|数学李四|信息系统李四|操作系统李四|数据结构李四|数据处理李四|PASCAL语言陈冬|数据库陈冬|数学陈冬|信息系统陈冬|操作系统陈冬|数据结构陈冬|数据处理陈冬|PASCAL语言 内连接 - INNER JOIN内连接（INNER JOIN）根据连接条件结合两个表（table1 和 table2）的列值来创建一个新的结果表。查询会把 table1 中的每一行与 table2 中的每一行进行比较，找到所有满足连接条件的行的匹配对。当满足连接谓词时，A 和 B 行的每个匹配对的列值会合并成一个结果行。 内连接（INNER JOIN）是最常见的连接类型，是默认的连接类型。INNER 关键字是可选的。 下面是内连接（INNER JOIN）的语法：1SELECT ... FROM table1 [INNER] JOIN table2 ON conditional_expression ... 比如要查询有选修课学生的考试成绩（有的有选修课，有的没有） 1234567891011121314151617sqlite&gt; SELECT student.sno,sname,grade ...&gt; FROM student ...&gt; INNER JOIN sc ...&gt; ON student.sno = sc.sno;95001|李勇|6595001|李勇|8895001|李勇|5795001|李勇|7995001|李勇|4595001|李勇|9095001|李勇|8195002|刘晨|9095002|刘晨|8095002|刘晨|5595003|王敏|095019|李四|6695020|陈冬| 如果还需要查找得到该学生的选修课程名，那么就需要连接多个表，还用到了AND运算符 12345678910111213141516sqlite&gt; SELECT sname,student.sno,cname,grade ...&gt; FROM sc INNER JOIN student,course ...&gt; ON sc.cno = course.cno AND student.sno = sc.sno;李勇|95001|数据库|65李勇|95001|数学|88李勇|95001|信息系统|57李勇|95001|操作系统|79李勇|95001|数据结构|45李勇|95001|数据处理|90李勇|95001|PASCAL语言|81刘晨|95002|数学|90刘晨|95002|信息系统|80刘晨|95002|操作系统|55王敏|95003|数学|0李四|95019|数学|66陈冬|95020|数据库| 外连接 - OUTER JOIN外连接（OUTER JOIN）是内连接（INNER JOIN）的扩展。虽然 SQL 标准定义了三种类型的外连接：LEFT、RIGHT、FULL，但 SQLite 只支持 左外连接（LEFT OUTER JOIN）。 外连接（OUTER JOIN）声明条件的方法与内连接（INNER JOIN）是相同的，使用 ON、USING 或 NATURAL 关键字来表达。最初的结果表以相同的方式进行计算。一旦主连接计算完成，外连接（OUTER JOIN）将从一个或两个表中任何未连接的行合并进来（有的时候需要包含没有关联行的那些行），外连接的列使用 NULL 值，将它们附加到结果表中。 下面是左外连接（LEFT OUTER JOIN）的语法： 1SELECT ... FROM table1 LEFT OUTER JOIN table2 ON conditional_expression ... 左外连接（LEFT OUTER JOIN）的左边的表是参照的主体，即根据连接条件，除了返回主体表与非主体中连接成功的行外，还返回主体表与非主体表连接不成功的记录。所以，左外连接中表的连接先后会导致不同的结果。 12345678910111213141516171819sqlite&gt; SELECT student.sno,sname,grade ...&gt; FROM student LEFT OUTER JOIN sc ...&gt; ON student.sno = sc.sno;95001|李勇|6595001|李勇|8895001|李勇|5795001|李勇|7995001|李勇|4595001|李勇|9095001|李勇|8195002|刘晨|9095002|刘晨|8095002|刘晨|5595003|王敏|095004|张立|95005|王强|95015|张三|95019|李四|6695020|陈冬| 12345678910111213141516sqlite&gt; SELECT student.sno,sname,grade ...&gt; FROM sc LEFT OUTER JOIN student ...&gt; ON student.sno = sc.sno;95001|李勇|6595001|李勇|8895001|李勇|5795001|李勇|7995001|李勇|4595001|李勇|9095001|李勇|8195002|刘晨|9095002|刘晨|8095002|刘晨|5595003|王敏|095019|李四|6695020|陈冬| 七、SQLite Having 子句HAVING 子句允许指定条件来过滤将出现在最终结果中的分组结果。 WHERE 子句在所选列上设置条件，而 HAVING 子句则在由 GROUP BY 子句创建的分组上设置条件。 有必要说清楚分组GROUP BY与排序ORDER BY的区别，分组是将输出的结果以分组的形式呈现，而排序则是让输出结果按照顺序进行排列，换言之，分组的结果不一定是按照顺序排列的。where和having的功能也差不多，区别在于where是在数据分组前进行过滤，而having是在数据分组后进行过滤的。 下面是 HAVING 子句在 SELECT 查询中的位置： 123456SELECTFROMWHEREGROUP BYHAVINGORDER BY 1234567sqlite&gt; SELECT * ...&gt; FROM student ...&gt; GROUP BY sage ...&gt; HAVING sage &gt; 18 ...&gt; ORDER BY sdept;95019|李四|男|20|CS95004|张立|男|19|IS 八、SQLite Delete 语句SQLite 的 DELETE 查询用于删除表中已有的记录。可以使用带有 WHERE 子句的 DELETE 查询来删除选定行，否则所有的记录都会被删除。 带有 WHERE 子句的 DELETE 查询的基本语法如下： 12DELETE FROM table_nameWHERE [condition];]]></content>
      <categories>
        <category>coding</category>
      </categories>
      <tags>
        <tag>SQL</tag>
        <tag>SQLite</tag>
        <tag>数据库</tag>
        <tag>基础教程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[五步完美实现Win10 + Ubuntu 17.04双系统启动]]></title>
    <url>%2F2017%2Ffive-steps-win10-ubuntu-dual-boot%2F</url>
    <content type="text"><![CDATA[网络上各种双系统安装的教程良莠不齐，大多相互抄袭，很容易让人对Linux“敬而远之”，其实并没有那么难，本文去繁从简，简单而又清晰地介绍如何在Windows 10下五个步骤完美实现 Win10 + Ubuntu 17.04双系统启动。 需要注意一点的是，现在大多数计算机都拥有固态和机械两块硬盘，Windows一般安装在固态硬盘中，所以系统启动的时候会在固态硬盘中寻找启动项。笔者的Windows系统安装在固态硬盘里，但是我在安装的时候却把Ubuntu的磁盘分区划在机械硬盘上，导致安装成功后仍旧进入不了Ubuntu。所以，最好还是在Windows系统所在硬盘下给Ubuntu划分磁盘空间，至于其中的原因，我也不是特别清楚。 怎么理解双系统？怎么安装？你的计算机好比是你的房子，房子有不同的装修风格，也就是不同的操作系统。安装双系统就好比把暂时没有使用的空间“租”给别人，但同时又不影响自己的使用，那么就得腾出一间房间出来（分配磁盘空间），然后把想装的系统安装到这个空间，再把大门的钥匙（用easybcd添加新的启动项）给“租”房子的人，两个系统就可以互不影响了。 1、分出空闲空间：首先，在现有计算机下划出一个专门给Ubuntu的磁盘分区，推荐Windows自带的磁盘管理工具。快捷键“win+X”-&gt;“磁盘管理”-&gt;“压缩卷”（Windows安装在固态硬盘则压缩固态硬盘，机械硬盘同理），压缩完之后会多出了一块“未分配空间”。 2、下载一个Ubuntu的最新系统，现在有LTS 16.04版本的了，在国内建议去网易开源镜像站下载对应32位或64位安装包。 3、软碟通，制作光盘映像文件，下载地址UltraISO - EZB Systems, Inc，这是用于将ISO系统文件刻录成启动盘的工具，你需要准备一个U盘，写入方式：USB-ZIP+。教程参见：使用UltraISO制作U盘启动盘。U盘启动盘做好之后，你还需要知道如何在重启的时候进入boot menu选择界面，视具体情况而定，下面总结了常见启动菜单快捷键 主板BOOTMENU： 快捷键 品牌电脑BOOTMENU: 快捷键 华硕主板 F8 东芝笔记本 F12 映泰主板 F9 华硕笔记本 F8 七彩虹主板 F11 惠普电脑 F9 微星主板 F11 联想笔记本 F10 技嘉主板 F12 联想台式机 F12 富士康主板 ESC 清华同方台式机 F12 梅捷主板 ESC DELL电脑 F12 4、关闭计算机，插入已经做成启动盘的U盘，拔掉网线（Ubuntu在安装的同时会安装其他软件，影响安装速度），并重启计算机（笔者的台式计算机是微星主板，按F11进入boot menu）。开始安装Ubuntu系统，系统安装是图形化的界面，选择“简体中文”语言。软件暂时不要安装，继续，当出现要手动分盘符的时候，请选择底端的“其它选项”。 5、给Ubuntu分区 大多数习惯了Windows的用户，对Linux的树型结构目录不是十分了解，但是不了解就不代表不会用。对于初级用户，一切从简，给Ubuntu四个分区，分别是根目录’/‘、交换空间swap、引导分区目录’/boot’和home目录’/home’。根目录主要存储系统、软件还有一些其他文件，所有的文件都是由根目录衍生来的，所以这个分区尽可能大一点，至于具体如何规划这个分区不是现在需要考虑的。交换空间swap又叫做虚拟物理内存，它的功能计算机内存不太够用的时候存储那些不经常被CPU使用的程序，如果你的系统不是很忙，这个分区可以不需要。打个比方来理解引导分区目录，引导分区目录是摆在计算机开机时选择进入何种系统的“大门”，没有这张“大门”，就算有了“钥匙”也没有用。home目录的作用理解为Linux各个用户下共享的文件夹。 对20G未分配空间进行分区操作，分配4个区间 分区 操作 第一次分区 “空闲”处点“+”，进行如下设置：挂载点：“/”大小：5120MB新分区的类型：主分区新分区的位置：空间起始位置用于：EXT4日志文件系统 第二次分区 “空闲”处，继续点“+”，如下设置，挂载点：（不设置）大小：2048MB新分区的类型：逻辑分区新分区的位置：空间起始位置用于：交换空间 第三次分区 “空闲”处，继续点“+”，如下设置，挂载点：/boot （网上有的说不需要设置这项，但双系统引导时需要，先不要去理解这些）大小：200MB新分区的类型：逻辑分区新分区的位置：空间起始位置用于：EXT4日志文件系统 第四次分区 “空闲”处，继续点“+”，如下设置，挂载点：/home 大小：剩余全部空间，剩下显示多少，就多少新分区的类型：逻辑分区新分区的位置：空间起始位置用于：EXT4日志文件系统 第四次分区完成后请不要马上点“安装”！！！第四次分区完成后请不要马上点“安装”！！！第四次分区完成后请不要马上点“安装”！！！ 记得在“安装启动引导器的设备：”选项中选择/boot所在盘符，这里的是/dev/sda9，至此，点击“安装”，一直等到出现安装完成重启的提示，重启就好了，然后你会发现，重启之后仍然是Windows，还有最后一步。 5、安装 EasyBCD 2.3，这个软件是用于系统配置创建多重启动系统的引导文件，也就是新创建一个启动文件，可以让你的电脑在启动的时候，有进入何种系统的选择。“添加新条目” -&gt;“Linux/BSD”-&gt;类型“Grub (Legacy)”驱动器，选择“驱动器”-&gt;“添加条目”，保存后重启计算机。 这时计算机便有了两个启动选项：Windows和NeoSmart Linux，选择后者，大功告成。 参考链接 Linux 公社：Ubuntu 16.04 U盘安装图文教程 百度经验：win7+ubuntu 13.04双系统安装方法 继续阅读本站其他精彩文章，请 Ctrl+D 收藏！ 机器学习 编程语言 技术碎碎念 读书笔记]]></content>
      <categories>
        <category>教程</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Ubuntu</tag>
        <tag>双系统安装</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[处理不平衡数据——基于UCI人口调查数据集（二）]]></title>
    <url>%2F2017%2FR-imbalanced-data-2%2F</url>
    <content type="text"><![CDATA[本文是处理不平衡数据系列之二，在上一篇文章中，我们完成了对数据的预处理、可视化以及模型训练与预测等等工作，对数据有了整体的认识。在对实验数据进行预处理的时候，缺失值（missing values）和高相关性变量（variables with high correlation）是重点关注的对象。解决了这两个问题后，数据集样本不平衡的缺陷仍旧没有根除，所以针对数据分别进行了上采样、下采样以及SMOTE三种采样方法。显然，采样花费时间最久的SMOTE在模型中表现最佳，拿到了最高的准确率0.896，可是正当准备庆祝的时候，一个不幸的“消息”告诉我们：特异度（Specificity）只有0.254。也就是说，模型对预测收入高于5w的少数人群（minority class）表现不太好，这样的模型结果是不太令人满意的，能够拿到0.896的准确率自然也是在情理之中，毕竟正反样本的比例（96:4）摆在那里。为了克服这个缺陷，我们在R语言中采用了高效、性能强大的xgboost处理框架，最终得到理想的数据。 说句题外话，原本计划完成任务需花费10个番茄，实际耗时远远多出了预期的1倍多，整个五一就窝在实验室了。经过这个小小的项目后，深感“单兵作战”孤立无援的苦楚，唯有不断google，不断将写好的代码推倒重来，不断input、output······ 本项目github地址：https://github.com/swordspoet/UCI_imbalanced_data 一、初次尝试xgboost xgboost模型参数解释与设定 二、xgboost in mlr 数据预处理 调参与模型训练 参考链接 一、初次尝试xgboost为了训练出一个能够在预测正负样本表现均良好的模型，在这篇文章中我们会用到xgboost算法，xgboost(eXtreme Gradient Boosting)的作者是华盛顿大学的陈天奇，xgboost最大的特点在于，它能够自动利用CPU的多线程进行并行计算，同时在算法上加以改进提高了精度。随着越来越多的队伍借助xgboost取得了kaggle比赛中的胜利，xgboost在近年来变得十分流行。 xgboost不仅被封装成了Python库，何通制作了xgboost工具的R语言接口，所以R中安装后便可以直接使用。 照例，我们先对数据进行预处理，预处理的思路是：分别考虑训练集、测试集中的数值型变量和类别型变量，对数值型，剔除高度相关的变量，对类别型，剔除数据遗漏严重的变量。经过上述两个步骤后，再将数值型和类别型变量重新组合。因为R对内存的要求太苛刻了，完成数据预处理后，还需要将train,test,num_train,num_test,cat_train,cat_test从RStudio中移除以减少内存占用，不然会出现内存不够的情况。以笔者8G内存的台式机器为例，本次实验中CPU与内存满负荷运算是常事，还出现几次假死、崩溃的情况，所以在R中进行数据处理的时候一定要注意内存的管理。 123456789101112131415161718192021222324252627# 加载包library(caret)library(data.table)library(xgboost)# 加载数据集train &lt;- fread("E:/R/imbalancedata/train.csv",na.string=c(""," ","?","NA",NA)) test &lt;- fread("E:/R/imbalancedata/test.csv",na.string=c(""," ","?","NA",NA))table(is.na(train));table(is.na(test))# 将train,test数据切分为数值型和类别型factcols &lt;- c(2:5,7,8:16,20:29,31:38,40,41)numcols &lt;- setdiff(1:40,factcols)cat_train &lt;- train[,factcols, with=FALSE];cat_test &lt;- test[,factcols,with=FALSE]num_train &lt;- train[,numcols,with=FALSE];num_test &lt;- test[,numcols,with=FALSE]# 去掉数值型(num)数据中高度相关的变量ax &lt;- findCorrelation(cor(num_train), cutoff = 0.7)num_train &lt;- num_train[,-ax,with=FALSE];num_test &lt;- num_test[,-ax,with=FALSE]# 处理类别型(cat)数据中的遗漏值mvtr &lt;- sapply(cat_train, function(x)&#123;sum(is.na(x))/length(x)&#125;*100)mvte &lt;- sapply(cat_test, function(x)&#123;sum(is.na(x)/length(x))&#125;*100)# 将遗漏率小于5%的列单独挑选出来cat_train &lt;- subset(cat_train, select = mvtr &lt; 5 )cat_test &lt;- subset(cat_test, select = mvte &lt; 5)cat_train[is.na(cat_train)] &lt;- "Missing";cat_test[is.na(cat_test)] &lt;- "Missing" # 合并数值型和分类型数据d_train &lt;- cbind(num_train, cat_train);d_test &lt;- cbind(num_test, cat_test)rm(train,test,num_train,num_test,cat_train,cat_test) xgboost模型参数解释与设定完成了数据预处理，接着便是分类模型的构建。在运行xgboost前，有三类参数需要人工设定： general parameters, booster parameters and task parameters，xgboost的官方文档有关于这些参数的详细解释： General parameters relates to which booster we are using to do boosting, commonly tree or linear model Booster parameters depends on which booster you have chosen Learning Task parameters that decides on the learning scenario, for example, regression tasks may use different parameters with ranking tasks Tree Booster参数解释： eta [default=0.3, alias: learning_rate] 学习率，防止模型出现过拟合，默认取0.3，通常取值范围[0.01,3] 在每次迭代后，变量的权重会随之衰减 gamma [default=0, alias: min_split_loss] 模型正则化系数，gamma越大，意味着模型越不容易出现过拟合 max_depth [default=6] max_depth值越大，意味着模型越复杂，也越容易出现过拟合 max_depth的取值没有规定 min_child_weight [default=1] In classification, if the leaf node has a minimum sum of instance weight (calculated by second order partial derivative) lower than min_child_weight, the tree splitting stops. subsample[default=1][range: (0,1)] It controls the number of samples (observations) supplied to a tree. 通常取值范围为 (0.5,0.8) colsample_bytree[default=1][range: (0,1)] It control the number of features (variables) supplied to a tree 通常取值范围为 (0.5,0.9) Learning Task参数解释： Objective[default=reg:linear] Objective规定模型需要处理的任务 reg:linear - 线性回归 binary:logistic - 二分类LR回归 multi:softmax - 多分类softmax回归 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798&gt; tr_labels &lt;- d_train$income_level&gt; ts_labels &lt;- d_test$income_level&gt; new_tr &lt;- model.matrix(~.+0,data = d_train[,-c("income_level"),with=F])&gt; new_ts &lt;- model.matrix(~.+0,data = d_test[,-c("income_level"),with=F])&gt; tr_labels &lt;- as.numeric(tr_labels)-1&gt; ts_labels &lt;- as.numeric(ts_labels)-1&gt; dtrain &lt;- xgb.DMatrix(data = new_tr,label = tr_labels)&gt; dtest &lt;- xgb.DMatrix(data = new_ts,label= ts_labels)&gt; params &lt;- list(booster = "gbtree", + objective = "binary:logistic", + eta=0.3, gamma=0, max_depth=6, + min_child_weight=1, subsample=1, + colsample_bytree=1)&gt; xgbcv &lt;- xgb.cv( params = params, + data = dtrain, nrounds = 100, + nfold = 5, showsd = T, + stratified = T, print.every.n = 10,+ early.stop.round = 20, maximize = F)[1] train-error:0.049454+0.000231 test-error:0.050275+0.001244 Multiple eval metrics are present. Will use test_error for early stopping.Will train until test_error hasn't improved in 20 rounds.[11] train-error:0.045343+0.000408 test-error:0.046691+0.001152 [21] train-error:0.042996+0.000356 test-error:0.045323+0.001094 [31] train-error:0.041548+0.000311 test-error:0.044180+0.000912 [41] train-error:0.040261+0.000405 test-error:0.043739+0.000868 [51] train-error:0.039582+0.000303 test-error:0.043514+0.000995 [61] train-error:0.038914+0.000295 test-error:0.043308+0.000788 [71] train-error:0.038361+0.000195 test-error:0.043088+0.000858 [81] train-error:0.037948+0.000252 test-error:0.043003+0.000837 [91] train-error:0.037500+0.000189 test-error:0.042937+0.000921 [100] train-error:0.037144+0.000316 test-error:0.043063+0.001010 Warning messages:1: 'print.every.n' is deprecated.Use 'print_every_n' instead.See help("Deprecated") and help("xgboost-deprecated"). 2: 'early.stop.round' is deprecated.Use 'early_stopping_rounds' instead.See help("Deprecated") and help("xgboost-deprecated"). &gt; xgb1 &lt;- xgb.train (params = params, + data = dtrain, nrounds = 100, + watchlist = list(val=dtest,train=dtrain), + print.every.n = 10, + early.stop.round = 10, + maximize = F , eval_metric = "error")[1] val-error:0.049758 train-error:0.049714 Multiple eval metrics are present. Will use train_error for early stopping.Will train until train_error hasn't improved in 10 rounds.[11] val-error:0.046511 train-error:0.045644 [21] val-error:0.044937 train-error:0.042993 [31] val-error:0.044396 train-error:0.041504 [41] val-error:0.043915 train-error:0.040777 [51] val-error:0.044205 train-error:0.039835 [61] val-error:0.044486 train-error:0.038888 [71] val-error:0.044917 train-error:0.038467 [81] val-error:0.045007 train-error:0.038166 [91] val-error:0.044797 train-error:0.037890 [100] val-error:0.044917 train-error:0.037665 Warning messages:1: 'print.every.n' is deprecated.Use 'print_every_n' instead.See help("Deprecated") and help("xgboost-deprecated"). 2: 'early.stop.round' is deprecated.Use 'early_stopping_rounds' instead.See help("Deprecated") and help("xgboost-deprecated"). &gt; xgbpred &lt;- predict (xgb1,dtest)&gt; xgbpred &lt;- ifelse (xgbpred &gt; 0.5,1,0)&gt; library(caret)&gt; confusionMatrix(xgbpred, ts_labels)Confusion Matrix and Statistics ReferencePrediction 0 1 0 92366 3271 1 1210 2915 Accuracy : 0.9551 95% CI : (0.9538, 0.9564) No Information Rate : 0.938 P-Value [Acc &gt; NIR] : &lt; 2.2e-16 Kappa : 0.5427 Mcnemar's Test P-Value : &lt; 2.2e-16 Sensitivity : 0.9871 Specificity : 0.4712 Pos Pred Value : 0.9658 Neg Pred Value : 0.7067 Prevalence : 0.9380 Detection Rate : 0.9259 Detection Prevalence : 0.9587 Balanced Accuracy : 0.7291 'Positive' Class : 0 &gt; mat &lt;- xgb.importance (feature_names = colnames(new_tr),model = xgb1)&gt; xgb.plot.importance (importance_matrix = mat[1:20]) 其实，即使是给模型设定默认的参数也能得到意想不到的准确率，xgboost在kaggle社区中如此受欢迎也是有理由的。运行，训练模型（耗时约4分钟）并预测，模糊矩阵confusionMatrix(xgbpred, ts_labels)结果显示模型准确率达到了95.51%，然而这并不是重点。提升模型对预测收入高于5w的少数人群（minority class）的预测能力才是我们的目标，结果显示特异度（Specificity）达到47.12%，比上一个朴素贝叶斯提升了11.7%，效果仍然不是特别完美，不过也还可以了！无论是准确率还是其他衡量指标，xgboost得出的结果是全面优于之前的朴素贝叶斯模型的，那么还有没有提升的空间呢？ 二、xgboost in mlr 2016年，R语言的用户迎来了mlr包的诞生，mlr，即machine learning in R。mlr是R语言中为专门应对机器学习问题而开发的包，在mlr出现之前，R语言中是不存在像Scikit-Learn这样的科学计算工具的。mlr包为在R中用机器学习方法解决问题提供了一套自有的框架，涵盖了分类、聚类、回归、生存分析等问题，另外mlr还为参数调优、预测结果分析、数据预处理等与机器学习相关的话题贡献了较为完整的方案。如果说，Scikit-Learn在Python的各个库之间不分伯仲，那么R语言的mlr就可谓一枝独秀。 说了这么多，如果对mlr感兴趣的同学可以去RStudio里一睹“真容”；mlr也专门为用户建立了一个教程网站：Machine Learning in R: mlr Tutorial，可以去官网找一个例子来跑一跑；这是mlr的github项目，由于mlr的普及率还不算太高，官方文档也还在优化中，所以在google上找到关于mlr的资源还不是特别多，所以建议大家在使用过程中出现问题的话去项目中提issue或者在issue中找答案，这是最有效的办法！ 在R语言中使用mlr包解决机器学习问题只需要牢记三个步骤即可： Create a Task：导入数据集，创建任务，可以是分类、回归、聚类等等 Make a Learner：构建模型，模型构建过程中涉及到参数设定、参数调节诸多技巧 Fit the model：拟合模型 Make predictions：预测 在R中，变量可以归结为名义型、有序型、或连续型变量，类别（名义型）变量和有序类别（有序型）在R中称为因子（factor）。 更多关于R语言数据结构的内容参见这篇文章。 值得注意的是mlr包对数据的格式是有要求的，mlr任务函数不接受字符型（char）变量，所以在构建任务函数前，必须确保将所有的变量转换为因子型（factor），作者的解释是 All these things are possible pre-processors, which can be a model that wraps xgboost, when before doing train/predict, run the pre-processing and feed processed data to xgboost. So it is not hard.This is also reason why I do not explicit support factor in the tree construction algorithm. There could be many ways doing so, and in all the ways, having an algorithm optimized for sparse matrices is efficient for taking the processed data.Normal tree growing algorithm only support dense numerical features, and have to support one-hot encoding factor explicitly for computation efficiency reason. 在mlr中的xgboost，似乎并不需要进行太多的数据预处理，xgboost的作者回复issue时是这样说的 “….. xgboost treat every input feature as numerical, with support for missing values and sparsity. The decision is at the userSo if you want ordered variables, you can transform the variables into numerical levels(say age). Or if you prefer treat it as categorical variable, do one hot encoding.” 也就是说xgboost视每个特征均为数值型，同时还支持遗漏变量和稀疏性数据，至于对数据进行何种预处理，决定权在用户手上。不同于之前，本次数据预处理仅仅是将字符型变量转换成因子，然后feed给mlr，mlr就直接开始创建任务（Create task）、构建模型（Make a learner）了，简单而且粗暴。 数据预处理下面，我们将使用mlr包继续提升模型的预测效果，照例首先加载数据和包。 123456789101112131415161718192021222324252627# 载入数据和包&gt; library(data.table)data.table 1.9.8 The fastest way to learn (by data.table authors): https://www.datacamp.com/courses/data-analysis-the-data-table-way Documentation: ?data.table, example(data.table) and browseVignettes("data.table") Release notes, videos and slides: http://r-datatable.com&gt; library(xgboost)Warning message:程辑包‘xgboost’是用R版本3.3.3 来建造的 &gt; library(caret)载入需要的程辑包：lattice载入需要的程辑包：ggplot2载入程辑包：‘caret’&gt; library(mlr)载入需要的程辑包：ParamHelpersWarning messages:1: 程辑包‘mlr’是用R版本3.3.3 来建造的 2: 程辑包‘ParamHelpers’是用R版本3.3.3 来建造的 The following object is masked from ‘package:caret’: train&gt; train &lt;- fread("E:/R/imbalancedata/train.csv",na.string=c(""," ","?","NA",NA))&gt; test &lt;- fread("E:/R/imbalancedata/test.csv",na.string=c(""," ","?","NA",NA))&gt; setDT(train)&gt; setDT(test) 在加载包的时候需要注意mlr和caret的加载顺序，caret应该在mlr之前载入，否则训练模型的时候R不清楚到底是加载caret的train还是mlr的train，从而导致如下错误 12Error in unique.default(x, nmax = nmax) : unique() applies only to vectors 一定要确保 123The following object is masked from ‘package:caret’: train 调参与模型训练在对模型进行训练时，R的运算速度一直是一个问题，其中一个就是只能使用单线程计算。但是R在2.14版本之后，R就内置了parallel包，强化了R的并行计算能力。parallel包可以很容易的在计算集群上实施并行计算，在多个CPU核心的单机上，也能发挥并行计算的功能。笔者用的计算机是4核i5-6600K的CPU与8G内存，即使是中端配置的机器也需要满负荷计算约一小时才能得到最优参数。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768&gt; char_cols &lt;- colnames(train)[sapply(train,is.character)]&gt; for(i in char_cols) set(train,j=i,value = factor(train[[i]]))&gt; for(i in char_cols) set(test,j=i,value = factor(test[[i]]))&gt; train_task &lt;- makeClassifTask(data = train, target = "income_level")Warning in makeTask(type = type, data = data, weights = weights, blocking = blocking, : Provided data is not a pure data.frame but from class data.table, hence it will be converted.&gt; test_task &lt;- makeClassifTask(data = test, target = "income_level")Warning in makeTask(type = type, data = data, weights = weights, blocking = blocking, : Provided data is not a pure data.frame but from class data.table, hence it will be converted.&gt; train_task &lt;- createDummyFeatures(obj = train_task)&gt; train_task &lt;- createDummyFeatures(obj = train_task)&gt; set.seed(2002)&gt; xgb_learner &lt;- makeLearner("classif.xgboost",predict.type = "response")&gt; xgb_learner$par.vals &lt;- list(+ objective = "binary:logistic",+ eval_metric = "error",+ nrounds = 150,+ print.every.n = 50)&gt; xg_ps &lt;- makeParamSet( + makeIntegerParam("max_depth",lower=3,upper=10),+ makeNumericParam("lambda",lower=0.05,upper=0.5),+ makeNumericParam("eta", lower = 0.01, upper = 0.5),+ makeNumericParam("subsample", lower = 0.50, upper = 1),+ makeNumericParam("min_child_weight",lower=2,upper=10),+ makeNumericParam("colsample_bytree",lower = 0.50,upper = 0.80))&gt; rancontrol &lt;- makeTuneControlRandom(maxit = 5L) #do 5 iterations&gt; set_cv &lt;- makeResampleDesc("CV",iters = 5L,stratify = TRUE)&gt; library(parallel)&gt; library(parallelMap)&gt; parallelStartSocket(cpus = detectCores())Starting parallelization in mode=socket with cpus=4.&gt; xgb_tune &lt;- tuneParams(learner = xgb_learner, task = train_task, resampling = set_cv, + measures = list(acc,tpr,tnr,fpr,fp,fn), par.set = xg_ps, + control = rancontrol)[Tune] Started tuning learner classif.xgboost for parameter set: Type len Def Constr Req Tunable Trafomax_depth integer - - 3 to 10 - TRUE -lambda numeric - - 0.05 to 0.5 - TRUE -eta numeric - - 0.01 to 0.5 - TRUE -subsample numeric - - 0.5 to 1 - TRUE -min_child_weight numeric - - 2 to 10 - TRUE -colsample_bytree numeric - - 0.5 to 0.8 - TRUE -With control class: TuneControlRandomImputation value: -0Imputation value: -0Imputation value: -0Imputation value: 1Imputation value: InfImputation value: InfExporting objects to slaves for mode socket: .mlr.slave.optionsMapping in parallel: mode = socket; cpus = 4; elements = 5.[Tune] Result: max_depth=5; lambda=0.171; eta=0.295; subsample=0.855; min_child_weight=5.54; colsample_bytree=0.735 : acc.test.mean=0.958,tpr.test.mean=0.989,tnr.test.mean=0.482,fpr.test.mean=0.518,fp.test.mean=1.28e+03,fn.test.mean= 413&gt; xgb_tune$yacc.test.mean tpr.test.mean tnr.test.mean fpr.test.mean fp.test.mean fn.test.mean 0.9575036 0.9889762 0.4818275 0.5181725 1283.2000000 412.6000000 &gt; xgb_tune$x$max_depth[1] 5$lambda[1] 0.1711398$eta[1] 0.295421$subsample[1] 0.8545802$min_child_weight[1] 5.541689$colsample_bytree[1] 0.7345529 xgb_tune$x查看参数调节得出的最优结果，将最优参数设定在模型xgb_new中，然后进行训练，这时便出现了我们前面提到的错误unique() applies only to vectors（当然github项目上给出的代码已经修正了）。出现这个错误之后，刚开始并不清楚原因在哪个地方，在下面的代码执行日志中可以发现我在不停地重新赋值再训练还有重新创建任务（因为我之前在R中遇到过将同一段代码先后两次执行，第一次错误，第二次却成功的情况），来来回回尝试了十几次，直到后来在github找到关于这条错误信息的issue，原来是caret和mlr的加载顺序弄错了。然后，用detach(&quot;package:caret&quot;)和detach(&quot;package:mlr&quot;)命令先将两个包移除，再按照先加载caret后加载mlr的顺序，最后再重新赋值训练，成功。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131&gt; xgb_new &lt;- setHyperPars(learner = xgb_learner, par.vals = xgb_tune$x)&gt; xgb_model &lt;- train(learner = xgb_new, task = train_task)Error in train.default(learner = xgb_new, task = train_task) : argument "y" is missing, with no default&gt; xgb_new &lt;- setHyperPars(learner = xgb_learner, par.vals = xgb_tune$x)&gt; xgb_model &lt;- train(xgb_new, train_task)Error in unique.default(x, nmax = nmax) : unique() applies only to vectors&gt; rm(xgb_new)&gt; xgb_new &lt;- setHyperPars(learner = xgb_learner, par.vals = xgb_tune$x)&gt; xgb_model &lt;- train(xgb_new, train_task)Error in unique.default(x, nmax = nmax) : unique() applies only to vectors&gt; xgb_tune$yacc.test.mean tpr.test.mean tnr.test.mean fpr.test.mean fp.test.mean fn.test.mean 0.9575036 0.9889762 0.4818275 0.5181725 1283.2000000 412.6000000 &gt; xgb_new &lt;- setHyperPars(xgb_learner, par.vals = xgb_tune$x)&gt; xgb_model &lt;- train(learner = xgb_new, task = train_task)Error in train.default(learner = xgb_new, task = train_task) : argument "y" is missing, with no default&gt; xgb_model &lt;- train(xgb_new, train_task)Error in unique.default(x, nmax = nmax) : unique() applies only to vectors&gt; xgb_tune$yacc.test.mean tpr.test.mean tnr.test.mean fpr.test.mean fp.test.mean fn.test.mean 0.9575036 0.9889762 0.4818275 0.5181725 1283.2000000 412.6000000 &gt; xgb_new &lt;- setHyperPars(learner = xgb_learner, par.vals = xgb_tune$x)&gt; xgmodel &lt;- train(xgb_new, train_task)Error in unique.default(x, nmax = nmax) : unique() applies only to vectors&gt; xgb_tuneTune result:Op. pars: max_depth=5; lambda=0.171; eta=0.295; subsample=0.855; min_child_weight=5.54; colsample_bytree=0.735acc.test.mean=0.958,tpr.test.mean=0.989,tnr.test.mean=0.482,fpr.test.mean=0.518,fp.test.mean=1.28e+03,fn.test.mean= 413&gt; xgb_new &lt;- setHyperPars(learner = xgb_learner, par.vals = xgb_tune$x)&gt; xgmodel &lt;- train(xgb_new, train_task)Error in unique.default(x, nmax = nmax) : unique() applies only to vectors&gt; xgb_new &lt;- setHyperPars(makeLearner("classif.xgboost"), par.vals = xgb_tune$x)&gt; xgmodel &lt;- train(xgb_new, train_task)Error in unique.default(x, nmax = nmax) : unique() applies only to vectors&gt; xgb_newLearner classif.xgboost from package xgboostType: classifName: eXtreme Gradient Boosting; Short name: xgboostClass: classif.xgboostProperties: twoclass,multiclass,numerics,prob,weights,missings,featimpPredict-Type: responseHyperparameters: nrounds=1,verbose=0,max_depth=5,lambda=0.171,eta=0.295,subsample=0.855,min_child_weight=5.54,colsample_bytree=0.735&gt; xgmodel = train(xgb_new, train_task)Error in unique.default(x, nmax = nmax) : unique() applies only to vectors&gt; xgb_learner &lt;- makeLearner("classif.xgboost",predict.type = "response")&gt; xgb_learner$par.vals &lt;- list(+ objective = "binary:logistic",+ eval_metric = "error",+ nrounds = 150,+ print.every.n = 50)&gt; xg_ps &lt;- makeParamSet( + makeIntegerParam("max_depth",lower=3,upper=10),+ makeNumericParam("lambda",lower=0.05,upper=0.5),+ makeNumericParam("eta", lower = 0.01, upper = 0.5),+ makeNumericParam("subsample", lower = 0.50, upper = 1),+ makeNumericParam("min_child_weight",lower=2,upper=10),+ makeNumericParam("colsample_bytree",lower = 0.50,upper = 0.80))&gt; xgmodel = train(xgb_new, train_task)Error in unique.default(x, nmax = nmax) : unique() applies only to vectors&gt; train_task &lt;- makeClassifTask(data = train, target = "income_level")Warning in makeTask(type = type, data = data, weights = weights, blocking = blocking, : Provided data is not a pure data.frame but from class data.table, hence it will be converted.&gt; test_task &lt;- makeClassifTask(data = test, target = "income_level")Warning in makeTask(type = type, data = data, weights = weights, blocking = blocking, : Provided data is not a pure data.frame but from class data.table, hence it will be converted.&gt; train_task &lt;- createDummyFeatures(obj = train_task)&gt; test_task &lt;- createDummyFeatures(obj = test_task)&gt; xgmodel = train(xgb_new, train_task)Error in unique.default(x, nmax = nmax) : unique() applies only to vectors&gt; xgb_new &lt;- setHyperPars(xgb_learner, par.vals = xgb_tune$x)&gt; xgmodel = train(xgb_new, train_task)Error in unique.default(x, nmax = nmax) : unique() applies only to vectors&gt; library(caret)&gt; xgmodel = train(xgb_new, train_task)Error in unique.default(x, nmax = nmax) : unique() applies only to vectors&gt; xgmodel = caret::train(xgb_new, train_task)Error in unique.default(x, nmax = nmax) : unique() applies only to vectors&gt; library(caret)&gt; library(data.table)&gt; library(mlr)&gt; library(xgboost)&gt; xgmodel = train(xgb_new, train_task)Error in unique.default(x, nmax = nmax) : unique() applies only to vectors&gt; (packages())Error: could not find function "packages"&gt; (.packages()) [1] "randomForest" "parallelMap" "parallel" "caret" "ggplot2" "lattice" "xgboost" [8] "mlr" "ParamHelpers" "data.table" "stats" "graphics" "grDevices" "utils" [15] "datasets" "methods" "base" &gt; detach("package:caret")&gt; detach("package:mlr")&gt; (.packages()) [1] "randomForest" "parallelMap" "parallel" "ggplot2" "lattice" "xgboost" "ParamHelpers" [8] "data.table" "stats" "graphics" "grDevices" "utils" "datasets" "methods" [15] "base" &gt; library(caret)&gt; library(mlr)载入程辑包：‘mlr’The following object is masked from ‘package:caret’: trainWarning message:程辑包‘mlr’是用R版本3.3.3 来建造的 &gt; xgmodel = train(xgb_new, train_task)[1] train-error:0.050866 [51] train-error:0.041344 [101] train-error:0.039279 [150] train-error:0.037895 Warning message:'print.every.n' is deprecated.Use 'print_every_n' instead.See help("Deprecated") and help("xgboost-deprecated"). 经历了千辛万苦，模型终于训练好了，胜利的曙光似乎就在前方，终于可以进行预测了！然而，猝不及防，正当我们将测试集的income_level与xgb_prediction进行对比时，an error thrown again，这次是The data contain levels not found in the data.。错误信息直接翻译过来的意思是数据中包含levels not found，分别查看预测结果xgb_prediction与test$income_level，发现原来是两者的标签设置不一样，xgb_prediction预测的结果是-50000和+50000两种，而原测试集目标变量test$income_level的标签是-50000和50000+.两个level，标签不同自然无法比较。 解决办法也挺简单，执行test[,income_level:= ifelse(income_level == &quot;-50000&quot;,&quot;-50000&quot;,&quot;+50000&quot;)]将50000+.替换为+50000即可。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284&gt; predict_xgb &lt;- predict(xgmodel, test_task)&gt; xgb_prediction &lt;- predict_xgb$data$response&gt; confusionMatrix(test$income_level, xgb_prediction)Error in confusionMatrix.default(test$income_level, xgb_prediction) : The data contain levels not found in the data.&gt; xgb_prediction [1] -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 [16] -50000 -50000 -50000 -50000 +50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 +50000 [31] -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 +50000 -50000 -50000 -50000 -50000 -50000 -50000 [46] -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 [61] -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 [76] -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 [91] -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 [106] -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 +50000 -50000 -50000 -50000 [121] -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 [136] -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 [151] -50000 -50000 -50000 -50000 +50000 -50000 -50000 -50000 -50000 -50000 +50000 -50000 -50000 -50000 -50000 [166] -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 [181] -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 [196] -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 [211] -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 [226] +50000 -50000 -50000 -50000 -50000 -50000 +50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 [241] -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 [256] -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 +50000 [271] -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 [286] -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 +50000 -50000 -50000 [301] -50000 -50000 -50000 -50000 -50000 +50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 [316] -50000 -50000 -50000 -50000 -50000 -50000 +50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 [331] -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 [346] -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 +50000 -50000 -50000 -50000 -50000 -50000 [361] -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 [376] -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 +50000 -50000 +50000 -50000 -50000 -50000 -50000 [391] -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 [406] -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 [421] -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 [436] -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 [451] -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 [466] -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 [481] -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 [496] -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 [511] -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 [526] -50000 -50000 -50000 -50000 -50000 -50000 -50000 +50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 [541] -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 [556] -50000 +50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 +50000 -50000 -50000 -50000 -50000 -50000 [571] -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 [586] -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 +50000 -50000 -50000 -50000 -50000 -50000 -50000 [601] -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 [616] -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 [631] -50000 -50000 -50000 -50000 -50000 -50000 +50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 [646] -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 [661] -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 +50000 -50000 -50000 [676] -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 [691] -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 [706] -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 +50000 -50000 -50000 [721] -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 [736] -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 [751] -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 +50000 -50000 -50000 -50000 [766] -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 [781] -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 [796] -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 +50000 -50000 -50000 -50000 -50000 [811] -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 [826] -50000 -50000 +50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 [841] -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 [856] -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 [871] -50000 -50000 -50000 -50000 -50000 -50000 +50000 -50000 +50000 -50000 -50000 -50000 -50000 -50000 -50000 [886] -50000 -50000 -50000 +50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 +50000 [901] +50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 [916] -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 +50000 -50000 -50000 -50000 -50000 -50000 -50000 [931] +50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 [946] -50000 -50000 -50000 -50000 +50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 [961] -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 [976] -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 [991] -50000 -50000 -50000 -50000 -50000 +50000 -50000 -50000 -50000 -50000 [ reached getOption("max.print") -- omitted 98762 entries ]Levels: -50000 +50000&gt; confusionMatrix(test$income_level, xgb_prediction)Error in confusionMatrix.default(test$income_level, xgb_prediction) : The data contain levels not found in the data.&gt; confusionMatrix(xgb_prediction$data$response,xgb_prediction$data$truth)Error in xgb_prediction$data : $ operator is invalid for atomic vectors&gt; xg_confused &lt;- confusionMatrix(test$income_level,xgb_prediction)Error in confusionMatrix.default(test$income_level, xgb_prediction) : The data contain levels not found in the data.&gt; test$income_level [1] -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 [14] -50000 -50000 -50000 -50000 -50000 -50000 50000+. -50000 -50000 -50000 -50000 -50000 -50000 [27] -50000 -50000 -50000 -50000 -50000 -50000 -50000 50000+. -50000 -50000 -50000 -50000 50000+. [40] -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 [53] -50000 -50000 -50000 -50000 -50000 -50000 -50000 50000+. -50000 -50000 -50000 -50000 -50000 [66] -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 [79] -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 [92] -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 50000+. -50000 -50000 -50000 [105] -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 [118] -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 50000+. -50000 -50000 -50000 -50000 [131] -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 [144] -50000 -50000 -50000 -50000 -50000 -50000 50000+. -50000 -50000 -50000 -50000 50000+. -50000 [157] -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 [170] -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 [183] -50000 -50000 -50000 50000+. -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 [196] -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 50000+. -50000 [209] -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 [222] -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 50000+. -50000 -50000 [235] -50000 -50000 -50000 50000+. -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 [248] -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 [261] -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 [274] -50000 -50000 -50000 -50000 50000+. -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 [287] -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 50000+. -50000 [300] -50000 -50000 -50000 -50000 -50000 -50000 50000+. -50000 -50000 -50000 -50000 -50000 -50000 [313] -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 50000+. -50000 -50000 -50000 [326] -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 [339] -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 [352] -50000 -50000 50000+. 50000+. -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 [365] -50000 -50000 -50000 50000+. -50000 -50000 50000+. -50000 -50000 -50000 -50000 -50000 -50000 [378] -50000 -50000 -50000 -50000 -50000 -50000 50000+. -50000 -50000 -50000 -50000 -50000 -50000 [391] 50000+. -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 [404] -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 [417] -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 [430] -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 [443] -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 [456] -50000 -50000 -50000 -50000 -50000 -50000 50000+. -50000 -50000 -50000 -50000 -50000 -50000 [469] -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 [482] -50000 -50000 50000+. -50000 -50000 -50000 50000+. -50000 -50000 -50000 -50000 -50000 -50000 [495] -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 [508] -50000 -50000 -50000 50000+. -50000 -50000 -50000 -50000 -50000 -50000 -50000 50000+. -50000 [521] -50000 50000+. -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 50000+. [534] -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 [547] -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 50000+. -50000 -50000 [560] -50000 -50000 50000+. -50000 -50000 50000+. -50000 -50000 -50000 -50000 -50000 -50000 -50000 [573] -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 [586] -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 [599] -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 50000+. -50000 -50000 -50000 -50000 [612] -50000 -50000 -50000 -50000 -50000 -50000 50000+. -50000 -50000 -50000 -50000 -50000 -50000 [625] -50000 -50000 50000+. -50000 -50000 -50000 -50000 50000+. -50000 -50000 -50000 -50000 -50000 [638] -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 [651] -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 50000+. -50000 -50000 [664] -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 50000+. -50000 -50000 -50000 [677] -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 [690] -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 [703] -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 50000+. -50000 [716] -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 50000+. -50000 -50000 -50000 -50000 [729] -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 [742] -50000 -50000 -50000 50000+. -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 [755] -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 50000+. -50000 -50000 -50000 -50000 [768] -50000 -50000 -50000 -50000 -50000 -50000 -50000 50000+. 50000+. -50000 -50000 -50000 -50000 [781] -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 50000+. -50000 -50000 -50000 -50000 [794] -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 50000+. [807] -50000 -50000 -50000 -50000 -50000 -50000 50000+. -50000 -50000 -50000 -50000 -50000 -50000 [820] -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 [833] -50000 -50000 -50000 -50000 -50000 -50000 50000+. -50000 -50000 -50000 -50000 -50000 -50000 [846] -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 [859] -50000 50000+. -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 50000+. [872] -50000 -50000 -50000 -50000 -50000 50000+. -50000 -50000 -50000 -50000 -50000 -50000 -50000 [885] -50000 -50000 -50000 -50000 50000+. -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 [898] -50000 -50000 50000+. 50000+. -50000 -50000 -50000 -50000 50000+. -50000 -50000 -50000 -50000 [911] -50000 50000+. -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 50000+. -50000 -50000 [924] 50000+. -50000 -50000 -50000 -50000 -50000 -50000 50000+. -50000 50000+. -50000 -50000 -50000 [937] -50000 -50000 -50000 -50000 -50000 -50000 50000+. -50000 -50000 -50000 -50000 -50000 -50000 [950] 50000+. -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 [963] -50000 -50000 -50000 -50000 50000+. -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 [976] -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 -50000 50000+. -50000 -50000 [989] -50000 -50000 -50000 -50000 -50000 -50000 -50000 50000+. -50000 -50000 -50000 -50000 [ reached getOption("max.print") -- omitted 98762 entries ]Levels: -50000 50000+.&gt; test[,income_level:= ifelse(income_level == "-50000","-50000","+50000")]&gt; test$income_level [1] "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" [12] "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "+50000" "-50000" "-50000" [23] "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" [34] "+50000" "-50000" "-50000" "-50000" "-50000" "+50000" "-50000" "-50000" "-50000" "-50000" "-50000" [45] "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" [56] "-50000" "-50000" "-50000" "-50000" "+50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" [67] "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" [78] "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" [89] "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" [100] "-50000" "+50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" [111] "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" [122] "-50000" "-50000" "-50000" "-50000" "+50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" [133] "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" [144] "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "+50000" "-50000" "-50000" "-50000" "-50000" [155] "+50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" [166] "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" [177] "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "+50000" "-50000" [188] "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" [199] "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "+50000" "-50000" "-50000" [210] "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" [221] "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" [232] "+50000" "-50000" "-50000" "-50000" "-50000" "-50000" "+50000" "-50000" "-50000" "-50000" "-50000" [243] "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" [254] "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" [265] "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" [276] "-50000" "-50000" "+50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" [287] "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" [298] "+50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "+50000" "-50000" "-50000" [309] "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" [320] "-50000" "-50000" "+50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" [331] "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" [342] "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" [353] "-50000" "+50000" "+50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" [364] "-50000" "-50000" "-50000" "-50000" "+50000" "-50000" "-50000" "+50000" "-50000" "-50000" "-50000" [375] "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "+50000" "-50000" [386] "-50000" "-50000" "-50000" "-50000" "-50000" "+50000" "-50000" "-50000" "-50000" "-50000" "-50000" [397] "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" [408] "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" [419] "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" [430] "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" [441] "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" [452] "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "+50000" [463] "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" [474] "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "+50000" [485] "-50000" "-50000" "-50000" "+50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" [496] "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" [507] "-50000" "-50000" "-50000" "-50000" "+50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" [518] "-50000" "+50000" "-50000" "-50000" "+50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" [529] "-50000" "-50000" "-50000" "-50000" "+50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" [540] "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" [551] "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "+50000" "-50000" "-50000" "-50000" "-50000" [562] "+50000" "-50000" "-50000" "+50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" [573] "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" [584] "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" [595] "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" [606] "-50000" "+50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" [617] "-50000" "+50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "+50000" [628] "-50000" "-50000" "-50000" "-50000" "+50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" [639] "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" [650] "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" [661] "+50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" [672] "-50000" "+50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" [683] "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" [694] "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" [705] "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "+50000" "-50000" [716] "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "+50000" "-50000" "-50000" [727] "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" [738] "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "+50000" "-50000" "-50000" "-50000" [749] "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" [760] "-50000" "-50000" "-50000" "+50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" [771] "-50000" "-50000" "-50000" "-50000" "+50000" "+50000" "-50000" "-50000" "-50000" "-50000" "-50000" [782] "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "+50000" "-50000" "-50000" "-50000" [793] "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" [804] "-50000" "-50000" "+50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "+50000" "-50000" [815] "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" [826] "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" [837] "-50000" "-50000" "+50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" [848] "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" [859] "-50000" "+50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" [870] "-50000" "+50000" "-50000" "-50000" "-50000" "-50000" "-50000" "+50000" "-50000" "-50000" "-50000" [881] "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "+50000" "-50000" "-50000" [892] "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "+50000" "+50000" "-50000" [903] "-50000" "-50000" "-50000" "+50000" "-50000" "-50000" "-50000" "-50000" "-50000" "+50000" "-50000" [914] "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "+50000" "-50000" "-50000" "+50000" [925] "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "+50000" "-50000" "+50000" "-50000" "-50000" [936] "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "+50000" "-50000" "-50000" "-50000" [947] "-50000" "-50000" "-50000" "+50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" [958] "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "+50000" "-50000" [969] "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" [980] "-50000" "-50000" "-50000" "-50000" "-50000" "-50000" "+50000" "-50000" "-50000" "-50000" "-50000" [991] "-50000" "-50000" "-50000" "-50000" "-50000" "+50000" "-50000" "-50000" "-50000" "-50000" [ reached getOption("max.print") -- omitted 98762 entries ]&gt; xg_confused &lt;- confusionMatrix(test$income_level,xgb_prediction)&gt; xg_confusedConfusion Matrix and Statistics ReferencePrediction -50000 +50000 -50000 92699 877 +50000 3433 2753 Accuracy : 0.9568 95% CI : (0.9555, 0.9581) No Information Rate : 0.9636 P-Value [Acc &gt; NIR] : 1 Kappa : 0.5398 Mcnemar's Test P-Value : &lt;2e-16 Sensitivity : 0.9643 Specificity : 0.7584 Pos Pred Value : 0.9906 Neg Pred Value : 0.4450 Prevalence : 0.9636 Detection Rate : 0.9292 Detection Prevalence : 0.9380 Balanced Accuracy : 0.8613 'Positive' Class : -50000 查看模糊矩阵得到的结果，模型准确率达到95.68%，并且特异度（Specificity），也就是对负样本的预测准确率达到75.84%，可以说，已经非常不错了！至此，UCI人口调查数据的折腾就暂时告一段落了，如果有时间我还会继续发表研究这个数据以及学习xgboost的心得！ （完） 参考链接 关于xgboost不接受字符型变量的讨论：Factors #95 关于出现unique() applies only to vectors错误的讨论：Error in unique.default(x, nmax = nmax) : unique() applies only to vectors #1407；Error in makeParamSet when tuning hyperparameters of nnet #1418 mlr入门教程：Machine Learning in R: mlr Tutorial Get Started with XGBoost in R 在R语言中针对初学者的xgboost和调参教程：Beginners Tutorial on XGBoost and Parameter Tuning in R 知乎专栏：强大的机器学习专属R包——mlr包 mlr-tutorial:Imbalanced Classification Problems]]></content>
      <categories>
        <category>Machine-Learning</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>数据分析</tag>
        <tag>R</tag>
        <tag>Machine-Learning</tag>
        <tag>xgboost</tag>
        <tag>mlr</tag>
        <tag>UCI</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[13个“番茄”搞定SQL]]></title>
    <url>%2F2017%2Fspending-13-tomatoes-getting-sql-done%2F</url>
    <content type="text"><![CDATA[学习SQL的材料是《SQL必知必会》这本小册子，小册子浓缩了SQL的精华，很适合从未接触过SQL的初学者学习。标题中的“番茄”指的是番茄工作时间，不过我的单个番茄工作时间为45分钟，阅读完册子的核心部分加上做笔记总共花费了13个番茄时间。 一、什么是SQL？SQL是structured query language（结构化查询语言）的缩写，是一种专门用来与数据库沟通的语言。 SQL优点： 几乎所有的DBMS都支持SQL 简单易学 可以进行非常复杂和高级的数据库操作 四个重要概念 表 用来存储某种特定类型的数据的结构化清单，存储在表中的数据是相同类型的 列 表中的一个字段，一个或多个列组成表 行 表中的一个记录，也即是数据库记录（record） 主键（primary key） 表的行或列中唯一标识自己的值，如顾客ID、身份证号 二、检索数据-SELECT语句首先明确要选择什么（select what？），从什么地方（where？）选择；SQL不区分大小写；多条SQL语句必须以（;）分隔开来。 例如，检索一个列： 12select prod_namefrom Products; 也可以检索多个列： 12select prod_id, prod_name, prod_pricefrom Products; 也可以用通配符（*）来检索所有列 12select *from Products; 检索列下面不等的值 123select distinct vend_idform Products;# distinct告诉DBMS只返回不同vend_id行，注意distinct作用与所有的列 返回特定的行 123select top 5 prod_namefrom Product;# 返回Products列的前5行 当然，不同的DBMS有不同的语法规则，但是基本的语句往往是可以移植的，复杂的语句就不行。其实SQL很好理解的，看查询语句就跟学习简单得英语语法一样，并且词汇量还不大，转来转去就那么几个关键词。 三、排序检索数据-ORDER BY按照单个列进行排列 12345select prod_namefrom Productsorder by prod_name;# 对返回的结果以prod_name的顺序进行排列；# order by应当保证是select语句的中的最后一条子句 按照多个列进行排列 1234select prod_id, prod_price, prod_namefrom Productsorder by prod_price, prod_name# 对返回的结果以prod_price, prod_name进行排列 按位置进行排序 1234select prod_id, prod_price, prod_namefrom Productsorder by 2, 3;# 表现先按第二个列再按第三个列进行排序，这个好处在于不用输入列名 按照指定顺序进行排列，SQL默认返回的结果是升序排列，有的时候我们需要返回的结果能够按照降序或者升序来进行排列，在SQL语句中可以通过升序（asc）和降序（desc）语句实现 1234select prod_id, prod_price, prod_namefrom Productsorder by prod_price desc;# 根据价格的降序进行排列 也可以对指定列进行升序或者降序排列 1234select prod_id, prod_price, prod_namefrom Productsorder by prod_price desc, prod_name;# 表示prod_price降序排列，prod_name升序排列 四、过滤数据1. SELECT WHERE在select语句中，数据根据where子句中指定的搜索条件进行过滤，可以进行单个值、范围值、不匹配值、空值检查。另外还可以通过AND和OR组合多个条件进行过滤，不过AND和OR条件得组合需要用括号分隔加以分组。 123select ...from ...where ...; 操作符 说明 检查 = 等于 &lt;&gt;或!= 不等于 不匹配检查 &lt; 小于 &lt;= 小于等于 !&lt; 不小于 &gt; 大于 >= 大于等于 !&gt; 不大于 between 在指定的两个值之间 范围值检查 is null 为null值 空值检查 2. NOT和IN操作符where子句中的NOT操作符有且只有一个功能，那就是否定后面的条件，NOT也从不单独使用 12select ...where not ...; IN操作符取一组由逗号分隔、括在圆括号中间的合法值，IN操作符有诸多优点： 清楚、直观 运行更快 可以包含其他SELECT语句 123select prod_name, prod_pricefrom Productswhere ven_id in ('DLL01', 'BRS01'); 3. 使用通配符（wildcard）过滤主要介绍三种通配符，搭配where使用可以进行快速搜索，但是不要过多使用 百分号%通配符 1234567select prod_id, prod_namefrom productswhere prod_name like 'Fish%'# 执行这条语句时，将检索任意以Fish开头的词# 注意区分大小写，'fish%'和'Fish%'搜索出来的结果是不同的# 如果搜索条件为%Fish%，则表示匹配任何位置上包含Fish的值# %还可以匹配给定位置的0个或者多个字符，如'F%y'、'F%y%' 下划线_通配符的用途与%一样，不过它只能匹配单个字符，而不是多个，如果需要匹配多个则需要多添加一个下划线__ 1234select prod_id, prod_namefrom productswhere prod_name like '__ inch teddy bear'# 注意匹配的是两个通配符 方括号[]通配符用来指定一个字符集，它必须匹配指定位置的一个字符 1234# 找出所有J或M开头的结果select ...from ...where ... like '[JM]%' 1234# 用前缀字符^(脱字号)找出所有J或M开头以外的结果select ...from ...where ... like '[^JM]%' 四、计算字段这里介绍两种SQL语言中计算字段的手段，分别是拼接、算术计算。 首先是拼接，假如表中包含姓名和地址信息，且需要在格式化的名称中列出供应商的名字， 1234select name + '('+ address + ')'from peopleorder by name;# 加号‘+’也可以换成‘||’，两者输出的结果相同 输出的结果 12zhangsan (hunan)lisi (hubei) 我们发现上述返回的结果并不理想，空格和括号并不是我们想要的，如果需要返回的结果没有空格，可以使用SQL的RTRIM()函数去掉字符串右边的所有空格来完成，同理，LTRIM()则是去掉字符串左边所有的空格，TRIM()去掉字符串左右两边所有空格。 123select rtrim(name) + '('+ rtrim(address) + ')'from peopleorder by name; 输出的结果 12zhangsan(hunan)lisi(hubei) 第二种则是对检索出来的数据进行算术计算， 123456789select prod_id, quantity, item_price, quantity * item_price as expanded_pricefrom order_itemswhere order_num = 2000;# 从order_item中选择若干字段，并以其中字段计算得来的结果作为# 新的计算列expand_price# as是一种别名用法 五、处理数据对一个值或多个值操作常用的文本处理函数 函数 说明 LEFT() 返回字符串左边的字符 LENGTH() 返回字符串的长度 LOWER() 将字符串转换为小写 LTRIM() 去掉字符串左边的空格 RIGHT() 返回字符串右边的字符 RTRIM() 去掉字符串右边的空格 SOUNDEX() 返回与字符串发音类似的字符 UPPER() 将字符串转换为大写 常用的数值处理函数还有ABS()、EXP()、PI()、SIN()、COS()等等，上述两种数值处理函数均针对一个数值进行的操作，如果要针对表中的一列进行操作则需要AVG()、COUNT()、SUM()、MAX()、MIN()等等函数，这几个函数在进行操作时都会忽略空的行（NULL）。值得注意的是COUNT()函数的用法，COUNT(*)对表中的行数进行计数，不管表中包含的是空值还是非空值，而COUNT(column)则只对非空值的行计数。 数据分组数据分组用到两个语句，分别是GROUP BY和HAVING。 ①有必要说清楚分组GROUP BY与排序ORDER BY的区别，分组是将输出的结果以分组的形式呈现，而排序则是让输出结果按照顺序进行排列，换言之，分组的结果不一定是按照顺序排列的。 ②where和having的功能也差不多，区别在于where是在数据分组前进行过滤，而having是在数据分组后进行过滤的。 六、查询数据在SQL中，查询数据主要有子查询、内联结、外联结和组合查询等方法。子查询是通过嵌套查询语句来实现的，子查询语句只能返回一个列，不论是可读性还是调试性，子查询都有明显的缺陷，不推荐使用子查询。 相比子查询，另外一种查询方式——联结查询则是一种更方便的查询方法。 联结查询如果我们手头上有两个表，这两个表中存在相同的列，在多个表中检索出数据则需要通过内联结。 123456789101112# 创建表与表之间的联结select vend_name, prod_name, prod_pricefrom vendors, productswhere vendors.ven_id=products.ven_id;# 完全限定列名的用法# 下面的语句返回的结果跟上面的语句执行的结果一样# 不同的是，两个表之间的关系是以INNER JOIN联结的# ON子句的效果与where子句的效果一样select vend_name, prod_name, prod_pricefrom vendors INNER JOIN products ON vendors.ven_id=products.ven_id; 除了内联结之外，SQL中还有外联结、左（右）外联结和全外联结几种模式，这里就不一一讨论了。 组合查询当查询时需要多个where select限定语句时，只需在语句之间添加关键字UNION即可，简单一点理解就是取两个或多个查询结果的交集，所以这就要求where select返回的结果必须具有相同的格式。 1234567select ...from ...where ...UNIONselect ...from ...where ... 六、插入、更新、删除数据用INSERT插入数据，不管是插入完整的行，插入行的一部分还是插入某些查询的结果，values值的个数必须与行数要相等。 12insert into customers(..,..)values('','',''); 1234# 从一个表复制到另一个表select *into custcopyfrom customers; 更新和删除数据分别用到UPDATE和DELETE语句，比如要更新某个客户的电子邮件 1234# 更新customers表下ID为10003客户的电子邮件update customersset cust_email = "kim@store.com"where cust_id = '10003' 用SQL语句删除数据也是很简单的，比如删除掉某一行客户的信息，记住delete从表中删除行，但是并不删除表本身。记住一条重要的原则，使用delete语句的时候一定要配合where子句使用，防止误操作。 12delete from customerswhere cust_id = '10003' 七、创建表使用CREATE TABLE语句创建表，创建表的时候必须给出下列信息： 表的名字 列的名字和定义，用逗号分隔 比如， 123456create table products(prod_id char(10) not null, vend_id char(10) not null, ....);# not null指的是在插入或更新行时，该列必须有值 到此，SQL的基础知识就差不多总结结束了，这篇文章简单介绍了如何检索、排序、过滤数据，以及如何针对数据进行简单操作，可以说十分小白。不过作为一个在写这篇笔记之前几乎没有接触过数据库语言的菜鸟，错误在所难免，本文章不建议给初学者作为学习参考，仅作个人防止遗忘，要学习SQL还是去看看《SQL必知必会》这本小册子吧。]]></content>
      <categories>
        <category>coding</category>
      </categories>
      <tags>
        <tag>笔记</tag>
        <tag>SQL</tag>
        <tag>SQL必知必会</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大清相国——小说毕竟是小说]]></title>
    <url>%2F2017%2Fpremier-of-qing-dynasty%2F</url>
    <content type="text"><![CDATA[“清官多酷,陈廷敬是清官,却宅心仁厚； 好官多庸,陈廷敬是好官,却精明强干； 能官多专,陈廷敬是能官,却从善如流； 德官多懦,陈廷敬是德官,却不乏铁腕。” 王跃文写的《大清相国》中曾这样赞扬陈廷敬，康熙皇帝也曾给予陈廷敬“几近完人”的评价。 看完这部小说后高度评价陈廷敬做官做人的读者自然不在少数，但是，小说毕竟是小说，陈廷敬做官为人处世无可挑剔，然而抛开小说人物设定的背景，生硬地将他那套纵横官场的做法放到现在需要重新考虑了。 首先，陈廷敬是个“富二代”，他出生的时候祖上的家业就做到了南洋，出生的时候就财务自由，然而幸运的是，他并不堕落，才华横溢，智商爆表的他二十岁就中了进士；而后，贵人不断，力保他的卫向书、“智囊”岳父大人等等，身边神奇般地聚集了王朝马汉之流，两房小妾一个知书达理、一个武艺了得，出门在外自然是没话说；最最最重要的，他有一个好领导——康熙，康熙皇帝想做“正确的事”，他的包容是这个情商几乎为零、官场屹立不倒大臣的最强辅助。可以说这个投胎自带GPS头带主角光环的小说人物比起张汧、高士奇等角色就在起跑线就赢了太多了。 小说毕竟是小说，看看就好，我对陈廷敬为官之道并不太感兴趣（放到现在的企业管理不太合适），而是他的选择能力和跨行业超强的工作能力。索额图、明珠两党派之争正盛的时候他选择独善其身，哪一方都不加入，耐心地在其中周旋，待时机成熟后再一并除掉。调查富伦、王继文和理顺钱法则证明了他在多领域超强的工作能力，比如调查研究、金融、报告写作能力。特别是理顺钱法那几个章节相当精彩，不懂金融的陈廷敬在短时间内就能够发现到问题的症结所在（钱贱铜贵 ），关键时候当机立断，绑了科尔昆和工匠头子向忠。 同一届的张汧和高士奇，这两个人从底层突破层层阶级枷锁，一度无限接近最高权力中心，张汧做到了巡抚，高士奇虽没有功名但也得到皇帝的宠信，得以入住紫禁城。他们自是聪明，工作能力也不差，对比陈廷敬他们输在了什么地方，是格局。格局这个属性往往是与生俱来的，后天不经学习或高人点化甚至自身都察觉不了。他们两人摇摆不定，不具备发现问题本质的能力，一看到哪里有风吹草动就沉不住气，或反咬、或chose the wrong side，最后免不了变成操盘手的棋子，自己还浑然不知。 作者王跃文用“等”、“稳”、“忍”、“狠”、“隐”五个字概括了他官场50年不倒的秘诀，但是这五个字却永远也让陈廷敬做不好一个领导，他最多只能成为服务统治阶级的二把手，能够得以善终就是最大的成功了。王岐山多次在公开场合给机关干部推荐这部书，除了希望公务员学习他的清廉、品德、工作能力，恐怕还有别的深意吧。]]></content>
      <categories>
        <category>思想王国</category>
      </categories>
      <tags>
        <tag>读书笔记</tag>
        <tag>大清相国</tag>
        <tag>王跃文</tag>
        <tag>陈廷敬</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[优达学城-深度学习任务4：卷积神经网络]]></title>
    <url>%2F2017%2Fdeep-learning-assignment-4-cnn%2F</url>
    <content type="text"><![CDATA[卷积神经网络（CNN）是一种十分强大的深度学习方法，在这次课程中没有对CNN做过多的探讨，这可苦了我这个菜鸟（累计番茄12*45mins）！卷积层、池化层、全连接什么的概念弄得我一头雾水，到现在我都没怎么弄明白中间的计算过程是怎样实现的，只是知道CNN大概是模仿人类的视觉体验，通过提取图像的局部特征而对获取事物的整体感知，还有参数个数的计算，至于如何去提取那就跟卷积层个数、步长、采样方法的选取有关了。要想彻彻底底弄明白这些东西，不是看几篇东抄抄西抄抄的中文博客就可以的，必须得要看论文和阅读代码！ 处理这个任务的时候还出了一点小插曲，运行代码的时候出现dlerror: cudnnCreate not found的报错信息。开始以为是cudnn的文件误删了，又或者是tensorflow在不同操作系统下的问题，于是我尝试重新下载cudnn还有到github上提问，甚至还安装了Ubuntu！在这里花费了我不少“番茄”，最终还是到QQ群里提问得知windows当前只支持cudnn5版本，重新下载cudnn5之后再更新了CUDA安装路径下的bin、lib、include文件运行成功！而我之前是用的6.0的版本。做到任务4了还犯这么低级的错误，真是羞愧。 几个有用的链接 卷积神经网络的可视化 cs231n Visualizing and Understanding Convolutional Networks]]></content>
      <categories>
        <category>Machine-Learning</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Machine-Learning</tag>
        <tag>深度学习</tag>
        <tag>图像处理</tag>
        <tag>卷积神经网络</tag>
        <tag>CNN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Windows10下Hadoop的安装]]></title>
    <url>%2F2017%2Fhadoop-setup-on-win10%2F</url>
    <content type="text"><![CDATA[一、Hadoop下载和添加环境变量稳定版Hadoop下载地址：https://mirrors.tuna.tsinghua.edu.cn/apache/hadoop/common/stable/，选择大小为204M名为`hadoop-2.7.3.tar.gz`的安装包，然后解压到硬盘（我的放在E盘了）。 添加环境变量添加“HADOOP_HOME”系统变量，并添加到系统变量的Path中，按照下图操作 1.找到“高级系统设置” 2.点击“环境变量” 3.新建系统变量 4.将新建系统变量添加到Path中 二、安装JDKJDK的安装很重要！！！ JDK（Java SE Development Kit）是使用 Java 编程语言构建应用、小程序和组件的开发环境。JDK下载地址：http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html，根据计算机操作系统（我的是Windows64位），选择下载相应的安装包。 jdk默认会安装在C:\Program Files下，而这样做是不可以的，因为在接下来配置Hadoop的时候，Hadoop会因为C:\Program Files路径中有一个空格而出现JAVA_HOME无法找到的错误。 所以，我在E盘下新建了一个名为java（记住文件夹名不能有空格！）的空文件夹用于安装jdk，然后点击jdk安装包 1.记下jdk版本号，更改安装路径 2.找到目标安装文件夹，填写版本号 3.点击“下一步” 4.因为我事先已经安装了Java，所以安装jdk时，jdk会提醒我一并安装Java，但是这并不需要，所以放心关闭就好 5.点击“否” 6.jdk安装成功，可以看到安装文件已经在安装路径下了 到此为止，Hadoop环境变量的配置和jdk的安装完成了，Hadoop的安装差不多完成一半了。 三、配置Hadoop配置Hadoop的四个关键文件如下： 文件名称 格式 描述 hadoop-env.cmd Windows命令脚本 记录脚本中要用到的环境变量，以运行Hadoop core-site.xml Hadoop配置XML Hadoop Core的配置项，例如HDFS和mapreduce常用的I/O设置 hdfs-site.xml Hadoop配置XML Hadoop守护进程的配置项，包括namenode、辅助namenode和datanode等 mapred-site.xml Hadoop配置XML mapreduce守护进程的配置项，包括jobtracker和tasktracker（每行一个） 下面给出我的配置信息，大家打开文件后直接添加便可 1.编辑hadoop-2.7.3\hadoop-2.7.3\etc\hadoop\hadoop-env.cmd文件 12@rem The java implementation to use. Required.set JAVA_HOME=E:\java\jdk1.8.0_131 # 填写你的jdk安装路径，路径有空格的话会报错 2.编辑hadoop-2.7.3\hadoop-2.7.3\etc\hadoop\core-site.xml文件 123456&lt;configuration&gt;&lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://localhost:9000&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 3.编辑hadoop-2.7.3\hadoop-2.7.3\etc\hadoop\hdfs-site.xml文件 1234567891011121314&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;/hadoop/data/namenode&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;/hadoop/data/datanode&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 4.编辑hadoop-2.7.3\hadoop-2.7.3\etc\hadoop\mapred-site.xml文件 123456&lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 5.编辑hadoop-2.7.3\hadoop-2.7.3\etc\hadoop\yarn.xml文件 12345678910&lt;configuration&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services.mapreduce.shuffle.class&lt;/name&gt; &lt;value&gt;org.apache.hadoop.mapred.ShuffleHandler&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 四、格式化并启动Hadoop格式化HDFS文件系统，hdfs namenode -format 打开cmd，cd到\hadoop\hadoop-2.7.3\sbin，输入start-all，启动Hadoop，同时弹出四个窗口，Namenode、Datanode、YARN resourcemanager、YARN nodemanager四个进程启动成功。 打开localhost:8088，进入web页面，Hadoop安装成功。 参考1.http://toodey.com/2015/08/10/hadoop-installation-on-windows-without-cygwin-in-10-mints/]]></content>
      <categories>
        <category>教程</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>安装教程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[处理不平衡数据——基于UCI人口调查数据集（一）]]></title>
    <url>%2F2017%2FR-imbalanced-data-1%2F</url>
    <content type="text"><![CDATA[Like it or not , a bitter truth about data science industry is that self-learning, certification, coursework is not sufficient to get you a job. 本项目github地址：https://github.com/swordspoet/UCI_imbalanced_data 前言这是一篇关于如何处理美国人口调查不平衡数据集的文章，是基于R来处理的。 学习R语言，如果没有在实际数据集当中去来回操作，说到底还是纸上谈兵。大家还是相信这句话，Talk is cheap, show me the code。个人觉得R语言的学习曲线比较高，当中的东西太多太杂，编码起来特别麻烦，文档方面做得也不如Python那么全面，要想在短时间内在R中做到触类旁通简直艰难。在完成这个小项目的过程中，踩了好多坑，切切实实体会到了R在处理规模数据方面的性能短板，还有教科书提到的数据清洗和工程应用中的数据清洗完全是两回事。写完这篇文章之后，感觉自己的编程能力太渣，代码的编写阅读量还是太少，任重道远~ 在实际工程中，有一些极端条件（欺诈检测、癌症诊断、在线广告捕捉）会使得数据集十分不平衡，毕竟干坏事、得绝症的个体在人群中的比例是较小的，这样就导致目标变量的分布可能会呈现极端分布态势，给我们的模型预估带来困难。 做这个项目有什么好处呢？ 处理不平衡数据是一个技巧性的工作 数据的维度高，因此，它可以帮助你理解和克服机器的内存问题 提升你的编程、数据分析和机器学习技能 一、描述问题&amp;生成假设 任务要求：给定拥有多个变量的数据集，目标是根据这些变量来建立一个预测收入水平是大于50000还是小于50000的分类模型。 Prediction task is to determine the income level for the person represented by the record. Incomes have been binned at the $50K level to present a binary classification problem, much like the original UCI/ADULT database. 从问题的描述来看，这是一个二分类问题，任务要求目标函数输出的结果只有两种类型，即“是”或者“否”。 在这个项目中我们用到的是来自UCI机器学习的数据集，这是一份美国人口的调查数据，打开数据集，我们会发现列名全部都丢失了，完好的数据集在这里下载训练集、测试集。 一般，我们拿到数据要做的第一件事便是先打开看看，思考数据量有多大？有多少个变量？变量是连续型的还是非连续型的？有没有缺失数据？目标变量在哪里？······ 二、探索数据123456# 加载包和数据集# na.string=c()定义多个变量（"" " " "?" "NA" NA）为NAlibrary(data.table)train &lt;- fread("E:/R/imbalancedata/train.csv",na.string=c(""," ","?","NA",NA)) test &lt;- fread("E:/R/imbalancedata/test.csv",na.string=c(""," ","?","NA",NA)) 从数据加载之后的情况来看，训练集中有199523个观测值，41个变量，测试集的观测值有99762个，同样也是41个变量。回到我们的目标，我们的目标是要建立预测收入的模型，那么因变量是什么？是收入水平（income_level）。 找到因变量（income_level），训练集和测试集下的income_level变量下只有-50000和+50000两种取值分别对应小于50000和大于50000两种取值。为了方便，将income_level下的取值替换为0（小于50000）和1（大于50000）。 1234567unique(train$income_level)unique(test$income_level)[1] "-50000" "+50000"[1] "-50000" "50000+."# 将目标变量取值替换为0,1，ifelse(test,yes,no)train[,income_level:= ifelse(income_level == "-50000",0,1)] test[,income_level:= ifelse(income_level == "-50000",0,1)] 接着，继续查看训练集的正负样本分布情况 123round(prop.table(table(train$income_level))*100) 0 1 94 6 从返回结果可以看出，原始训练集的正负样本分布非常不均衡，收入水平小于5万的人群占据了94%，大于5万的人群仅仅占了6%（毕竟有钱人还是少！）。这是一个典型的不平衡数据集，正负样本差别太大，很容易对模型的准确性造成误解。例如有98个正例，2个反例，那么只需要建立一个永远返回正例的预测模型就能轻松达到98%的精确度，这样显然是不合理的。 那么如何由这种不平衡数据集学习到一个准确预测收入水平的模型呢？这是主要要解决的难题，在解决这个问题之前需要对数据进行预处理。 三、数据预处理首先，从数据集介绍的页面可以了解到，变量被分为nominal（名义型）和continuous（连续型）两种类型，即分别对应类别型和数值型。 对数据进行预处理时，我们先将这两种不同的数据类型切分开来分别处理，data.table包可以帮助我们快速简单地完成这个任务。 12345678910111213#数据集介绍的页面已经告诉我们哪些特征为nominal或是continuousfactcols &lt;- c(2:5,7,8:16,20:29,31:38,40,41)numcols &lt;- setdiff(1:40,factcols)# lapply的.SD的用法train[,(factcols) := lapply(.SD, factor), .SDcols = factcols]train[,(numcols) := lapply(.SD, as.numeric), .SDcols = numcols]test[,(factcols) := lapply(.SD, factor), .SDcols = factcols]test[,(numcols) := lapply(.SD, as.numeric), .SDcols = numcols]# 将训练集和测试集中的类别变量和数值变量分别提取出来cat_train &lt;- train[,factcols, with=FALSE]cat_test &lt;- test[,factcols,with=FALSE]num_train &lt;- train[,numcols,with=FALSE]num_test &lt;- test[,numcols,with=FALSE] 经过上述的处理，训练集和测试集分别被拆分为类别型和数值型两个部分，一共是4个子数据集，并且在处理的过程中规定了它们的类型是factor还是numeric。 四、数据可视化单纯查看数据集无法得到直观的感受，“一图胜千言”，图形是最简单直观的办法，下面我们会用到ggplot2和plotly两个强大的绘图包。 123456789101112library(ggplot2)library(plotly)# geom_histogram()直方图# geom_density()密度图# aes设定x轴y轴的名称tr &lt;- function(a)&#123;ggplot(data = num_train, aes(x= a, y=..density..)) +geom_histogram(fill="blue",color="red",alpha = 0.5,bins =100) + geom_density()ggplotly()&#125;tr(num_train$age)tr(num_train$capital_losses) 以上两个分布图符合我们的常识，年龄分布在0~90岁之间，年龄越大，所占比例越小。 自变量与因变量之间的关系我们分别考虑训练集中的数值型变量和类别型变量与income_level的关系。 首先看数值型的，数值型训练集num_train下有wage_per_hour、capital_gains、capital_losses、dividend_from_Stocks等等几个变量，我们选取了四个关联程度较大的指标，可以看出，大部分年龄段处于25-65的人收入水平income_level为1（大于50000），他们的小时工资（wage per hour）处在1000美元到4000美元的水平。这个事实进一步强化了我们认为年龄小于20的人收入水平小于50000的假设。 12345#income_level属于类别型的，被切分到了cat_train中num_train[,income_level := cat_train$income_level]ggplot(data=num_train,aes(x = age,y=wage_per_hour))+geom_point(aes(colour=income_level))+scale_y_continuous("wage perhour", breaks = seq(0,10000,1000)) 股票收益对收入的影响也是比较大的，收入水平大于50000的人群基本上股票分红都超过了30000美元 123num_train[,income_level := cat_train$income_level]ggplot(data=num_train,aes(x = age,y=dividend_from_Stocks))+geom_point(aes(colour=income_level))+ scale_y_continuous("dividend from stocks", breaks = seq(0,10000,5000)) 类似地，我们可以还可以查看capital_gains资本增值和capital_losses资本贬值与收入水平的关系，如下图所示 同样的，我们也可以将分类变量以可视化的形式展现出来，对于类别数据，dodged条形图比一般条形图能展更多的信息。在dodged条形图中，可以发现很多有趣的东西，比如本科毕业生年薪超过5万的人数最多，拥有博士学位的人年薪超过5万和低于5万的比例是相同的，白人年薪超过5万的人群远远多于其他肤色的种族 123all_bar &lt;- function(i)&#123; ggplot(cat_train,aes(x=i,fill=income_level))+geom_bar(position = "dodge", color="black")+scale_fill_brewer(palette = "Pastel1")+theme(axis.text.x =element_text(angle = 60,hjust = 1,size=10))&#125; 五、数据清洗检查遗漏值数据清洗时数据分析的一个重要步骤，首先检查训练集和测试集中是否有遗漏值 12table(is.na(num_train))table(is.na(num_test)) 从反馈的结果来看，FALSE分别等于1396661=199523×7，698334=99762×7，故训练集和测试集中的数值型（num）部分没有一个遗漏值，这是一个不错的消息！而检查类别型（cat）数据时，数据遗漏的情况就比较严重了。 12345678&gt; table(is.na(cat_train)) FALSE TRUE 6367191 416591 &gt; table(is.na(cat_test)) FALSE TRUE 3183124 208784 删除高度相关变量变量之间的相关性对模型的准确性存在影响，caret包能够帮助我们检查数值型变量之间的相关性，筛选出高度相关的变量 12345library(caret)x &lt;- cor(num_train)ax &lt;- findCorrelation(x, cutoff=0.7)num_train &lt;- num_train[,-ax,with=FALSE]num_test &lt;- num_test[,weeks_worked_in_year := NULL] 筛选的结果显示，weeks_worked_in_year变量与其他变量存在相当高的相关性。这很好理解，因为一年之中工作的时间越长那么相应的工资、回报也会随之上涨，所以要把这个高度相关性的变量剔除掉，这样num_train当中就只剩下7个变量了。 似乎数值型的数据已经处理得差不多了，下面我们来看看类别型的数据cat_train，首先检查每一列数据的遗漏情况 12345mvtr &lt;- sapply(cat_train, function(x)&#123;sum(is.na(x))/length(x)&#125;)*100mvte &lt;- sapply(cat_test, function(x)&#123;sum(is.na(x)/length(x))&#125;*100)mvtrmvte 大部分的列情况比较乐观，但是有的列甚至有超过50%的数据遗漏（这有可能是由于采集数据难度所致，特别是人口普查），将遗漏率小于5%的列挑选出来，遗漏率太高的变量剔除掉。 12345678910111213141516cat_train &lt;- subset(cat_train, select = mvtr &lt; 5 )cat_test &lt;- subset(cat_test, select = mvte &lt; 5)#对于挑选出来的遗漏率小于5%的列的遗漏值，比较好的办法是将其标记为“Unavailable”cat_train &lt;- cat_train[,names(cat_train) := lapply(.SD, as.character),.SDcols = names(cat_train)]for (i in seq_along(cat_train)) set(cat_train, i=which(is.na(cat_train[[i]])), j=i, value="Unavailable")cat_train &lt;- cat_train[, names(cat_train) := lapply(.SD,factor), .SDcols = names(cat_train)]cat_test &lt;- cat_test[, (names(cat_test)) := lapply(.SD, as.character), .SDcols = names(cat_test)]for (i in seq_along(cat_test)) set(cat_test, i=which(is.na(cat_test[[i]])), j=i, value="Unavailable")cat_test &lt;- cat_test[, (names(cat_test)) := lapply(.SD, factor), .SDcols = names(cat_test)] 六、数据操作——规范化处理在前面的分析中，有的类别变量下个别水平出现的频率很低，这样的数据对我们的分析作用不是特别大。在接下来的步骤中，我们的任务是将这些变量下频率低于5%的水平字段设置为”Other”。处理完类别型数据之后，对于数值型数据，各个变量下的水平分布过于稀疏，所以需要将其规范化。 123456789101112#将cat_train和cat_test中每列下出现频率低于5%的水平设置为“Other”for(i in names(cat_train))&#123; p &lt;- 5/100 ld &lt;- names(which(prop.table(table(cat_train[[i]])) &lt; p )) levels(cat_train[[i]])[levels(cat_train[[i]]) %in% ld] &lt;- "Other"&#125;for(i in names(cat_test))&#123; p &lt;- 5/100 ld &lt;- names(which(prop.table(table(cat_test[[i]])) &lt; p )) levels(cat_test[[i]])[levels(cat_test[[i]]) %in% ld] &lt;- "Other"&#125; 12345678910111213141516171819202122232425#"nlevs"参数：返回每列下维度的个数，测试集和训练集是否匹配summarizeColumns(cat_train)[,"nlevs"] summarizeColumns(cat_test)[,"nlevs"]num_train[,.N,age][order(age)]num_train[,.N,wage_per_hour][order(-N)]#以0，30，90为分隔点，将年龄段划分为三个区段，“young”，“adult”，“old”num_train[,age:= cut(x = age,breaks = c(0,30,60,90),include.lowest = TRUE,labels = c("young","adult","old"))]num_train[,age := factor(age)]num_test[,age:= cut(x = age,breaks = c(0,30,60,90),include.lowest = TRUE,labels = c("young","adult","old"))]num_test[,age := factor(age)]#将wage_per_hour，capital_gains，capital_losses，dividend_from_Stocks设置为只有0和大于0两个水平num_train[,wage_per_hour := ifelse(wage_per_hour == 0,"Zero","MoreThanZero")][,wage_per_hour := as.factor(wage_per_hour)]num_train[,capital_gains := ifelse(capital_gains == 0,"Zero","MoreThanZero")][,capital_gains := as.factor(capital_gains)]num_train[,capital_losses := ifelse(capital_losses == 0,"Zero","MoreThanZero")][,capital_losses := as.factor(capital_losses)]num_train[,dividend_from_Stocks := ifelse(dividend_from_Stocks == 0,"Zero","MoreThanZero")][,dividend_from_Stocks := as.factor(dividend_from_Stocks)]num_test[,wage_per_hour := ifelse(wage_per_hour == 0,"Zero","MoreThanZero")][,wage_per_hour := as.factor(wage_per_hour)]num_test[,capital_gains := ifelse(capital_gains == 0,"Zero","MoreThanZero")][,capital_gains := as.factor(capital_gains)]num_test[,capital_losses := ifelse(capital_losses == 0,"Zero","MoreThanZero")][,capital_losses := as.factor(capital_losses)]num_test[,dividend_from_Stocks := ifelse(dividend_from_Stocks ==0,"Zero","MoreThanZero")][,dividend_from_Stocks := as.factor(dividend_from_Stocks)] 组合数据 mlr()是R语言中专门应对机器学习问题而开发的包，而在mlr出现之前，R语言中是不存在像Scikit-Learn这样的科学计算工具的。在R语言中使用mlr包只要记住三个步骤即可： Create a Task：创建一个任务task Make a Learner：给创建的任务选择相应的算法，如分类、聚类等等 Train Them：然后再训练它！ 1234567891011d_train &lt;- cbind(num_train, cat_train)d_test &lt;- cbind(num_test, cat_test)library(mlr)train.task &lt;- makeClassifTask(data = d_train, target = "income_level")test.task &lt;- makeClassifTask(data = d_test, target = "income_level")# Remove constant features from a data set.# Constant features can lead to errors in some models # and obviously provide no information in the training set that can be learned fromtrain.task &lt;- removeConstantFeatures(train.task)test.task &lt;- removeConstantFeatures(test.task) 七、处理不平衡数据集应对不平衡数据集，通常的技巧有上采样(oversampling)、下采样(undersampling)，以及过采样的一种代表性方法SMOTE(SyntheticMinority Oversampling TEchnique)算法。 上采样：即增加一些正例使得正、反例数目接近，然后再进行学习 下采样：去除一些反例使得正、反例数目接近，然后进行学习 SMOTE：SMOTE 是上采样的代表算法，它通过对训练集里的正例进行插值来产生额外的正例，主要思想是通过在一些位置相近的少数类样本中插入新样本来达到平衡的目的 EasyEnsemble：EasyEnsemble 是下采样的代表性算法，它利用集成学习机制，将样本划分为若干个集合供不同的学习器使用 下采样法的时间开销通常远远小于上采样法，因为前者丢弃了很多反例，使得训练集远小于初始训练集，下采样另外一个可能的缺陷在于它可能会导致信息的丢失。上采样法增加了很多正例，训练集的大小大于初始训练集，训练时间开销变大，而且容易导致过拟合。 更多关于SMOTE采样方法，Chawla 的这篇文章有详细的说明。 1234567train_under &lt;- undersample(train.task, rate = 0.1)table(getTaskTargets(train_under))train_over &lt;- oversample(train.task, rate = 15)table(getTaskTargets(train_over))system.time(train_smote &lt;- smote(train.task, rate = 15, nn=3)) 八、训练Naive Bayesian分类器123456789naive_learner &lt;- makeLearner("classif.naiveBayes", predict.type = "response")naive_learner$par.vals &lt;- list(laplace = 1)folds &lt;- makeResampleDesc("CV", iters = 10, stratify = TRUE)fun_cv &lt;- function(a)&#123; crv_val &lt;- resample(naive_learner,a,folds,measures = list(acc,tpr,tnr,fpr,fp,fn)) crv_val$aggr&#125; 模型评估12345678910111213141516171819202122232425262728293031323334# 训练结果，从训练结果得知，对不平衡数据集采取不同的采样# 方法得出的结果截然不同fun_cv(train.task)# Result: # acc.test.mean=0.732,tpr.test.mean=0.721,tnr.test.mean=0.896,# fpr.test.mean=0.104,fp.test.mean= 129,fn.test.mean=5.21e+03fun_cv(train_under)# Result:# acc.test.mean=0.785,tpr.test.mean=0.656,tnr.test.mean=0.914,# fpr.test.mean=0.0856,fp.test.mean=1.59e+03,fn.test.mean=6.44e+03fun_cv(train_over)# Result: acc.test.mean=0.785,tpr.test.mean=0.656,# tnr.test.mean=0.914,fpr.test.mean=0.0856,fp.test.mean=1.59e+03,fn.test.mean=6.44e+03fun_cv(train_smote)# Result: acc.test.mean=0.896,tpr.test.mean=0.842,tnr.test.mean=0.951,# fpr.test.mean=0.0488,fp.test.mean= 906,fn.test.mean=2.96e+03nB_model &lt;- train(naive_learner, train_smote)nB_predict &lt;- predict(nB_model, test.task)nB_prediction &lt;- nB_predict$data$responsedCM &lt;- confusionMatrix(d_test$income_level,nB_prediction)precision &lt;- dCM$byClass['Pos Pred Value']recall &lt;- dCM$byClass['Sensitivity']Specificity &lt;- dCM$byClass['Specificity']# 准确率precision ：0.844# 召回率recall 0.985# 真阴性率Specificity：0.254# F值f_measure &lt;- 2*((precision*recall)/(precision+recall))f_measure# Pos Pred Value 0.9089276 由实验结果，学习得到的模型达到0.844的准确率和0.985的召回率，而真反例仅仅为0.254。也就是说明训练得到的模型在预测收入低于50000的时候表现还行，而一旦要预测收入大于50000的时候表现就不佳了，测试集中仅仅只有25.4%的负样本被检测出来了。这样的结果不太令人满意，在接下来的文章中我们继续探讨其他模型是不是有更好的效果。 （未完） 参考文献 Chawla N V, Bowyer K W, Hall L O, et al. SMOTE: synthetic minority over-sampling technique[J]. Journal of artificial intelligence research, 2002, 16: 321-357. Chawla N V. Data mining for imbalanced datasets: An overview of Data mining and knowledge discovery handbook. Springer US, 2005: 853-867. This Machine Learning Project on Imbalanced Data Can Add Value to Your Resume]]></content>
      <categories>
        <category>Machine-Learning</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>数据分析</tag>
        <tag>R</tag>
        <tag>Machine-Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[优达学城-深度学习任务3：正则化]]></title>
    <url>%2F2017%2Fdeep-learning-assignment-3-regularization%2F</url>
    <content type="text"><![CDATA[接上次任务2，深度学习任务3引入了新的模型优化技术——正则化，通过在LR与神经网络模型中添加正则化项，使得模型的准确率有了显著的提升。 参考 Regularization with TensorFlow 谷歌深度学习公开课任务 3: 正则化]]></content>
      <categories>
        <category>Machine-Learning</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Machine-Learning</tag>
        <tag>深度学习</tag>
        <tag>图像处理</tag>
        <tag>正则化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[What does -1 mean in numpy reshape?]]></title>
    <url>%2F2017%2Fwhat-does--1-mean-in-numpy-reshape%2F</url>
    <content type="text"><![CDATA[numpy.reshape(a, newshape, order=’C’)[source]，参数newshape是啥意思？ 根据Numpy文档的解释： newshape : int or tuple of ints The new shape should be compatible with the original shape. If an integer, then the result will be a 1-D array of that length. One shape dimension can be -1. In this case, the value is inferred from the length of the array and remaining dimensions. 大意是说，数组新的shape属性应该要与原来的配套，如果等于-1的话，那么Numpy会根据剩下的维度计算出数组的另外一个shape属性值。 举几个例子或许就清楚了，有一个数组z，它的shape属性是(4, 4) 123456z = np.array([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12], [13, 14, 15, 16]])z.shape(4, 4) z.reshape(-1) 12z.reshape(-1)array([ 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]) z.reshape(-1, 1) 也就是说，先前我们不知道z的shape属性是多少，但是想让z变成只有一列，行数不知道多少，通过z.reshape(-1,1)，Numpy自动计算出有12行，新的数组shape属性为(16, 1)，与原来的(4, 4)配套。 1234567891011121314151617z.reshape(-1,1)array([[ 1], [ 2], [ 3], [ 4], [ 5], [ 6], [ 7], [ 8], [ 9], [10], [11], [12], [13], [14], [15], [16]]) z.reshape(-1, 2) newshape等于-1，列数等于2，行数未知，reshape后的shape等于(8, 2) 123456789z.reshape(-1, 2)array([[ 1, 2], [ 3, 4], [ 5, 6], [ 7, 8], [ 9, 10], [11, 12], [13, 14], [15, 16]]) 同理，只给定行数，newshape等于-1，Numpy也可以自动计算出新数组的列数。 参考1.stackoverflow: What does -1 mean in numpy reshape?]]></content>
      <categories>
        <category>coding</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Numpy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[优达学城-深度学习任务2：SGD]]></title>
    <url>%2F2017%2Fdeep-learning-assignment-2-sgd%2F</url>
    <content type="text"><![CDATA[接任务1处理过后的数据，在优达学城深度学习任务2里，首次运用了google的深度学习框架Tensorflow（GPU的速度杠杠的），加入ReLUs（修正线性单元）搭建起第一个神经网络。还比较两种优化方法：梯度下降和随机梯度下降，明显后者的速度更快效果更好。个人觉得到底是针对“懒惰工程师”的课程，整个课程一个公式都没有（当然既然调用现成的框架在课程里就暂时可以不去了解），设计者主要的目的是要听众掌握解决问题的“模板”。学完这个任务发现有几个非常重要的Numpy用法，有时间再整理一下。]]></content>
      <categories>
        <category>Machine-Learning</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Machine-Learning</tag>
        <tag>深度学习</tag>
        <tag>图像处理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[优达学城-深度学习任务1：notMNIST]]></title>
    <url>%2F2017%2Fdeep_learning_assignment_1%2F</url>
    <content type="text"><![CDATA[这是优达学城推出的深度学习课程任务1，主办方给了两个英文字母的数据集（A-J）：notMNIST_large和notMINIST_small，通过预处理数据集，在此基础上训练一个简单的逻辑回归模型， 这个模型能够识别不同字体的英文字母的一个子集。通过训练全部200000张图片，模型的准确率达到0.8945。 任务介绍前一段时间打算学习fast.ai推出的深度学习系列课程，课程主讲的两个老师说了很多大实话，他们做的教程也很好，课程论坛十分活跃，提出的问题都可以得到回应。但是后来发现课程有太多地方没有完善，于是就放弃了，转而投奔优达学城的深度学习课程。 任务1没有涉及太多高级技巧，可对于我这种第一次处理图像数据的菜鸟还是挺伤脑筋的，google给出的代码并不完全正确，有几处bug。 正如业内人士说的，在XX中，大多数时间都被用在清洗数据上，这个任务也不例外。任务1中预处理数据占据全部任务时间的90%，包括将图像数据转换成3D数组、归一化、验证数据平衡、处理重叠样本（通过比较哈希值实现快速找出重复样本）等数据预处理方法，而训练所用的模型是现成的（off-the-shelf），来自sklearn.linear_model的LogisticRegression，最后达到的准确率也还不错。 解决过程]]></content>
      <categories>
        <category>Machine-Learning</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Machine-Learning</tag>
        <tag>深度学习</tag>
        <tag>图像处理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Matplotlib绘图举例]]></title>
    <url>%2F2017%2Fpython-matplotlib-plot-examples%2F</url>
    <content type="text"><![CDATA[本文主要介绍如何用Matplotlib绘制简单的图形，特别需要理解子图（subplot）的用法 Matplotlib是Python的绘图工具的核心，功能十分强大，可以实现各种各样的效果，而功能强大的弊端便是学习成本较高。Matplotlib绘图参数众多，要想全部掌握这些参数的用法可能性不大，较好的方法是先掌握几个核心图形的绘制技巧，比如散点图、曲线图、直方图等等，还有就是子图（subplot）的概念。 子图是允许在一张图片中同时绘制多个不同的图形，这样绘制出来的图片比较适合对比，使决策者能够在较短时间内了解事件的全局。 散点图、曲线图、直方图、subplot123456789101112131415161718192021222324252627282930313233343536373839404142434445464748import numpy as npimport matplotlib.pyplot as pltfig = plt.figure()# 散点图1N = 50x = np.random.rand(N)y = np.random.rand(N)colors = np.random.rand(N)# 散点图2n = 10X = np.random.uniform(0, 1, n)Y = np.random.uniform(0, 10, n)area = np.pi * (15 * np.random.rand(n))**2# 曲线图A = np.linspace(-np.pi, np.pi, 256, endpoint=True) # np.linspace(x, y)设定变量的取值范围C, S = np.cos(A), np.sin(A)# 直方图x_1 = np.arange(-10, +10, dtype=int)y_1 = np.random.normal(0, 1, 1000) # 生成1000个服从（0,1）分布的随机数# 散点图1-子图(2, 2, 1)plt.subplot(221)plt.title('scatter plot with area', fontsize=16, fontweight='bold') # plt.title()给子图添加标题plt.xticks([-0.1, 0, +1])plt.yticks([-0.1, 0, +1]) # 设置x轴、y轴的刻度plt.scatter(x, y, s=area, c=colors, alpha=0.5)# 散点图2-子图(2, 2, 2)plt.subplot(222)plt.title('scatter plot', fontsize=16, fontweight='bold')plt.scatter(X, Y)# 曲线图-子图(2, 2, 3)plt.subplot(223)plt.plot(A, C)plt.plot(A, S)# 给子图(2, 2, 3)添加图例plt.plot(A, C, color="blue", linewidth=2.5, linestyle="-", label="cosine")plt.plot(A, S, color="red", linewidth=2.5, linestyle="-", label="sine")plt.legend(loc='upper left', frameon=False)# 直方图-子图(2, 2, 4)plt.subplot(224)plt.hist(y_1, 20, normed=False)plt.xlabel('Value')plt.ylabel('Frequency')plt.xlim(-10, +10) # 设定x轴的取值范围plt.show() 饼图12345678910111213141516171819mport numpy as npimport matplotlib.pyplot as plt# 饼图ax = plt.axes([0.025, 0.025, 0.95, 0.95], polar=True)N = 20theta = np.arange(0.0, 2*np.pi, 2*np.pi/N)radii = 10*np.random.rand(N) # 生成1到10的随机数width = np.pi/4*np.random.rand(N)bars = plt.bar(theta, radii, width=width, bottom=0.0)for r, bar in zip(radii, bars): bar.set_facecolor(plt.cm.jet(r/10.)) bar.set_alpha(0.5)ax.set_xticklabels([])ax.set_yticklabels([])plt.show() 参考 http://www.labri.fr/perso/nrougier/teaching/matplotlib/]]></content>
      <categories>
        <category>coding</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>绘图</tag>
        <tag>matplotlib</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[思想王国|硅谷钢铁侠]]></title>
    <url>%2F2017%2Felon-musk-silicon-valley-iron-man%2F</url>
    <content type="text"><![CDATA[“我要你考虑超前的事情，我要你能够用力地思考，每一天都思考到头疼。我希望你每天晚上睡觉的时候头都会疼。” 领导之道马斯克是一个爱说大话的人，他经常会发布连影都见不到的计划；另外他对产品的要求“朝令夕改”，把工程师折磨得死去活来；如果员工胆敢提加薪的要求，他马上会让他扫地出门；他对外号称只用两年便可以让“猎鹰1号”升空，结果却花了6年时间。由此看来，马斯克是个不折不扣的“混球”。可是为什么仍然会有一大批人死心塌地地跟着他？ “这不是借口。我感到非常失望。你需要弄清楚，什么对你来说更重要。我们正在改变世界、改变历史，如果你不打算全力以赴，那你就别干了。”—马斯克给因为孩子出生而错过活动员工的邮件 我认为其中关键的一点在于，马斯克拥有“达尔文式的乐观”，猎鹰1号到猎鹰9号，作为一个商业发射机构，SpaceX从零开始制造火箭，期间经历了无数挫折，每次失败对于正常人来说都是致命的。可马斯克不同，他是怀着拯救地球使命的超级英雄！他有着精确描绘未来的抽象能力，使得工程师们不得不相信“不可能”也是有可能的。马斯克经常说的一句话是“我知道我们肯定能做到，只是花多少精力和时间的问题”。碰巧的是，实际结果往往跟他的设想一致，比如坚持Model S使用铝制车身以减轻重量、经费消耗殆尽花了6年时间直到第四次发射才成功的“猎鹰1号”（之后便获得了NASA26亿美元的资助，波音获得了42亿美元）。 就在前几天，SpaceX再次打破记录，成功实现回收火箭重复利用，距离猎鹰9号成功发射并且一级火箭成功回收（2015年12月21日）不过两年，至此它们已经成功完成8次火箭回收。 工作哲学假如要造一辆汽车，不管那么多，先让它跑起来再说！ 快速地学习并做出实物是马斯克的工作哲学，他本人学东西也很快，马斯克给工程师们制定的任务往往“不可能完成”，并且成本还要低。比如波音公司要完成一个涡轮泵需要大概5年的时间，他要求工程师尽力而为，最终用了13个月完成了，并且成本大大低于同业。马斯克的商业模式优势是传统的航空企业所不能比拟的，传统企业往往过于谨慎而止步不前，在他们看来，SpaceX不过一个异想天开的富翁的玩物，对于SpaceX采用一般工业原料从而竭力减轻制造成本的举动不屑一顾。 马斯克预期10年后，公司的日营收入可以达到1000万美元，所以时间是当前最紧要的问题。所以进度每拖延一天，就相当于损失1000万美元。 从这个角度看，“从0到1”也有其独特的优势，SpaceX与Model S就像一个“野蛮人”进入了瓷器博物馆，它们无视规则（因为它们本身就毫无规则可言），一切从实用主义出发，无论黑猫还是白猫，能够抓老鼠的就是好猫。马斯克削减成本的商业模式使得SpaceX每次发射只收取9000万美元，而竞争对手则需要3.8亿美元！ 真实而不造作的人 马斯克真正突出的地方在于，他将复杂的物理概念与商业计划结合的能力。不仅如此，他还显示了将一项科研成果转化为营利性企业的非凡才能。 马斯克有两面，一面是严谨务实的工程师，另一面则是视成本为生命的商人，再加上他“肩负拯救全人类”的伟大使命，喜怒无常、脾气古怪、“混球”这类的词眼形容马斯克也就不足为奇。没错，他就是一个混球，如果他认为你做出来的东西是一坨屎（前提是自己一定得要懂），那么他会直言不讳地说出来，“最严重的错误，就是告诉马斯克他的要求是无法实现的”。Zip2和PayPal的经历告诉他一定得掌握公司的控制权，只有暴君式的独裁管理模式才能够让存活率几乎为零的商业火箭发射机构生存下来。而且奇迹般地同时从事航天、电动汽车和太阳能三个长期发展停滞不前的行业，并且还取得了巨大的成功。 千万不要以传统的眼光来检验马斯克，因为他不是传统意义上的企业家，从他的身上你学不到成功的秘诀和管理的经验，“天才”+“使命”+“财富”=马斯克，三者缺一不可。 真想不到地球上还有如此具有英雄主义色彩的人，他竟然一心想把人类送上火星。 “嗯，咱们还是要再做些更有意思的事情！”]]></content>
      <categories>
        <category>思想王国</category>
      </categories>
      <tags>
        <tag>读书笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python OS文件目录方法]]></title>
    <url>%2F2017%2Fpython-os-file-methods%2F</url>
    <content type="text"><![CDATA[os.listdir(path) os.listdir()方法用于返回指定的文件夹包含的文件或文件夹的名字的列表。这个列表以字母顺序。 它不包括 ‘.’ 和’..’ 即使它在文件夹中。 path—指定文件夹路径，path是字符串格式，也可以通过“相加”得来，os.listdir(data_folders + &#39;/&#39; + str(folder_name)) 对如下图结构的文件夹，列出该文件夹包含的文件或文件夹名列表，可以用os.listdir()方法，它会返回一个list 12345&gt;&gt;&gt; import os&gt;&gt;&gt; data_folders = "E://Python/data/notMNIST_large"&gt;&gt;&gt; folder_names = os.listdir(data_folders)&gt;&gt;&gt; print(folder_names)['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J'] os.path.join() join是连接字符串的方法，遗憾的是，join()只能对一个变量进行拼接。如果需要对多个变量进行拼接就要用到os.path.join()方法了。 在Win10下用os.path.join()进行路径拼接会有一点麻烦，因为弄不明白拼接的符号为什么是双反斜杠“\\”，如此一来，拼接之后的路径名是不合法的，返回到Python就会蹦出“No such a directory”的错误，比如 12345&gt;&gt;&gt; import os&gt;&gt;&gt; src = 'E://Python/data/notMNIST_small' #路径&gt;&gt;&gt; file_name = 'A' #文件名&gt;&gt;&gt; os.path.join(src, file_name)'E://Python/data/notMNIST_small\\A' #显然这个路径的格式是错误的 在Win10下式支持单斜杠/的，那么用replace将双反斜杠\\\\替换为单斜杠/就可以解决路径错误的问题了 123456&gt;&gt;&gt; import os&gt;&gt;&gt; src = 'E://Python/data/notMNIST_small'&gt;&gt;&gt; file_name = 'A'&gt;&gt;&gt; m = os.path.join(src, dst)&gt;&gt;&gt; m.replace('\\', '/')'E://Python/data/notMNIST_small/A' #路径格式正确 os.stat() os.stat() 方法用于在给定的路径上执行一个系统 stat 的调用，返回文件的一些系统信息，如大小、修改时间、上一次访问的时间等等，stat()方法语法格式如下：os.stat(path) 1234import osstat_info = os.path('../As long as you love me.mp4')print(stat_info.st_size) # 返回文件大小 更多参考：Python OS文件目录的方法]]></content>
      <categories>
        <category>coding</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>文件目录</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习算法系列（8）神经网络与BP算法]]></title>
    <url>%2F2017%2Fmachine-learning-algorithm-series-neural-networks-and-backpropagation%2F</url>
    <content type="text"><![CDATA[机器学习中的神经网络（neural networks）算法受到生物界神经系统处理信息的启发，比如大脑处理信息的方式。跟人类一样，神经网络的训练也是一个学习的过程，通过大量的学习，神经网络能够完成特定的任务，比如图像分类识别、疾病预测判断等等。在这篇文章里将简单介绍神经元工作原理和神经网络模型，重点在于理解反向传导算法（BP）中参数的更新过程，并用一个实例解释了BP算法。 本文目录 神经网络 理解神经元的工作原理 简单神经元模型 神经网络模型 多层神经网络 反向传导（BP）算法思路 定义损失函数 随机初始化参数 计算残差 BP算法的一次实例更新迭代 第一步：前馈传导 第二步：反向传导 参考文献 神经网络理解神经元的工作原理“神经网络”一词起源于早先模拟（mimic）人类大脑的研究中，而发展到今天的“神经网络”早就已经是一个庞大的交叉学科领域，当然本文讨论的范围在机器学习与神经网络交叉的部分。在进行正式讨论之前，我们有必要先了解一些神经网络的前世今生。 关于神经网络最广泛的定义是 神经网络是由具有适应性的简单单元组成的广泛并且并行互连的网络，它的组织能够模拟生物神经系统对真实世界物体所作出的交互反应（Kohonen，1988） 从生物学的角度，大脑是人体的最高指挥中心，人的听觉、嗅觉、味觉、触觉、视觉等等感官感受都是通过神经网络来接收、传递、处理的。 神经网络的基本单元是神经元（neuron），神经元的基本组成有细胞核（cell）、树突（dendrite）、轴突（axon）、轴突末梢（axon terminal）。神经元有接收、处理、传递信息的功能。 当树突从其他神经元接收化学物质(多巴胺、乙酰胆碱)，神经元会变得“兴奋”，于是通过轴突将信息传递给另外的神经元，化学物质在树突与轴突的传递形成电流，如果两个神经元之间的电位差别超过“阈值”，那么它便会被“激活”，这就是神经元的基本工作模式。 在人类的大脑中约有860亿计个神经元，$10^{14}$到$10^{15}$个突触（synapse），正是这些简单的神经元组合可以完成各类复杂工作，于是成就了地球上最智慧的生物——人类。 简单神经元模型回到机器学习，首先，我们从最简单的神经元模型讲起，也就是神经网络中仅仅只有一个神经元（single neuron）的情形，如图示 在上面这个模型中，神经元收到来自其他来自3个神经元（$x_i,i=1,…,3$）的信息，即输入（input），这些输入通过带权重（weights）（$w_j,j=0,1,…,3$）的连接进行传递，对线性组合（$w\cdot x$）的传递汇总，进行激活（activation）处理之后再与当前神经元的阈值进行比较，如果大于阈值，那么该神经元则被激活。注意，这里我们通常将$x_0$和$w_0$均取为1，即把$w_0$当做偏置项（bias）。 将上述过程与生物学的神经元类比，蓝色的圈表示突触，桔色的圈表示细胞核，蓝色与桔色的圈之间的连接表示树突，激活之后的计算结果就好比电位差，只有存在电位差，神经元才有可能会被激活。 我们通常选择sigmoid函数作为激活函数$f(·)$（activation function） f(w,x)=\frac{1}{1+e^{-w^Tx}}细心的读者可能会有疑惑，前面说好的“阈值比较”过程去哪里了？不是还有一个判断的过程吗？其实，神经元模型中的激活函数便完成了这个任务，观察sigmoid函数 可以看出，线性组合经过sigmoid函数处理，输出值被映射到$(0,1)$的范围内，所以我们可以在此范围灵活地设置阈值以决定神经元是否被激活。“1”对应于激活，“0”对应于抑制。把许多个这样的神经元模型按照一定的结构联系起来，就得到了神经网络。除了sigmoid函数可以作为激活函数，常用的激活函数还有双曲正切函数（tanh）、 ReLU（Rectified Linear Unit）、Maxout等，这里就不详细展开。 神经网络模型像这样只有输入层和输出层两层的神经元模型也被叫做感知机，感知机能够实现简单的与、或、非逻辑运算，除此之外，感知机学习能力非常有限，对于线性不可分的问题束手无策。 讲到这里，有必要插播一段神经网络研究的历史，神经网络的发展可谓一波三折 首次尝试：1943年McCulloch和Pitts基于神经学奠定了神经网络的基础，拥有单个神经元的神经网络又被称为是“M-P神经元模型”，M-P神经元模型能够解决简单的逻辑问题。除此之外，还有两队人马也加入到了神经网络的研究队伍，分别是IBM和 (Farley and Clark, 1954; Rochester, Holland, Haibit and Duda, 1956)等人。 百家争鸣：神经网络在多个领域发扬光大，如心理学、工程行业。1958年Rosenblatt设计并发展了感知机，使它能够进行简单图像识别，在当时引起了极大轰动。 十年停顿：1969年Minsky和Papert写了一本名为《感知机》的书，书中详细论证了单层感知机无法解决非线性问题，而多层神经网络的训练算法则看不到希望，这一悲观情绪直接导致了神经网络研究的十年停顿，研究经费大幅缩减。 “…our intuitive judgment that the extension (to multilayer systems) is sterile” “···我们最先对多层神经网络系统的判断是徒劳的” 第二春：1986年Rumelhar和Hinton等人提出了反向传播（Backpropagation，BP）算法，解决了两层神经网络所需要的复杂计算量问题，从而带动了业界使用两层神经网络研究的热潮。 再陷低谷：20世纪90年代中期，Vapnik提出了支持向量机方法，由于不需要调参、全局最优、高效，并在比赛中表现非常好，支持向量机的出现打败神经网络，神经网络再陷低谷。 横空出世：2006年Hinton在《科学》发表论文，提出“深度学习”的概念。2012年，Hinton和他的两个学生在ImageNet图像分类比赛中借助深度学习一举夺魁，深度学习横空出世，谁与争锋！ 多层神经网络 神经网络最左边的一层叫做输入层（input layer），最右边的叫做输出层（output layer），中间的所有节点则称为隐藏层（hidden layer）。一般神经网络越复杂，它的隐藏层数则越多，理论上能够胜任的任务也就越复杂。上面的神经网络中，一共有三个输入单元（最下方的偏置项不算），三个隐藏单元和一个输出单元，它是一个回归模型（regression model）。 为了更好地描述神经网络，我们引入三个参数，系数$W_{ij}^{(l)}$表示第$l$层的$j$个单元与第$(l+1)$层第$i$单元的权重，系数$b_i^{(l)}$表示第$l$层第$i$单元的偏置项（注意，偏置项的权重取1），$a_i^{(l)}$表示$l$层第$i$个单元的激活值。 当神经网络得到输入$x_n$，$n$个输入由带权重的连接分别传递到下一层的各个节点，下一层节点将上一层传递下来的数值汇总传入激活函数$f(·)$，神经网络的参数计算过程如下 a_1^{(2)}=f(W_{11}^{(1)}x_1+W_{12}^{(1)}x_2+W_{13}^{(1)}x_3+b_1^{(1)})\\ a_2^{(2)}=f(W_{21}^{(1)}x_1+W_{22}^{(1)}x_2+W_{23}^{(1)}x_3+b_2^{(1)})\\ a_3^{(2)}=f(W_{31}^{(1)}x_1+W_{32}^{(1)}x_2+W_{33}^{(1)}x_3+b_3^{(1)})\\ h(W,x)=a_1^{3}=f(W_{11}^{(2)}a_1^{(2)}+W_{12}^{(2)}a_2^{(2)}+W_{13}^{(2)}a_3^{(2)}+b_1^{(2)}) （注：系数$W_{ij}^{(l)}$表示第$l$层的$j$个单元与第$(l+1)$层第$i$单元的权重） 我们可以简洁一点表示，令 \begin{equation}\begin{split}z^{(l+1)} = W^{(l)}x + b^{(l)} \a^{(l+1)} = f(z^{(l+1)})\end{split}\end{equation} 不考虑偏置项，拥有三个输入单元，三个隐藏单元和一个输出单元的神经网络涉及到12（3×3+3×1）个参数。 考虑该神经网络数值计算过程，像这样给定输入和参数，数值计算一层一层往前推进然后得到一个输出，没有反馈，没有循环的神经网络，我们也给它取了一个名字，前馈神经网络（feedforward neural network）。 实际上，神经网络模型还可以有多个隐藏层，每个隐藏层又可以设定多个参数；神经网络的输出也可以多个，如果具有多个输出，那么神经网络模型任务就由回归（一个输出）变成分类（多个输出）了。理论上神经网络越复杂，参数越多，它能够完成的任务就更复杂。比如2015年微软亚洲研究院在ImageNet计算机识别挑战赛中使用了深达百层的神经网络，比以往成功的神经网络层数多了5倍以上，系统错误率低至3.57%。 反向传导（BP）算法思路反向传导算法（Backpropagation Algorithm，简称BP算法）是优化神经网络参数迭代的主要算法，BP算法的思路是 前馈传导运算，得到各输出层的输出； 从输出层反推进行反向传导计算，利用梯度下降算法更新参数 在进行反向传导算法推导之前，我们有必要了解几个重要的概念，比如定义损失函数，在参数初始化上要保证随机性，以及最重要的是理解残差的概念。 神经网络的参数众多，而优化神经网络参数用到的批量梯度下降算法中主要的计算又是众多的求偏导计算，所以在进行反向求导时一定要明白参数之间的联系。 定义损失函数进行优化之前，要定义优化的目标是谁，即目标函数。对于单个样本，定义损失函数为 J(W,b;x,y)=\frac{1}{2} {||h(W,b,x)-y||}^2那么对于整个样本集，损失函数可以定义为 J(W,b)= \frac{1}{m} \sum J(W,b;x,y) + \frac{\lambda}{2} \sum W^2上式中的第一项为平方差项，第二项为正则化项，目的是为了防止过拟合。我们的目标是针对参数$W$和$b$来求$J(W,b)$的最小值。 随机初始化参数为了求解神经网络，我们需要将参数初始化为一个很小的，接近零的随机数，然后对目标函数使用优化算法。注意，如果所有的参数都是相同的，那么所有的隐藏单元都会得到相同的值，如此一来便失去了优化的意义，随机初始化就是要消除它。通常，我们可以让随机参数服从一个很小的正态分布。 计算残差残差在反向传导算法中是一个非常非常非常重要，而且有点点难理解的概念！！！ 本文中残差的定义来自斯坦福的教程，原教程的公式推导我觉得有点太“详细”了，经常是看到公式后面忘了字母的定义是什么，所以我在此基础上做了简化，希望能够方便理解。 在这之前，务必记住以下两个重要定义 z^{(l+1)} = W^{(l)}x + b^{(l)}我们定义残差为 \begin{equation}\begin{split}\delta_i^{(nl)} &amp;= \frac {\partial} {\partial{z_i^{(nl)}}} \frac{1}{2} (y_i - a_j^{(nl)})^2 \&amp;=\frac {\partial} {\partial{z_i^{(nl)}}} \frac{1}{2} (y_i - f(z_j^{(nl)}))^2 \&amp; =-(y_i-f(z_i^{(nl)}))\cdot f’(z_i^{(nl)})\&amp; =-(y_i-a_i^{(nl)}) \cdot f’(z_i^{(nl)})\\label{residual0}\end{split}\end{equation} 需要注意的是，残差中求偏导的对象是$z_i^{(nl)}$，至于为什么是$z_i^{(nl)}$而不是权重系数，下面会具体说明。 为什么是$z_i^{(nl)}$? 式$\eqref{residual0}$给出了$nl$层，也就是输出层向隐藏层的残差，那么考虑一个只有一个隐藏层的神经网络，再考虑向输入层推进呢？也就是求解$\delta_i^{(nl-1)}$，有 \begin{equation}\begin{split}\delta_i^{(nl-1)} &amp;= \frac {\partial} {\partial{z_i^{(nl-1)}}} \frac{1}{2} \sum (y_i - a_j^{(nl)})^2 \&amp;=\sum \frac {\partial} {\partial{z_i^{(nl-1)}}} \frac{1}{2} (y_i - f(z_j^{(nl)}))^2 \&amp; =\sum -(y_i-f(z_i^{(nl)}))\cdot f’(z_j^{(nl)}) \cdot \frac{\partial z_j^{(nl)}}{\partial z_j^{(nl-1)}}\&amp; =\sum \delta_j^{(nl)} \frac{\partial z_j^{(nl)}} {\partial z_j^{(nl-1)}}\&amp; =(\sum \delta_j^{(nl)} W^{nl-1})f’(z_i^{(nl-1)})\\label{residual1}\end{split}\end{equation} 这下就终于明白了为什么残差中求偏导的对象是$z_i^{(nl)}$了，因为$z_i^{(nl)}=W^{(nl)}x+b$，神经网络中各个层的权重参数众多，所以这样一来只需要对$z_i^{(nl)}$再求一次偏导，便可以得到想要的权重系数的偏导数。 BP算法的一次实例更新迭代第一步：前馈传导如下图，考虑一个神经网络，该神经网络一共有三层，其中有一个隐藏层，两个输入单元，两个隐藏层单元，两个输出单元，外加两个取值为1的偏置项。 以隐藏层第一个激活值$a_1^{(2)}$计算为例（神经网络选择sigmoid函数作为激活函数），连接线上的数字为随机初始化的权重参数，输入节点经加权汇总之后汇入隐藏层。计算方法如下 a_1^{(3)}=f(W_{11}^{(2)}a_1^{(2)}+W_{12}^{(2)}a_2^{(2)}+b_1^{(2)})a_2^{(3)}=f(W_{21}^{(2)}a_1^{(2)}+W_{22}^{(2)}a_2^{(2)}+b_1^{(2)}) 经过前馈传导运算，神经网络最终得到两个输出，分别是0.7513和0.7729。假设真实标签为[0.01, 0.99]，输出和真实标签有误差，需要对其进行优化。 第二步：反向传导计算总误差，总的误差 E_T = \frac{1}{2}\sum (y_i - a^{(nl)})^2, a^{(nl)}表示输出层所以 E_T = \frac{1}{2}(0.01-0.7513)^2+\frac{1}{2}(0.99-0.7729)^2 = 0.298 输出层→隐藏层的权值更新$W_{11}^{(2)}$ 考虑$W_{11}^{(2)}$对整体误差的影响，是一个链式求导的过程。 有 \frac {\partial{E_T}} {\partial{z_{11}^{(3)}}} =\frac {\partial{E_T}} {\partial{a_1^{(3)}}} \frac{\partial{a_1^{(3)}}}{\partial{z_{11}^{(3)}}}其中 z_{11}^{(3)} = f(W_{11}^{(2)}a_1^{(2)}+W_{12}^{(2)}a_2^{(2)}+b_1^{(2)})由式$\eqref{residual0}$，以及sigmoid函数的求导法则，可得 \delta_1^{3}=-(0.01-0.7513)*0.7513*(1-0.7513)=0.1385因为需要更新的权值是$W{11}^{(2)}$，所以再对$z{11}^{(3)}$求$W{11}^{(2)}$偏导即可。假定学习率为0.5，那么$W{11}^{(2)}$更新如下 W_{11}^{(2)} := W_{11}^{(2)} - \eta \cdot \delta_1^{3} \cdot a_1^{(2)}即 W_{11}^{(2)} := 0.4 -0.5*0.1385*0.5932=0.3589同理，我们也可以求得$\delta2^{3}$，并更新相应的权值$W{2j}^{(2)}$，计算过程可以参照$W_{11}^{(2)}$的更新 \delta_2^{3}= -(0.99-0.7729)*0.7529*(1-0.7529)=-0.0381 隐藏层→输入层权值更新$ W_{11}^{(1)} $ 然而仅仅更新一层的参数是不够的，既然是反向传导，参数的更新便是从右至左的过程，如果要更新输入层至隐藏层之间的参数，又该如何计算呢？如下图，$W_{11}^{1}$的权值（0.15）影响到了两个输出 $W{11}^{(1)}$影响到$a_1^{(2)}$的输出，而$a_1^{(2)}$接受来自两个输出$a_1^{(3)}$和$a_2^{(3)}$的误差，所以我们先将偏导数进行到$a_1^{(2)}$，实际上是求解$\sum \delta_j^{(3)} W{ij}^{2}$ \begin{split}\sum \deltaj^{(3)} W{ij}^{(2)} &amp;= \frac {\partial{E_T}} {\partial{a_1^{(3)}}} \frac{\partial{a_1^{(3)}}}{\partial {a_1^{(2)}}} + \frac {\partial{E_T}} {\partial{a_2^{(3)}}} \frac{\partial{a_2^{(3)}}}{\partial {a_1^{(2)}}} \&amp;=0.13850.4+(-0.0381)0.5 \&amp;=0.0538+(-0.0190)\&amp;=0.0348 \\end{split} 至此，来自两个输出的权值影响就全部汇集到了$a1^{(2)}$。接下来，我们只需要考虑$W{11}^{(1)}$对$a_1^{(2)}$的影响了，这就变得简单多啦！由式$\eqref{residual1}$可得 f'(z_1^{2}) = 0.5932*(1-0.5932) = 0.2413\begin{equation}\begin{split}\delta1^{(2)} = (\sum \delta_j^{(3)} W{ij}^{(2)}) \cdot f’(z1^{2})\\label{key}\end{split}\end{equation}故$ W{11}^{(1)} $权值更新如下： W_{11}^{(1)} := W_{11}^{(1)} - \eta \cdot \delta_1^{(2)} \cdot a_1^{(1)} = 0.1498W_{11}^{(1)} := 0.15 - 0.5 * 0.0348 * 0.2413 * 0.05 = 0.1498反向传导最关键的地方在于残差的数值传递，即式$\eqref{key}$，一定要正确理解$\delta_1^{(2)}$与$\delta_j^{(3)}$之间是如何联系起来的！神经网络中的其他参数也可以按照同样的方法进行更新，本文不再赘述。 本文中给出的例子有两个输出，故数值传递比只有一个输出的要更复杂，如果只有一个输出的话，那我们计算$\frac {\partial{E_T}} {\partial {a_1^{(2)}}}$就不用考虑$a_2^{(3)}$的影响 其实这样理解起来挺麻烦的，我们换一个角度看看 \frac {\partial {a_1^{(2)}}}{\partial W_{11}^{(1)}}=0.5932*(1-0.5932)*0.05=0.0120 \frac {\partial{E_T}}{\partial W_{11}^{(1)}} = 0.0348*0.0120=0.0004再应用梯度下降算法，$ W_{11}^{(1)} $权值更新如下： W_{11}^{(1)} := 0.15 - 0.5 * 0.0004 = 0.1498跟上面的方法得出的结果也是一样的，这里并不是说从残差的角度去计算就不好，而是从一个平时我们习惯的视角去更新参数能够加深理解。 参考文献 反向传播算法Python代码 | 我爱自然语言处理 神经网络 - Ufldl 神經元》. 2016. 维基百科，自由的百科全书 微软实现深层神经网络重大技术突破 - 微软亚洲研究院 CS231n Convolutional Neural Networks for Visual Recognition Neural Networks 《机器学习》. 周志华 《统计学习方法》. 李航 《The Elements of Statistical Learning》Jerome H. Friedman, Robert Tibshirani, and Trevor Hastie A Step by Step Backpropagation Example – Matt Mazur]]></content>
      <categories>
        <category>Machine-Learning</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>神经网络</tag>
        <tag>neural networks</tag>
        <tag>Machine-Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[理解Numpy中的广播（Broadcasting）机制]]></title>
    <url>%2F2017%2F2017-03-17-Numpy-broadcasting%2F</url>
    <content type="text"><![CDATA[通过一张图理解Numpy的广播机制 Numpy中的广播指的是在进行数学运算时，如果两个数组的shape属性不一致，那么小的数组就会沿着大的数组“广播”开来，这样一来，两个原本大小不一的数组就可以进行数学运算了。广播是一个不错的偷懒办法，但是效率不高降低运算速度通常也为人诟病。 上面这张图片解释得非常形象，这里用文字概括两条规则： 广播之后，输出数组的shape是输入数组shape的各个轴上的最大值，然后沿着较大shape属性的方向复制延伸； 要进行广播机制，要么两个数组的shape属性一样，要么其中有一个数组的shape属性必须有一个等于1； 参考 图片内容来源：http://www.labri.fr/perso/nrougier/from-python-to-numpy/?utm_source=mybridge&amp;utm_medium=blog&amp;utm_campaign=read_more#broadcasting https://docs.scipy.org/doc/numpy-1.10.0/user/basics.broadcasting.html]]></content>
      <categories>
        <category>coding</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python Numpy 和 SciPy-入门教程]]></title>
    <url>%2F2017%2F2017-03-15-python-numpy-tutorials%2F</url>
    <content type="text"><![CDATA[本文简单介绍了Python的基本概念、Numpy和SciPy的简单用法，非常适合初学者以及已经入门需要复习的Python学习者。文章来源于CS231n课程给予初学者的Python的初级教程，斯坦福大学的CS231n（ Convolutional Neural Networks for Visual Recogniton），开课者是著名计算机视觉学者李飞飞教授。 原文链接：http://cs231n.github.io/python-numpy-tutorial/ 目录 Python 基本数据类型 容器（Containers） 列表（Lists） 字典（Dictionaries） 集合（Sets） 元组（Tuples） 函数（Functions） 类（Classes） Numpy 数组（Arrays） 数组索引（Array indexing） 数据类型 Array math 广播（broadcasting） SciPy 图像处理（Image operations） 点之间的距离 Matplotlib 作图（Plotting） 子图（Subplots） 图像（Images） PythonPython是一种动态、多参数的高级编程语言。在Python里，用几行简单易读的代码就能让你天马行空的想法实现，所以有人说Python读起来就像是伪代码不无道理。 例如，在Python实现经典快排算法只需要这么几行 1234567891011def quicksort(arr): if len(arr) &lt;= 1: return arr pivot = arr[len(arr) / 2] left = [x for x in arr if x &lt; pivot] middle = [x for x in arr if x == pivot] right = [x for x in arr if x &gt; pivot] return quicksort(left) + middle + quicksort(right) print quicksort([3,6,8,10,1,2,1])# Prints "[1, 1, 2, 3, 6, 8, 10]" 基本数据类型与大多数编程语言类似，Python有许多数据类型，包括整数（integer）、浮点数（float）、布尔数（boolean）和字符串（string）。这些数据类型与其他编程语言的相比也没什么两样。 数字（Numbers）：整数和浮点数的操作与其他编程语言的也没什么两样 1234567891011121314x = 3print type(x) # Prints "&lt;type 'int'&gt;"打印数据的类型print x # Prints "3"print x + 1 # Addition; prints "4"print x - 1 # Subtraction; prints "2"print x * 2 # Multiplication; prints "6"print x ** 2 # Exponentiation; prints "9" 指数运算x += 1print x # Prints "4"x *= 2print x # Prints "8"y = 2.5print type(y) # Prints "&lt;type 'float'&gt;"print y, y + 1, y * 2, y ** 2 # Prints "2.5 3.5 5.0 6.25" 注意，与其他编程语言不同，Python没有自增（++）和自减（—）的运算。 Python也还有为长整型和复杂数据准备的自建类型，你可以在这个文档链接中找到相关细节信息。 布尔型（Booleans）：Python可以实现所有的布尔逻辑运算，不同于其他编程语言，它的运算符号都是英语单词，比如and、or、not，而不是（&amp;&amp;,||） 1234567t = Truef = Falseprint type(t) # Prints "&lt;type 'bool'&gt;"print t and f # Logical AND; prints "False"print t or f # Logical OR; prints "True"print not t # Logical NOT; prints "False"print t != f # Logical XOR; prints "True" 字符串（Strings）： 12345678hello = 'hello' # String literals can use single quotes(单引号或双引号都没关系)world = "world" # or double quotes; it does not matter.print hello # Prints "hello"print len(hello) # String length; prints "5"(字符串的长度)hw = hello + ' ' + world # 用空格连接字符串print hw # prints "hello world"hw12 = '%s %s %d' % (hello, world, 12) # 打印字符串的特定类型print hw12 # prints "hello world 12" Python中的字符串还有一大堆好玩的玩法，比如 12345678s = "hello"print s.capitalize() # Capitalize a string; prints "Hello"（首字母大写）print s.upper() # Convert a string to uppercase; prints "HELLO"print s.rjust(7) # Right-justify a string, padding with spaces; prints " hello"print s.center(7) # Center a string, padding with spaces; prints " hello "print s.replace('l', '(ell)') # Replace all instances of one substring with another;替换子字符串 # prints "he(ell)(ell)o"print ' world '.strip() # Strip leading and trailing whitespace; prints "world"跳过字符串前面的空格 你可以在这个链接中找到一些关于字符串的操作细节。 容器（Containers）Python有几个自建容器类型：列表、字典、集合和元组。 列表（Lists）Python中的列表等同于一个数组，然而列表可以改变大小，还可以容纳不同类型的元素，在Python中列表用[]围起来 123456789xs = [3, 1, 2] # Create a listprint xs, xs[2] # Prints "[3, 1, 2] 2"print xs[-1] # Negative indices count from the end of the list; prints "2" # 负数索引从列表的最后开始算起xs[2] = 'foo' # Lists can contain elements of different typesprint xs # Prints "[3, 1, 'foo']"列表中可以容纳不同类型的元素xs.append('bar') # Add a new element to the end of the listprint xs # Prints "[3, 1, 'foo', 'bar']"x = xs.pop() # Remove and return the last element of the list，去掉列表中的最后一个元素print x, xs # Prints "bar [3, 1, 'foo']" 分片(Slicing)：除了使用索引访问单个元素，Python还可以使用分片操作来访问某个范围内的元素 123456789nums = range(5) # range is a built-in function that creates a list of integersprint nums # Prints "[0, 1, 2, 3, 4]"print nums[2:4] # Get a slice from index 2 to 4 (exclusive); prints "[2, 3]"print nums[2:] # Get a slice from index 2 to the end; prints "[2, 3, 4]"print nums[:2] # Get a slice from the start to index 2 (exclusive); prints "[0, 1]"print nums[:] # Get a slice of the whole list; prints ["0, 1, 2, 3, 4]"print nums[:-1] # Slice indices can be negative; prints ["0, 1, 2, 3]"nums[2:4] = [8, 9] # Assign a new sublist to a sliceprint nums # Prints "[0, 1, 8, 9, 4]" 循环(Loops)：针对列表中的元素，你也可以进行循环操作 1234animals = ['cat', 'dog', 'monkey']for animal in animals: print animal# Prints "cat", "dog", "monkey", each on its own line. 如果你想通过循环体来获得列表中元素的索引，还不如使用内建函数enumerate 1234animals = ['cat', 'dog', 'monkey']for idx, animal in enumerate(animals): print '#%d: %s' % (idx + 1, animal)# Prints "#1: cat", "#2: dog", "#3: monkey", each on its own line 列表解析(List comprehensions):当使用Python编程的时候，经常我们需要将一类数据转换成另一种类型的数据。举一个简单的例子，看看下面这段计算平方的代码 12345nums = [0, 1, 2, 3, 4]squares = []for x in nums: squares.append(x ** 2)print squares # Prints [0, 1, 4, 9, 16] （是不是很简单？对在Python下面就是这么通俗易懂，所以说“人生苦短，我用Python”是不无道理的。）你也可以通过列表解析让代码变得更简单 123nums = [0, 1, 2, 3, 4]squares = [x ** 2 for x in nums]print squares # Prints [0, 1, 4, 9, 16] 列表解析在有条件限制的情形下也可以使用 123nums = [0, 1, 2, 3, 4]even_squares = [x ** 2 for x in nums if x % 2 == 0]print even_squares # Prints "[0, 4, 16]" 字典（Dictionaries）Python中的字典数据类型存放键(key)、值(value)，与Java中的map和JavaScript中的对象（object）相似，在Python中你可以这样来使用它： 12345678910d = &#123;'cat': 'cute', 'dog': 'furry'&#125; # Create a new dictionary with some dataprint d['cat'] # Get an entry from a dictionary; prints "cute"print 'cat' in d # Check if a dictionary has a given key; prints "True"d['fish'] = 'wet' # Set an entry in a dictionaryprint d['fish'] # Prints "wet"# print d['monkey'] # KeyError: 'monkey' not a key of dprint d.get('monkey', 'N/A') # 字典中并没有该键，故也没有值; prints "N/A"print d.get('fish', 'N/A') # Get an element with a default; prints "wet"del d['fish'] # Remove an element from a dictionaryprint d.get('fish', 'N/A') # "fish" is no longer a key; prints "N/A" 如果你想知道得更多关于字典的细节，访问文档链接 循环(Loops)：在字典中遍历所有的键（keys）非常简单 12345d = &#123;'person': 2, 'cat': 4, 'spider': 8&#125;for animal in d: legs = d[animal] print 'A %s has %d legs' % (animal, legs) # %s是字符串通配符,%d是整形通配符，对象格式化输出# Prints "A person has 2 legs", "A spider has 8 legs", "A cat has 4 legs" 如果你想获取字典中的键以及相应的值，使用iteritems方法： 1234d = &#123;'person': 2, 'cat': 4, 'spider': 8&#125;for animal, legs in d.iteritems(): print 'A %s has %d legs' % (animal, legs)# Prints "A person has 2 legs", "A spider has 8 legs", "A cat has 4 legs" 字典解析(Dictionary comprehensions):与列表解析相似，结构化字典也相当简单。比如 123nums = [0, 1, 2, 3, 4]even_num_to_square = &#123;x: x ** 2 for x in nums if x % 2 == 0&#125;print even_num_to_square # Prints "&#123;0: 0, 2: 4, 4: 16&#125;" 集合（Sets）在Python中，集合是由一组无序的各不相同的元素组成。举个简单例子， 12345678910animals = &#123;'cat', 'dog'&#125;print 'cat' in animals # Check if an element is in a set; prints "True"print 'fish' in animals # prints "False"animals.add('fish') # Add an element to a setprint 'fish' in animals # Prints "True"print len(animals) # Number of elements in a set; prints "3"animals.add('cat') # 往集合中添加已经有的元素，集合不会发生任何变化print len(animals) # Prints "3"animals.remove('cat') # Remove an element from a setprint len(animals) # Prints "2" 如果你想知道得更多关于集合的细节，访问文档链接 循环：集合中的循环语法与字典中的类似；然而因为集合是无序的，所以你无法知道集合中元素的顺序： 1234animals = &#123;'cat', 'dog', 'fish'&#125;for idx, animal in enumerate(animals): print '#%d: %s' % (idx + 1, animal)# Prints "#1: fish", "#2: dog", "#3: cat" 集合解析（Set comprehensions）: 与列表和字典类似，我们可以通过Set comprehensions轻松构建集合 123from math import sqrtnums = &#123;int(sqrt(x)) for x in range(30)&#125;print nums # Prints "set([0, 1, 2, 3, 4, 5])" 元组（Tuples）元组与列表一样，也是一种序列，只不过，元组中的元素是不能被修改的。还有，在字典中元组可以作为键使用，在集合中当做元素，列表却不可以。比如 12345d = &#123;(x, x + 1): x for x in range(10)&#125; # Create a dictionary with tuple keys, Prints "&#123;(0, 1): 0, (1, 2): 1, (6, 7): 6, (5, 6): 5, (7, 8): 7, (8, 9): 8, (4, 5): 4, (2, 3): 2, (9, 10): 9, (3, 4): 3&#125;" 是无序的t = (5, 6) # Create a tupleprint type(t) # Prints "&lt;type 'tuple'&gt;"print d[t] # Prints "5"print d[(1, 2)] # Prints "1" 如果你想知道得更多关于元组的细节，访问文档链接。 函数（Functions）在Python中函数通过def来定义，比如， 1234567891011def sign(x): if x &gt; 0: return 'positive' elif x &lt; 0: return 'negative' else: return 'zero'for x in [-1, 0, 1]: print sign(x)# Prints "negative", "zero", "positive" 我们经常在定义函数时通常加上一些关键词声明（keywords augment），比如： 12345678def hello(name, loud=False): if loud: print 'HELLO, %s!' % name.upper() else: print 'Hello, %s' % namehello('Bob') # Prints "Hello, Bob"hello('Fred', loud=True) # Prints "HELLO, FRED!" 如果你想知道得更多关于函数的细节，访问文档链接。 类（Classes）Python中定义类的语法非常直接 12345678910111213141516class Greeter(object): # Constructor def __init__(self, name): self.name = name # Create an instance variable # Instance method def greet(self, loud=False): if loud: print 'HELLO, %s!' % self.name.upper() else: print 'Hello, %s' % self.name g = Greeter('Fred') # Construct an instance of the Greeter classg.greet() # Call an instance method; prints "Hello, Fred"g.greet(loud=True) # Call an instance method; prints "HELLO, FRED!" 如果你想知道得更多关于类的细节，访问文档链接 NumpyNumpy是Python中负责科学计算的核心库之一。Numpy在处理多维数组上性能强大，它还有许多处理这些数组的工具。如果你事先熟悉了MATLAB，你可能发现会对你入门numpy有点点帮助。 数组（Arrays）Numpy数组是相同类型的、有非负整数索引的、在网格状中的值。数据的维度等于数组的秩rank；数组的shape属性指的是数组的。。。。（说了这么多，按照我的理解，你就把Numpy中的数组看成是一个m*n的矩阵好了，shape属性是这个矩阵的行数和列数，索引的办法也是一层层的） 123456789101112import numpy as npa = np.array([1, 2, 3]) # Create a rank 1 arrayprint type(a) # Prints "&lt;type 'numpy.ndarray'&gt;"print a.shape # Prints "(3,)"print a[0], a[1], a[2] # Prints "1 2 3"a[0] = 5 # Change an element of the arrayprint a # Prints "[5, 2, 3]"b = np.array([[1,2,3],[4,5,6]]) # Create a rank 2 arrayprint b.shape # Prints "(2, 3)"print b[0, 0], b[0, 1], b[1, 0] # Prints "1 2 4" Numpy也提供了许多建立（construct）数据的函数： 1234567891011121314151617181920import numpy as npa = np.zeros((2,2)) # Create an array of all zerosprint a # Prints "[[ 0. 0.] # [ 0. 0.]]" b = np.ones((1,2)) # Create an array of all onesprint b # Prints "[[ 1. 1.]]"c = np.full((2,2), 7) # Create a constant arrayprint c # Prints "[[ 7. 7.] # [ 7. 7.]]"d = np.eye(2) # Create a 2x2 identity matrixprint d # Prints "[[ 1. 0.] # [ 0. 1.]]" e = np.random.random((2,2)) # Create an array filled with random valuesprint e # Might print "[[ 0.91940167 0.08143941] # [ 0.68744134 0.87236687]]" 通过这个文档链接获得更多关于创建数组的细节 数组索引（Array indexing）Numpy提供几个索引数组的方法 分片：与Python的列表相似，Numpy也可以被分片。因为数组可能是多维度的，你必须明确数组中每个维度的分片 12345678910111213141516171819import numpy as np# Create the following rank 2 array with shape (3, 4)# [[ 1 2 3 4]# [ 5 6 7 8]# [ 9 10 11 12]]a = np.array([[1,2,3,4], [5,6,7,8], [9,10,11,12]])# Use slicing to pull out the subarray consisting of the first 2 rows# and columns 1 and 2; b is the following array of shape (2, 2):# [[2 3]# [6 7]]b = a[:2, 1:3]# A slice of an array is a view into the same data, so modifying it# will modify the original array.print a[0, 1] # Prints "2"b[0, 0] = 77 # b[0, 0] is the same piece of data as a[0, 1]print a[0, 1] # Prints "77" 你也可以将分片索引和整数索引混合使用。 123456789101112131415161718192021222324import numpy as np# Create the following rank 2 array with shape (3, 4)# [[ 1 2 3 4]# [ 5 6 7 8]# [ 9 10 11 12]]a = np.array([[1,2,3,4], [5,6,7,8], [9,10,11,12]])# Two ways of accessing the data in the middle row of the array.# Mixing integer indexing with slices yields an array of lower rank,# while using only slices yields an array of the same rank as the# original array:row_r1 = a[1, :] # Rank 1 view of the second row of a row_r2 = a[1:2, :] # Rank 2 view of the second row of aprint row_r1, row_r1.shape # Prints "[5 6 7 8] (4,)"print row_r2, row_r2.shape # Prints "[[5 6 7 8]] (1, 4)"# We can make the same distinction when accessing columns of an array:col_r1 = a[:, 1]col_r2 = a[:, 1:2]print col_r1, col_r1.shape # Prints "[ 2 6 10] (3,)"print col_r2, col_r2.shape # Prints "[[ 2] # [ 6] # [10]] (3, 1)" 整数数组索引：当你使用切片索引Numpy数组时，结果永远是初始数组的一个子数组。相反，整数数组索引允许你使用数组中的数据重新建立绝对数组。比如： 1234567891011121314151617import numpy as npa = np.array([[1,2], [3, 4], [5, 6]])# An example of integer array indexing.# The returned array will have shape (3,) and print a[[0, 1, 2], [0, 1, 0]] # Prints "[1 4 5]"# The above example of integer array indexing is equivalent to this:print np.array([a[0, 0], a[1, 1], a[2, 0]]) # Prints "[1 4 5]"# When using integer array indexing, you can reuse the same# element from the source array:print a[[0, 0], [1, 1]] # Prints "[2 2]"# Equivalent to the previous integer array indexing exampleprint np.array([a[0, 1], a[0, 1]]) # Prints "[2 2]" 整数数组索引的一个有用技巧是选择或转换每一行的某一个元素 1234567891011121314151617181920212223import numpy as np# Create a new array from which we will select elementsa = np.array([[1,2,3], [4,5,6], [7,8,9], [10, 11, 12]])print a # prints "array([[ 1, 2, 3], # [ 4, 5, 6], # [ 7, 8, 9], # [10, 11, 12]])"# Create an array of indicesb = np.array([0, 2, 0, 1])# Select one element from each row of a using the indices in bprint a[np.arange(4), b] # Prints "[ 1 6 7 11]"# Mutate one element from each row of a using the indices in ba[np.arange(4), b] += 10print a # prints "array([[11, 2, 3], # [ 4, 5, 16], # [17, 8, 9], # [10, 21, 12]]) 布尔型数组索引：布尔型数组索引能够让你挑选出数组的绝对元素。通常，这种类型的索引被用于选择数组中满足某些条件的元素。比如 1234567891011121314151617181920import numpy as npa = np.array([[1,2], [3, 4], [5, 6]])bool_idx = (a &gt; 2) # Find the elements of a that are bigger than 2; # this returns a numpy array of Booleans of the same # shape as a, where each slot of bool_idx tells # whether that element of a is &gt; 2. print bool_idx # Prints "[[False False] # [ True True] # [ True True]]"# We use boolean array indexing to construct a rank 1 array# consisting of the elements of a corresponding to the True values# of bool_idxprint a[bool_idx] # Prints "[3 4 5 6]"# We can do all of the above in a single concise statement:print a[a &gt; 2] # Prints "[3 4 5 6]" 如果你想了解跟过关于数组索引的细节，访问文档链接 数据类型每个Numpy数组都是一组相同类型的数据。Numpy提供了许多数值类型，你可以用它们来建立数组。当你建立一个数组之后，Numpy就会尝试着猜测它的数据类型，函数就不一样，函数在创建数组的时候回特别声明它所处理数据的类型。比如 12345678910import numpy as npx = np.array([1, 2]) # Let numpy choose the datatypeprint x.dtype # Prints "int64"x = np.array([1.0, 2.0]) # Let numpy choose the datatypeprint x.dtype # Prints "float64"x = np.array([1, 2], dtype=np.int64) # Force a particular datatypeprint x.dtype # Prints "int64" 关于Numpy数据类型的更多细节，访问文档链接。 Array math基本的数学函数是可以适用数组的点乘的 123456789101112131415161718192021222324252627282930313233import numpy as npx = np.array([[1,2],[3,4]], dtype=np.float64)y = np.array([[5,6],[7,8]], dtype=np.float64)# Elementwise sum; both produce the array# [[ 6.0 8.0]# [10.0 12.0]]print x + yprint np.add(x, y)# Elementwise difference; both produce the array# [[-4.0 -4.0]# [-4.0 -4.0]]print x - yprint np.subtract(x, y)# Elementwise product; both produce the array# [[ 5.0 12.0]# [21.0 32.0]]print x * yprint np.multiply(x, y)# Elementwise division; both produce the array# [[ 0.2 0.33333333]# [ 0.42857143 0.5 ]]print x / yprint np.divide(x, y)# Elementwise square root; produces the array# [[ 1. 1.41421356]# [ 1.73205081 2. ]]print np.sqrt(x) 值得注意的是，不同于MATLAB，Numpy中的*是点乘运算，而非矩阵乘法运算。所以我们用点dot函数来计算向量的内积和进行矩阵和向量的乘法。点dot同时也可以作为Numpy模块中的函数 123456789101112131415161718192021import numpy as npx = np.array([[1,2],[3,4]])y = np.array([[5,6],[7,8]])v = np.array([9,10])w = np.array([11, 12])# Inner product of vectors; both produce 219print v.dot(w)print np.dot(v, w)# Matrix / vector product; both produce the rank 1 array [29 67]print x.dot(v)print np.dot(x, v)# Matrix / matrix product; both produce the rank 2 array# [[19 22]# [43 50]]print x.dot(y)print np.dot(x, y) Numpy有很多计算数组的函数，其中一个便是sum 1234567import numpy as npx = np.array([[1,2],[3,4]])print np.sum(x) # Compute sum of all elements; prints "10"print np.sum(x, axis=0) # Compute sum of each column; prints "[4 6]"print np.sum(x, axis=1) # Compute sum of each row; prints "[3 7]" 你可以在这个文档链接找到Numpy中所有的数学函数 除了用数组来进行数学函数计算之外，我们通常需要重塑或者对数组进行其他操作。矩阵的转置是一个最简单的例子，简单的用T就可以实现 123456789101112import numpy as npx = np.array([[1,2], [3,4]])print x # Prints "[[1 2] # [3 4]]"print x.T # Prints "[[1 3] # [2 4]]"# Note that taking the transpose of a rank 1 array does nothing:v = np.array([1,2,3])print v # Prints "[1 2 3]"print v.T # Prints "[1 2 3]" 对于数组处理，Numpy提供了许多函数，在这个文档找到更多细节。 广播（broadcasting）广播是Python中很强大的一种机制，在进行算术运算的时候，它能够允许Numpy对不同shape属性的数组进行操作。一般情况下， 我们会有一个较大的数组和一个较小的数组，也就是数组的shape属性不同，而我们想通过多次使用较小的数组来对较大的数组进行算术运算。 比如，假设我们需要给矩阵的每一行元素加上一个常数向量 123456789101112131415161718import numpy as np# We will add the vector v to each row of the matrix x,# storing the result in the matrix yx = np.array([[1,2,3], [4,5,6], [7,8,9], [10, 11, 12]])v = np.array([1, 0, 1])y = np.empty_like(x) # 创建一个跟x一样shape属性的空白矩阵# Add the vector v to each row of the matrix x with an explicit loopfor i in range(4): y[i, :] = x[i, :] + v# Now y is the following# [[ 2 2 4]# [ 5 5 7]# [ 8 8 10]# [11 11 13]]print y 这样做的确可行，但是当矩阵x非常大的时候，在Python中计算这样的循环任务将会非常缓慢。因为给矩阵x的每一行加上一个v等同于直接在矩阵上叠加vv，然后对x和叠加的矩阵vv进行点对点的相加。如下 12345678910111213141516import numpy as np# We will add the vector v to each row of the matrix x,# storing the result in the matrix yx = np.array([[1,2,3], [4,5,6], [7,8,9], [10, 11, 12]])v = np.array([1, 0, 1])vv = np.tile(v, (4, 1)) # Stack 4 copies of v on top of each otherprint vv # Prints "[[1 0 1] # [1 0 1] # [1 0 1] # [1 0 1]]"y = x + vv # Add x and vv elementwiseprint y # Prints "[[ 2 2 4 # [ 5 5 7] # [ 8 8 10] # [11 11 13]]" Numpy的广播操作则允许我们无需创建v的拷贝版本而直接进行计算。使用广播的话就简单很多了，广播通常也能够使你的代码更准确更迅速，换句话说，Python的广播机制使得运算更加方便 1234567891011import numpy as np# We will add the vector v to each row of the matrix x,# storing the result in the matrix yx = np.array([[1,2,3], [4,5,6], [7,8,9], [10, 11, 12]])v = np.array([1, 0, 1])y = x + v # Add v to each row of x using broadcastingprint y # Prints "[[ 2 2 4] # [ 5 5 7] # [ 8 8 10] # [11 11 13]]" 是不是简化了许多！ 对两个数组进行广播操作要遵循以下几个规则： 如果数组没有相同的秩（rank），那么将低秩数数组的shape属性加1，使得它们的shape属性一样； 123456789101112131415161718192021222324&gt;&gt;&gt; import numpy as np&gt;&gt;&gt; a = np.arange(0, 60, 10).reshape(-1, 1)&gt;&gt;&gt; aarray([[ 0], [10], [20], [30], [40], [50]])&gt;&gt;&gt; a.shape(6, 1)&gt;&gt;&gt; b = np.arange(0, 5)&gt;&gt;&gt; barray([0, 1, 2, 3, 4])&gt;&gt;&gt; b.shape(5,) # a与b的shape属性不同&gt;&gt;&gt; c = a + b&gt;&gt;&gt; carray([[ 0, 1, 2, 3, 4], [10, 11, 12, 13, 14], [20, 21, 22, 23, 24], [30, 31, 32, 33, 34], [40, 41, 42, 43, 44], [50, 51, 52, 53, 54]])&gt;&gt;&gt; c.shape(6, 5)#二维数组a，其shape为(6,1)，一维数组b，其shape为(5,)，由于a和b的shape长度不同，根据规则1，需要让b的shape向a对齐，于是将b的shape前面加1，补齐为(1,5) 广播之后，输出数组的shape是输入数组shape的各个轴上的最大值； |A (4d array)|8 x 1 x 6 x 1| |:—|—:| |B (3d array)| 7 x 1 x 5| |Result (4d array)|8 x 7 x 6 x 5| 在任何维度，一个数组的大小为1而另一个数组的大小比1大，那么第一个数组。 123456789101112131415161718192021222324252627282930313233343536373839404142import numpy as np# Compute outer product of vectorsv = np.array([1,2,3]) # v has shape (3,)w = np.array([4,5]) # w has shape (2,)# To compute an outer product, we first reshape v to be a column# vector of shape (3, 1); we can then broadcast it against w to yield# an output of shape (3, 2), which is the outer product of v and w:# [[ 4 5]# [ 8 10]# [12 15]]print np.reshape(v, (3, 1)) * w# Add a vector to each row of a matrixx = np.array([[1,2,3], [4,5,6]])# x has shape (2, 3) and v has shape (3,) so they broadcast to (2, 3),# giving the following matrix:# [[2 4 6]# [5 7 9]]print x + v# Add a vector to each column of a matrix# x has shape (2, 3) and w has shape (2,).# If we transpose x then it has shape (3, 2) and can be broadcast# against w to yield a result of shape (3, 2); transposing this result# yields the final result of shape (2, 3) which is the matrix x with# the vector w added to each column. Gives the following matrix:# [[ 5 6 7]# [ 9 10 11]]print (x.T + w).T# Another solution is to reshape w to be a row vector of shape (2, 1);# we can then broadcast it directly against x to produce the same# output.print x + np.reshape(w, (2, 1))# Multiply a matrix by a constant:# x has shape (2, 3). Numpy treats scalars as arrays of shape ();# these can be broadcast together to shape (2, 3), producing the# following array:# [[ 2 4 6]# [ 8 10 12]]print x * 2 其实这个地方我也没弄太明白，我觉得应该类似于矩阵里边的运算，两个矩阵进行运算，第一个矩阵的列数必须跟第二个矩阵的行数相等，不然无法运算。Python的广播机制应该跟这个意思差不太多，只不过它帮你省去了思考的时间，但是自己在使用数组运算的时候应该事先就要考虑好矩阵的维度，不然会得出截然不同的结果。 后面我会继续跟进这个话题。 SciPySciPy提供大量用于计算Numpy数组的函数，并且对于不同行业的科学和工程有广泛的应用 图像处理（Image operations）对于图像处理，SciPy有几个基本的处理函数。比如，从磁盘中读取图像并存放在Numpy数组中，可以在磁盘中写入Numpy数组存储为图像，可以重新调整图像大小。（图片本来由一个个的像素点组成，这些像素点即是组成数组的元素）下面是一个简单的例子 12345678910111213141516171819from scipy.misc import imread, imsave, imresize# Read an JPEG image into a numpy arrayimg = imread('assets/cat.jpg')print img.dtype, img.shape # Prints "uint8 (400, 248, 3)"# We can tint the image by scaling each of the color channels# by a different scalar constant. The image has shape (400, 248, 3);# we multiply it by the array [1, 0.95, 0.9] of shape (3,);# numpy broadcasting means that this leaves the red channel unchanged,# and multiplies the green and blue channels by 0.95 and 0.9# respectively.img_tinted = img * [1, 0.95, 0.9]# Resize the tinted image to be 300 by 300 pixels.img_tinted = imresize(img_tinted, (300, 300))# Write the tinted image back to diskimsave('assets/cat_tinted.jpg', img_tinted) 点之间的距离给定一个集合，函数scipy.spatial.distance.pdist可以计算所有点之间的距离 123456789101112131415161718import numpy as npfrom scipy.spatial.distance import pdist, squareform# Create the following array where each row is a point in 2D space:# [[0 1]# [1 0]# [2 0]]x = np.array([[0, 1], [1, 0], [2, 0]])print x# Compute the Euclidean distance between all rows of x.# d[i, j] is the Euclidean distance between x[i, :] and x[j, :],# and d is the following array:# [[ 0. 1.41421356 2.23606798]# [ 1.41421356 0. 1. ]# [ 2.23606798 1. 0. ]]d = squareform(pdist(x, 'euclidean'))print d scipy.spatial.distance.cdist是一个相似的函数，computes the distance between all pairs across two sets of points MatplotlibMatplotlib是一个专门用来作图的库。这小节中会对matplotlib.pyplot函数简单介绍 作图（Plotting）Matplotlib中最重要的一个函数是plot，它可用来处理二维数据。 12345678910import numpy as npimport matplotlib.pyplot as plt# Compute the x and y coordinates for points on a sine curvex = np.arange(0, 3 * np.pi, 0.1)y = np.sin(x)# Plot the points using matplotlibplt.plot(x, y)plt.show() # You must call plt.show() to make graphics appear. 运行代码，得到如下图形 再添加一点点额外代码就可以轻松同时画出多条曲线、添加标题、图例、轴标签 12345678910111213141516import numpy as npimport matplotlib.pyplot as plt# Compute the x and y coordinates for points on sine and cosine curvesx = np.arange(0, 3 * np.pi, 0.1)y_sin = np.sin(x)y_cos = np.cos(x)# Plot the points using matplotlibplt.plot(x, y_sin)plt.plot(x, y_cos)plt.xlabel('x axis label')plt.ylabel('y axis label')plt.title('Sine and Cosine')plt.legend(['Sine', 'Cosine'])plt.show() 子图（Subplots）你也可以使用subplot在同一张图片中画出不同的图像，下面是一个简单的例子，同时在一幅图中画出sin和cos的曲线 1234567891011121314151617181920212223import numpy as npimport matplotlib.pyplot as plt# Compute the x and y coordinates for points on sine and cosine curvesx = np.arange(0, 3 * np.pi, 0.1)y_sin = np.sin(x)y_cos = np.cos(x)# Set up a subplot grid that has height 2 and width 1,# and set the first such subplot as active.plt.subplot(2, 1, 1)# Make the first plotplt.plot(x, y_sin)plt.title('Sine')# Set the second subplot as active, and make the second plot.plt.subplot(2, 1, 2)plt.plot(x, y_cos)plt.title('Cosine')# Show the figure.plt.show() 图像（Images）使用imshow函数呈现图像 12345678910111213141516171819import numpy as npfrom scipy.misc import imread, imresizeimport matplotlib.pyplot as pltimg = imread('assets/cat.jpg')img_tinted = img * [1, 0.95, 0.9]# Show the original imageplt.subplot(1, 2, 1)plt.imshow(img)# Show the tinted imageplt.subplot(1, 2, 2)# A slight gotcha with imshow is that it might give strange results# if presented with data that is not uint8. To work around this, we# explicitly cast the image to uint8 before displaying it.plt.imshow(np.uint8(img_tinted))plt.show()]]></content>
      <categories>
        <category>coding</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>numpy</tag>
        <tag>scipy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习算法系列（7）朴素贝叶斯法]]></title>
    <url>%2F2017%2F2017-03-13-machine-learning-algorithm-series-naive-bayesian-method%2F</url>
    <content type="text"><![CDATA[朴素贝叶斯法，即naive Bayesian method，之所以“naive”，大概是因为它只用到了浅显的概率论知识，（注意！本文没有任何的数学公式推导！）理解起来也相当容易。但不要看它naive就小瞧了它，朴素贝叶斯法基于贝叶斯定理和特征条件独立，是一种分类方法，它在分类任务中应用相当广泛。在这篇文章中，我会详细介绍贝叶斯法中的三个重要概念，以及用一个小故事和一个Python实现简单文本分类的例子帮助理解。 假设你的朋友近来发现笔记本玩游戏越来越卡，关键时候团战竟然掉帧卡顿，到手的五杀生生没了。为了更好地守护高地，他决定组装一台台式机，在网上苦苦搜寻，找来了一份主要部件的配置单，并征求你的建议，能不能胜任大型游戏 内存 CPU 主板 硬盘 金士顿 骇客神条 8G Intel i5-6600 华硕 M5A78L-M 西数蓝盘 1TB 虽然你对计算机硬件不太懂，但是你会机器学习算法啊！这时，你手头正好有某神秘组织给你的计算机性能评测数据，那么如何利用好这些数据为你的朋友做出正确的选择呢？假设你有这方面的困惑，来，下面将向你展示如何用经典的贝叶斯方法解决这个问题。 贝叶斯其人英国统计学家托马斯·贝叶斯（1701？-1761），后来成为了英国皇家学会成员，同时他还是一名哲学家。贝叶斯以前做过神父，为了证明上帝的存在，他建立了概率统计学原理，遗憾的是，这个愿望直到他死都没有实现。贝叶斯本人和他的研究工作在他当时并没有受到多少人的关注，他是如何入选英国皇家学会会员也鲜有记载，直到后来数学家拉普拉斯使他的工作重新为世人所肯定，贝叶斯方法才逐渐为世人瞩目。 三个简单的概念朴素贝叶斯法的核心是贝叶斯定理，实际上是一个很简单的公式 \begin{equation}\begin{split}P(Y|X)=\frac{P(Y)P(X|Y)}{P(X)}\\label{bayesrule}\end{split}\end{equation} 假设我们有数据集$Y=(y_1,y_2,…,y_m)$，$X=(x_1,x_2,…,x_n)$是数据集的分类标记，我们考虑几个基本的概念 先验概率先验概率分布是基于已有的数据集得出的，比如要判断一台计算机的性能（$Y=(strong,weak)$）如何，那么我们可以根据已有的评测数据来进行判断。由已有的评测数据，可以轻松得到计算机性能强弱的概率，这也就是所谓的先验概率。先验概率很容易计算 \begin{equation}\begin{split}P(Y=y_i),k=1,2…,m\\label{prior}\end{split}\end{equation} 条件概率一般，要判断计算机性能强弱，我们会考虑一些什么特征$X$呢？例如，处理器核心数多少、硬盘类型、显卡性能高低、内存大小等等。条件概率就是假设事件发生时，其他现象发生的概率，如考虑计算机性能很强情形下，处理器、硬盘、显卡指标分别所占的概率。 \begin{equation}\begin{split}P(X=x_j|Y=y_i),k=1,2,…,m\\label{conditional}\end{split}\end{equation} 条件独立性假设条件独立性是贝叶斯法中最重要的一个假设，也就是在分类$y$在确定的情况下，各个条件之间是相互独立的，不然在计算$\eqref{conditional}$就会有指数级的灾难。既然我们假定各个特征之间相互是不影响的，这样一来，计算就简化了许多。 \begin{equation}\begin{split}P(X=xj|Y=y_i)=\prod{j=1}^{n}P(x_j|Y=y_i)\\label{indepentassumption}\end{split}\end{equation} 了解了三个重要概念之后，贝叶斯方法的学习远远还没有结束，仅仅知道基本的概念不足以完成分类的任务。类似于logistic regression，贝叶斯方法实际上也是一种基于概率的学习方法，它也是通过比较概率的相对大小来决定分类的结果。 后验概率最大化在贝叶斯统计中，一个随机事件或者一个不确定事件的后验概率是在考虑和给出相关证据或数据后所得到的条件概率。 计算$P(Y=y_i|X=x_j)$是最终目标，即后验概率。最大化后验概率化也是贝叶斯方法的目标，也就是我们要让式$\eqref{bayesrule}$的$P(X|Y)$最大 \begin{equation}\begin{split}arg \quad max\quad \prod_{j=1}^{n}P(x_j|Y=y_i)\\label{posteriorprobability}\end{split}\end{equation} 贝叶斯估计注意，朴素贝叶斯法和贝叶斯估计是不同的概念。朴素贝叶斯法中，用极大似然估计$\eqref{posteriorprobability}$可能会出现所要估计的概率值为0的情形，这种情形出现一般有两种原因：一是条件概率都很小，连乘会导致后验概率变得很小，一般会通过取对数的方式来避免；二是，当某个特征在训练集中没有与某个类同时出现过，那么直接计算的话，后验概率那就只能为零了，为了规避这种情况，通常要进行“平滑”（smoothing）处理，即拉普拉斯平滑（Laplace smoothing），我们也称这一方法为贝叶斯估计。如下式 \begin{equation}\begin{split}P(X=xj|Y=y_i)=\frac{|I{xj}|+\lambda}{|I{y_i=x_j}|+N_i\lambda}\\label{laplacesmoothing}\end{split}\end{equation} 式$\eqref{laplacesmoothing}$的$\lambda$取0是就是极大似然估计。当$\lambda$取1是，就是拉普拉斯平滑，它能够帮助避免训练样本集不充分而导致的概率估值为零的问题。 到底配置单是坑还是…？为了解决你朋友的问题，你根据神秘组织给你的数据，这组数据根据内存容量、CPU、显卡等指标评估计算机性能。 内存容量 CPU核心数 显卡 硬盘 性能强弱 8G 单核心 集成 机械 弱 8G 单核心 集成 固态 弱 4G 单核心 集成 机械 强 2G 双核心 集成 机械 强 2G 四核心 独立 机械 强 2G 四核心 独立 固态 弱 4G 四核心 独立 固态 强 8G 双核心 集成 机械 弱 8G 四核心 独立 机械 强 2G 双核心 独立 机械 强 8G 双核心 独立 固态 强 4G 双核心 集成 固态 强 4G 单核心 独立 机械 强 2G 双核心 集成 固态 弱 现在，贝叶斯就开始派上用场了！ 首先，根据$\eqref{prior}$先验概率，性能强弱指标下只有强弱之分，令$y_1$表示计算机性能“强”，$y_2$表示计算机性能“弱”。显然， P(Y=y_1)=(9+1)/(14+2)=5/8\\ P(Y=y_0)=(5+1)/(14+2)=3/8由文章开头给出的配置单，对应的特征$X=(8G,4cores,integrated,hdd)$。然后，计算条件概率 P(“8G”|Y=y_1)=(2+1)/(9+3)=3/12\\ P(“8G”|Y=y_0)=(3+1)/(5+3)=4/8\\ P(“4cores”|Y=y_1)=(3+1)/(9+3)=4/12\\ P(“4cores”|Y=y_0)=(1+1)/(5+3)=2/8\\ P(“integrated”|Y=y_1)=(3+1)/(9+2)=4/11\\ P(“integrated”|Y=y_0)=(4+1)/(5+2)=5/7\\ P(“hdd”|Y=y_1)=(3+1)/(9+2)=4/11\\ P(“hdd”|Y=y_0)=(3+1)/(5+2)=4/7\\对于给定的$X=(“8G”,“4cores”,“integrated”,“hdd”)$计算 P(Y=y_1)P(“8G”|Y=y_1)P(“4cores”|Y=y_1)P(“integrated”|Y=y_1)P(“hdd”|Y=y_1)=0.0069P(Y=y_0)P(“8G”|Y=y_0)P(“4cores”|Y=y_0)P(“integrated”|Y=y_0)P(“hdd”|Y=y_0)=0.0191因为$P(Y=y_0)P(“8G”|Y=y_0)P(“4cores”|Y=y_0)P(“integrated”|Y=y_0)P(“hdd”|Y=y_0)$较大，所以$Y=y_0$，从而可以初步判断按照此配置单组装的电脑性能为弱，绝对又是一个坑！（团战掉线，就跟那些无止尽刷野的坑货一样，都应该封号！） 说点题外话，上面例子中提到的配置单问题应该出现在了显卡上，集成显卡的属性使得整个性能大打折扣，所以关键的地方还是要抛弃原有的主板，考虑买一块独立显卡。 例子：利用贝叶斯进行简单的文本分类本例所用到的代码来自《机器学习实战》，非常值得推荐。代码虽然不多，但是给我最大的体会就是，懂算法的原理和实实在在在工程中应用实现是完全不同的两回事。正如本博客翻译的Numpy入门教程之前，我一直不明白在图像上怎么进行机器学习的工程应用，后来我翻译完教材之后才恍然大悟，原来图片的每一个像素点可以视为一个数字，所有的像素点不就组成了一个数组吗？开始看贝叶斯的时候，我也一直纳闷，这个贝叶斯方法怎么就可以用于文本分类呢？看不懂的时候，我就联想本文讲的这个组装电脑的故事，一点一点理解，明白了我得先有一个数据集，然后，在现有数据集的基础上计算先验概率，计算先验概率的时候还得注意数值下溢的问题，不然无法理解代码中的log(1.0 - pClass1)和log(pClass1)。光看算法，光看书没用啊！拿个例子来做一做，理解会更加深刻！ 处理过程 由样本数据集创建词汇表 根据输入的待划分数据创建词向量 计算先验概率，然后比较后验概率 测试 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364from numpy import *#导入样本数据集def loadDataSet(): postingList=[['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'], ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'], ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'], ['stop', 'posting', 'stupid', 'worthless', 'garbage'], ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'], ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']] classVec = [0,1,0,1,0,1] #1 is abusive, 0 not return postingList,classVec#由数据集创建词汇表 def createVocabList(dataSet): vocabSet = set([]) #创建一个空集，集合内的元素各不相同 for document in dataSet: vocabSet = vocabSet | set(document) #union of the two sets return list(vocabSet)#由输入词汇创建词向量def setOfWords2Vec(vocabList, inputSet): returnVec = [0]*len(vocabList) for word in inputSet: if word in vocabList: returnVec[vocabList.index(word)] = 1 #如果word在词汇表中，则向量相应位置为1 else: print "the word: %s is not in my Vocabulary!" % word return returnVecdef trainNB0(trainMatrix,trainCategory): numTrainDocs = len(trainMatrix) numWords = len(trainMatrix[0]) pAbusive = sum(trainCategory)/float(numTrainDocs) #实验样本中正例的个数，根据第一个函数，pAbusive=0.5 p0Num = ones(numWords); p1Num = ones(numWords) #change to ones() p0Denom = 2.0; p1Denom = 2.0 #所有词的个数出现次数初始化为1，为了避免概率出现0的情形 for i in range(numTrainDocs): if trainCategory[i] == 1: #一旦某个词在文档中出现，则该词对应的个数就要加1 p1Num += trainMatrix[i] #trainMatrix是一个array，故trainMatrix[i]是'[]' p1Denom += sum(trainMatrix[i]) else: p0Num += trainMatrix[i] p0Denom += sum(trainMatrix[i]) p1Vect = log(p1Num/p1Denom) p0Vect = log(p0Num/p0Denom) return p0Vect,p1Vect,pAbusivedef classifyNB(vec2Classify, p0Vec, p1Vec, pClass1): p1 = sum(vec2Classify * p1Vec) + log(pClass1) #element-wise mult p0 = sum(vec2Classify * p0Vec) + log(1.0 - pClass1) if p1 &gt; p0: return 1 else: return 0 def testingNB(): listOPosts,listClasses = loadDataSet() myVocabList = createVocabList(listOPosts) trainMat=[] for postinDoc in listOPosts: trainMat.append(setOfWords2Vec(myVocabList, postinDoc)) p0V,p1V,pAb = trainNB0(array(trainMat),array(listClasses)) testEntry = ['love', 'my', 'dalmation'] thisDoc = array(setOfWords2Vec(myVocabList, testEntry)) print testEntry,'classified as: ',classifyNB(thisDoc,p0V,p1V,pAb) testEntry = ['stupid', 'garbage'] thisDoc = array(setOfWords2Vec(myVocabList, testEntry)) print testEntry,'classified as: ',classifyNB(thisDoc,p0V,p1V,pAb) 个人在理解trainNB0(trainMatrix,trainCategory)这个函数的时候卡住了一下，因为我事先没注意到trainMatrix是一个array，因为它的数据类型是列表，所以我不明白为什么p1Num和trainMatrix[i]可以直接叠加（一个数组跟一个数字相加干嘛呢？）。另外，trainNB0(trainMatrix,trainCategory)还用到了拉普拉斯平滑的手段来防止概率为0的情形。这个例子还是存在不足之处的，比如说，数据集太小导致泛化能力不佳，如果我用一个在样本数据集中从未出现过的词来做测试，这个时候测试函数就无法正确检测了。 参考 https://zh.wikipedia.org/wiki/后验概率 统计学习方法.李航 机器学习.周志华 机器学习实战 一些关于贝叶斯的文章 学界 | 清华大学计算机系朱军教授：机器学习里的贝叶斯基本理论、模型和算法 贝叶斯推断及其互联网应用（一）：定理简介 数学之美番外篇：平凡而又神奇的贝叶斯方法]]></content>
      <categories>
        <category>Machine-Learning</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>算法</tag>
        <tag>贝叶斯</tag>
        <tag>bayesian</tag>
        <tag>naive-bayesian-method</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo-Next-CSS样式修改]]></title>
    <url>%2F2017%2F2017-03-12-hexo-next-beautify%2F</url>
    <content type="text"><![CDATA[Hexo-Next主题美化 1. 修改文章内链接文本样式将链接文本设置为蓝色，鼠标划过时文字颜色加深，并显示下划线。修改文件themes\next\source\css\_common\components\post\post.styl，添加如下css样式 123456789.post-body p a&#123; color: #0593d3; border-bottom: none; &amp;:hover &#123; color: #0477ab; text-decoration: underline; &#125;&#125; 选择.post-body是为了不影响标题，选择p是为了不影响首页“阅读全文”的显示样式。 2. 删除标题上下方横线修改文件themes/next/layout/_partials/header.swig文件，删除logo-line相关项 修改前 1234567&lt;div class="custom-logo-site-title"&gt; &lt;a href="&#123;&#123; config.root &#125;&#125;" class="brand" rel="start"&gt; &lt;span class="logo-line-before"&gt;&lt;i&gt;&lt;/i&gt;&lt;/span&gt; &lt;span class="site-title"&gt;&#123;&#123; config.title &#125;&#125;&lt;/span&gt; &lt;span class="logo-line-after"&gt;&lt;i&gt;&lt;/i&gt;&lt;/span&gt; &lt;/a&gt;&lt;/div&gt; 修改后 12345&lt;div class="custom-logo-site-title"&gt; &lt;a href="&#123;&#123; config.root &#125;&#125;" class="brand" rel="start"&gt; &lt;span class="site-title"&gt;&#123;&#123; config.title &#125;&#125;&lt;/span&gt; &lt;/a&gt;&lt;/div&gt; 3. 修改header背景颜色为白色找到themes\next\source\css\_schemes\Mist\header.styl文件，修改.header { background: $white; }，将颜色改为白色 4. 修改footer的文字居中找到themes/next/source/css/_schemes/Mist/index.styl文件中的footer，将.footer-inner的text-align: left修改为center 5. 修改post-title的颜色找到\themes\next\source\css\_common\components\post\post-title.styl文件，将.post-title-link下的color修改为自己想要的颜色 6. 为边框添加立体阴影效果找到\source\css\_common\components\post\post.styl文件，在use-motion .post下，添加box-shadow的代码box-shadow生成网站 7. 将菜单栏的文字颜色改为白色找到\themes\next\source\css\_schemes\Mist\menu.styl文件，在.menu-item a下添加color: white;，并将background改为header一样的颜色，那样就不会出现不协调的情形。 8. 改变post-meta的颜色找到\next\source\css\_common\components\post\post-meta.styl文件，在.post-meta下添加如下代码 123456a &#123; color: blueviolet; text-decoration: none; border-bottom: 1px solid #999; word-wrap: break-word; &#125; 更新于：2017/3/12 16:28:59 9. 新建导航栏找到主题文件下的\languages\zh-Hans.yml文件，在title和menu下添加machinelearning: 机器学习字段。然后再到主题配置文件_config.yml的menu下添加绝对路径，menu_icons下添加对应的FontAwesome图标 10. 静态资源cdn加速在主题配置文件中，找到vendors，添加字段，cdn地址来自bootcdn 12345678910111213141516171819202122vendors: # Internal path prefix. Please do not edit it. _internal: vendors # Internal version: 2.1.3 jquery: //cdn.bootcss.com/jquery/2.1.3/jquery.min.js # Internal version: 2.1.5 # Fancybox: http://fancyapps.com/fancybox/ fancybox: //cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.pack.js fancybox_css: //cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.min.css # Internal version: 1.0.6 fastclick: //cdn.bootcss.com/fastclick/1.0.6/fastclick.min.js # Internal version: 1.9.7 lazyload: //cdn.bootcss.com/jquery_lazyload/1.9.7/jquery.lazyload.min.js # Internal version: 1.2.1 velocity: //cdn.bootcss.com/velocity/1.3.1/velocity.min.js # Internal version: 1.2.1 velocity_ui: //cdn.bootcss.com/velocity/1.3.1/velocity.ui.min.js # Internal version: 0.7.9 ua_parser: //cdn.bootcss.com/UAParser.js/0.7.12/ua-parser.min.js # Internal version: 4.4.0 # http://fontawesome.io/ fontawesome: //cdn.bootcss.com/font-awesome/4.6.2/css/font-awesome.min.css 11. 添加百度统计之前用的是google统计，但是每次进入首页加载的时候太慢了，所以干脆换一个百度统计。在百度统计注册账号之后，按照官网文档教程，在主题配置文件下找到baidu_analytics字段，去掉前面的注释，添加百度统计的ID。 自动生成网站地图网站地图，又称站点地图，它就是一个页面，上面放置了网站上需要搜索引擎抓取的所有页面的链接。网站地图可以通过在线的形式生成，但是速度会有点慢，使用插件 hexo-generator-sitemap和hexo-generator-baidu-sitemap能自动生成站点地图，方法如下 123# 安装两个插件$ npm install hexo-generator-sitemap --save$ npm install hexo-generator-baidu-sitemap --save 到站点配置文件中，添加如下字段 1234sitemap: path: sitemap.xmlbaidusitemap: path: baidusitemap.xml 当hexo g -d之后，public文件夹下回自动生成网站地图，然后再将生成好的网站地图分别添加至百度和谷歌即可。 12. 设置点击加载 Disqus在没有满足科学上网条件下的网络，访问网站时往往由于 Disqus 而迟迟不能完全加载，下面这个教程则可以满足既能使用 Disqus，又不会影响网站加载的速度。 Hexo Next 主题点击加载 Disqus]]></content>
      <categories>
        <category>其他</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[计算机程序设计（C++）week03]]></title>
    <url>%2F2017%2F2017-03-11-MOOC-C%2B%2B-week03%2F</url>
    <content type="text"><![CDATA[计算机程序设计（C++）第三周课程笔记 结构化程序设计 要求：结构清晰、可读性强、易于分工和可调试 一般有三种设计形式 自顶向下：逐步分解 模块化：可以是相互独立的一条语句、程序或函数 结构化：顺序结构、选择结构（if-else,switch-case）、循环结构（while,do-while,for）三种基本控制结构 单路分支与多路分支单路分支只有一个判断条件，而多路分支有多个判断条件。 1234567891011121314151617181920212223242526#include &lt;iostream&gt;using namespace std;int main()&#123; double a, b, max; cin &gt;&gt; a &gt;&gt; b; max = a; if (a&lt;b) /* 单路循环结构 */ max = b; cout &lt;&lt; max &lt;&lt; endl; return 0;&#125;/* */#include &lt;iostream&gt;using namespace std;int main()&#123; double a, b, max; cin &gt;&gt; a &gt;&gt; b; if (a &lt; b) /* 多路循环结构 */ max = b; else max = a; cout &lt;&lt; max &lt;&lt; endl; return 0;&#125; 分支嵌套分支嵌套即if-else语句中又嵌套if-else语句。 1234567891011121314151617181920212223#include &lt;iostream&gt;using namespace std;int main()&#123; double a, b, c, max; cin &gt;&gt; a &gt;&gt; b &gt;&gt; c; if (a &gt; b) /*分支嵌套语句：if-else语句中又嵌套if-else*/ &#123; if (a &gt; c) max = a; else max = c; &#125; else &#123; if (b &lt; c) max = c; else max = b; &#125; cout &lt;&lt; max &lt;&lt; endl; return 0;&#125; 已知条件循环]]></content>
      <categories>
        <category>coding</category>
        <category>C++</category>
      </categories>
      <tags>
        <tag>笔记</tag>
        <tag>C++</tag>
        <tag>MOOC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习算法比较——LR vs. SVM]]></title>
    <url>%2F2017%2F2017-03-09-comparison-of-algorithm-LR-vs-SVM%2F</url>
    <content type="text"><![CDATA[逻辑斯蒂回归（Logistic regression，以下简称“LR”）和支持向量机（Support vector machine，以下简称“SVM”）都是机器学习中应用十分广泛的分类算法，两种算法在分类任务中的表现各有千秋，笔者在学习完LR和SVM后觉得有必要对这两种算法进行比较。参考论文及网络内容后，本文主要从思路、参数估计、样本分布下实验效果几个方面来进行对比。（个人笔记，如有纰漏，不吝赐教！） 1. 两种算法的思路比较在之前的一篇文章机器学习算法系列（3）Logistic Regression | Thinking Realm中我们提到 LR 本质上是一类广义线性模型（GLM），只是因为一般的线性模型无法较好地处理分类预测问题。LR 的目的在于最大化数据接近于真实标记的概率，实际上是概率模型的手段；数据越远离分隔超平面，LR的效果就越好。我们还可以根据实际情形来调整划分门槛（threshold），从而得到不同的分类结果。 SVM 试图找出一个超平面，模型优化的目标是使距离它最近的点到它的距离之和最大，通常通过核技巧使线性不可分的数据集映射到新的特征空间。 2. 参数估计方式LR 实际上是对线性回归模型的预测结果取逼近真实标记的对数几率（log odds），即反映了预测结果接近真实的可能性，可能性当然是越大越好。用最大似然估计法来估计参数 $w$，它是一种参数估计方法，LR的损失函数形式如下 \begin{equation}J(\theta)=-\frac{1}{m}\sum_{i=1}^{m}[y^{(i)}logh_w(x^{(i)})+(1-y^{(i)})log(1-h_w(x^{(i)}))]\end{equation} 与 LR 不同，SVM 的目标函数包含约束条件，它是一种非参数估计方法，故采用拉格朗日乘子法求解参数，得到损失函数如下 \begin{equation}L(w,b,\alpha)=\frac{1}{2}{||w||}^2-\sum_{i=1}^m{\alpha_i(1-y_i(w^Tx_i+b))} \label{lagrangefunction}\end{equation} 其二，两者用到的优化方法也不一样，LR 用到的优化方法有梯度下降算法（GD）和随机梯度下降算法（SGD），而 SVM 是通过序列最小最优算法（SMO）求解参数。（个人感觉理解 SMO 的难度远远大于 SGD） 对比 LR 和 SVM 的损失函数，两者都可以添加正则化项，SVM 的损失函数像是一个合页，故名为合页损失函数。合页损失函数实际上是0-1损失函数的优化版本，对比0-1损失函数，由下图，合页损失函数不仅要求分类正确，而且确信度够高时损失才会为0，所以，合页损失函数对学习有更高的要求。 3. 实验效果比较以下内容选自一篇用统计仿真方法比较 SVM 和 LR 的论文，关于 SVM 和 LR 的基础知识可以参考笔者之前的文章。下面是针对论文的部分实验结果讲解 3.1 单变量分布情形 一般情况下，多项式核 SVM 的误分类率比较高。另外，LR和线性核、径向基核、sigmoid 核 SVM 模型的表现差不多。当样本容量发生变化，sigmoid 核和多项式核比其他核的误分类率更低 样本数据不平衡的时候，LR 比 SVM 更胜一筹 在样本服从指数分布情形下，除了多项式核，SVM 跟 LR 表现得一样好，样本容量分布不平衡时，LR 比 SVM 模型表现更好；样本服从正态分布、泊松分布时，不推荐多项式核 SVM 这里是实验结果 3.2 混合分布情形 正态-泊松混合分布：SVM 比 LR 表现要好，特别是 d 很小 柯西-正态混合分布：SVM 模型比 LR 表现更佳 当基于单个变量来预测新的观测值属于哪个类别，SVM模型比LR更加合适。然而，SVM在泊松分布、指数分布和正态分布样本中，我们不推荐多项式核SVM模型，因为它的误分类率实在是太高了。考虑到多变量和多分布混合情形，当数据中存在高相关部分时，SVM比LR表现得更好。 4. 小结 视具体情况而定，优先考虑 LR，LR 简单、快速、易于解释，实在不行再考虑 SVM LR 得到的是一个可以解释决策置信度的概率，它的目标函数没有约束条件，并且十分平滑，而 SVM 不是基于概率的，它的目标函数有约束条件，因此两者的参数估计手段完全不同 SVM 的泛化性能佳，SVM 不会对有足够置信度的点支付一个惩罚，所以假设一个数据集已经被 SVM 求解，那么将一类点删除或是增加并不会改变 SVM 的求解结果 LR 在不平衡数据集中的表现优于 SVM，SVM 在样本服从混合分布情形下表现优于 LR LR 和 SVM 对数据的敏感程度不同，LR 中每一个数据点都会影响到效果，而 SVM 的性能并不依赖于整体数据 SVM 需要考虑数据的几何距离和函数距离，所以处理时需要对数据做归一化处理，而 LR 则不需要 5. 讨论 @梁斌penny: svm核方法升维，使得在高维上的线性切割，投射到实际空间就变为非线性的切割；lr通过增加大量非线性特征，使得获得非线性切割能力；深度学习通过二层以上神经网络获得非线性切割。核心都需要有非线性识别能力 @phunter_lau: 其实解决问题最有效的模型是LR和SVM，然后是决策树系列xgboost，对于特定结构化数据才是深度学习，最近火是因为一些之前不能做的大规模结构化数据问题能搞了，这不能说其他模型就没用了对吧 豆豆叶：Linear SVM不直接依赖数据分布，分类平面不受一类点影响；LR则受所有数据点的影响，如果数据不同类别strongly unbalance一般需要先对数据做balancing。Linear SVM 和 LR 有什么异同？ 6. 推荐阅读 http://www.kurims.kyoto-u.ac.jp/EMIS/journals/RCE/V35/v35n2a03.pdf www.cs.toronto.edu/~kswersky/wp-content/uploads/svm_vs_lr http://www.edvancer.in/logistic-regression-vs-decision-trees-vs-svm-part2/ 相关讨论：Linear SVM 和 LR 有什么异同？]]></content>
      <categories>
        <category>Machine-Learning</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>Logistic regression</tag>
        <tag>SVM</tag>
        <tag>比较逻辑回归和支持向量机</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[计算机程序设计（C++）作业2]]></title>
    <url>%2F2017%2F2017-03-07-MOOC-C%2B%2B-homework-2%2F</url>
    <content type="text"><![CDATA[计算机程序设计（C++）第二周单元测试及编程作业 注： a=b+c=1不正确，因为加号的优先级比等号要高，所以1不会赋值到b+c 所有的表达式不一定都有值 第4小题的执行结果应该为1 注： 三目运算符表达式&lt;条件&gt;?&lt;表达式1&gt;:&lt;表达式2&gt;；(a=(a&gt;=&#39;a&#39;&amp;&amp;a&lt;=&#39;z&#39;)?(a-32):a)表示ASCII码在小写字母a到z范围内相应的ASCII码减去32 ‘a’和a是不一样的，将字母写在一对单引号中表示字符，每个字符占一个字节，保存英文字符的ASCII码；可以参加整型数的运算，如&#39;a&#39;+1]]></content>
      <categories>
        <category>coding</category>
      </categories>
      <tags>
        <tag>C++</tag>
        <tag>MOOC</tag>
        <tag>编程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【译】如何给你的机器学习问题选择正确的算法]]></title>
    <url>%2F2017%2F2017-03-07-how-to-choose-right-algorithm-for-your-machine-learning-problem%2F</url>
    <content type="text"><![CDATA[随着机器学习浪潮的高涨，越来越多的算法在许多任务中表现得很好。但是我们通常不可能在事先知道哪种算法会是最优的。如果你有无限的时间逐一去尝试每一个算法那就另当别论。接下来的文章我们将依赖从模型选择和超参数调节中得到的知识向你一步一步展示如何来选择最优的算法。 原文地址：http://www.askaswiss.com/2017/02/how-to-choose-right-algorithm-for-your-machine-learning-problem.html Step 1:基本知识在深入讨论之前，我们应当确保已经疏通了基本的知识点。首先，我们应该知道机器学习主要有三大分类：监督学习、无监督学习和强化学习。 在监督学习中，每个数据点都有标签、类别或是数值。比如，给一幅图的标签分为猫或者狗；数值标签的例子是二手车的出售价格。监督学习的目的在于通过学习众多有标签的数据来对未来的数据做出预测——比如通过新照片识别动物（分类）、给二手车一个预测的价格。 在无监督学习中，数据点是没有标签的。另外，无监督学习算法的目标在于通过某种方式组织或描述其结构。也就是所谓的聚类，或者从不同的角度来观察复杂的数据并变使其变得简单。 而强化学习算法的目标在于根据每个数据点的反应做出选择。通常在机器人行业应用普遍，机器人通过传感器读取数据集中的数据，然后强化学习算法必须决定机器人的下一个动作。同时，强化学习算法还应用在物联网行业，比如算法收到一个信号并判断决策是好是坏。基于以上考虑，强化学习算法为了得到更高的奖赏会修改其策略。 Step 2：将问题分类接下来讨论分类问题，处理分类问题有两个步骤： 由输入数据分类：当我们有标签数据，这属于有监督学习问题。如果我们手上只有无标签数据而且我们想要发现它的结构，这属于无监督学习问题。如果我们想要通过与环境交互来最优化目标函数，这属于强化学习问题。 由输出数据分类：如果模型的输出是一个数值，这属于回归问题。如果模型的输出是一个类别，这属于分类问题。如果模型的输出是输入组的集合，那便属于聚类问题。 奏是这么简单！ 更一般地来说，当我们纠结于选择何种算法，倒不如问问自己要用算法来干什么： 分类：当数据被用来预测类别，有监督学习通常也叫做分类。比如判断一张图片中的动物是“猫”还是“狗”。当仅仅只有两种结果，属于二分类问题；当有超过两种结果时，比如预测下一届诺贝尔物理学奖的获得者，这就属于多分类问题了。 回归：当预测值是一个数值，比如股票价格，有监督学习也被称为回归 聚类：聚类分析或聚类通常是解决无监督学习问题最常用的手段。聚类是通过某种手段分组，然后将那些相似的个体聚集到一起的方法 异常值检测：有的时候算法的目的在于识别数据集中异常的点。在欺诈检测中，任何不寻常的高额信用卡消费都是值得怀疑的。正样本（欺诈手段）是如此之多，而训练样本又如此少，貌似通过算法学习出欺诈行为不可行。异常值检测的办法就是要学习出正常的消费行为是怎么样的（通过非欺诈交易的数据），并且能够识别出那种行为是明显不同的。 step3：找到合适的算法上面我们已经将需要解决的问题归类完毕，我们可以使用掌握的工具来识别出适当且实用的算法。 微软的Azure做了一个可以快速确定算法的列表，哪种算法可以应用于哪种问题上一目了然。虽然是针对Azure定制的，但是也具备普遍适用性。（PDF地址） 几个比较值得注意的算法是： 分类： 支持向量机（SVM）：支持向量机发现最大可能的划分边界。当两个类别不能清楚地得到划分时，支持向量能够帮助找到最大的可能边界。然而真正厉害之处在于，当遇到特征密集型的数据，如文本或染色体时，比起其他算法，支持向量机能够快速而且以较少过拟合的代价划分数据，另外它只会用到少量的内存。 人工神经网络（ANN）：人工神经网络是一种受到大脑启发的解决多分类、二分类和回归问题的学习型算法。它们有很多种类，包括感知机和深度学习（They come in an infinite variety, including perceptrons and deep learning）。这些算法通常要花费大量的时间进行训练，但是在实际应用领域中可以取得比较理想的效果。 逻辑斯蒂回归（Logistic regression）：尽管在名字中的“回归”二字令人费解，逻辑斯蒂回归通常却在二分类或多分类问题上是一个强有力的工具。它通过S型而非直线达到自然划分数据的效果。逻辑斯蒂回归给予了线性分类边界，所以当我们使用这个算法的时候，确保线性估计在可以承受的范围之内 决策树和随机森林：决策树（回归、二分类和、多分类、决策丛林（二分类、多分类）和提升决策树（回归和二分类）都是基于决策树这个基本的机器学习概念。决策树拥有很多类别，但无一例外地只干一件事——将特征子空间划分为标签大致相同的区域。这些区域可能是连续值也可能是类别，具体如何取决于你是做分类任务还是回归任务。 回归： 线性回归：通过一条直线（平面或超平面）拟合数据。它应用广泛、简单并且迅速，但是在一些问题上处理能力有限。 贝叶斯线性回归：拥有非常期望得到的效果：避免过拟合。贝叶斯方法通过对有可能的答案做出先验假设。另外一个优点就是，贝叶斯方法的参数很少。 提升决策树回归：正如前面提到，这个算法是基于决策树的，并通过将特征空间细分为具有大致相同标签的区域发挥效用。提升决策树通过限制其可以细分的次数以及每个区域中所允许的最少数据点来避免过拟合。该算法会构造一个树的序列，其中每棵树都会学习弥补之前的树留下来的误差。这能得到一个会使用大量的内存的非常精确的学习器。 聚类： K均值（k-means clustering）聚类的目标是将 n 组观测值分为 k 个聚类，其中每个观测值都属于其接近的那个均值的聚类，这些均值被用作这些聚类的原型。这会将数据空间分割成Voronoi 单元。 层次聚类寻求建立一个聚类层次结果，通常有两种方法。聚类聚集（Agglomerative clustering）是一种自下而上的方法，每个观测值从自己的聚类开始，随着层次的上升成对的聚类会逐渐融合。分裂聚类（Divisive clustering）是一种自上而下的方法，所有的观测值从同一个聚类开始，随着层次下降而逐渐分隔。通常，融合和分隔都是根据贪心法则的。层次聚类的结果通常以树状图的形式表现。 异常值检测 K最近邻（kNN）是用于分类和回归的非参数方法。在这两种情况下，输入都是由特征空间中与 k 最接近的训练样本组成的。在 kNN 分类中，输出是一个类成员。对象通过其 k 最近邻的多数投票来分类，其中对象被分配给 k 最近邻中最常见的类（k 为正整数，通常较小）。kNN 回归中的输出为对象的属性值，这个值为其 k 最近邻值的平均值。 单类支持向量机（One-class SVM）使用了非线性支持向量机的一个巧妙的扩展，单类支持向量机可以描绘一个严格概述整个数据集的边界。远在边界之外的任何新数据点都是非正常的，值得注意。 step4：实现所有的算法对于任意给定的问题，我们通常有很多可以解决该问题的候选算法。那么，我们如何知道该选哪一个呢？通常是不能直接得到问题的答案的，一般的办法是不断尝试。 原型开发最好分两步完成。在第一步中，我们希望通过小量的特征工程快速且粗糙地实现一些算法。在这个阶段，我们主要的目标是大概了解哪个算法表现得更好。这有点像是在招聘：得要列出一些职位的门槛来缩短列表。 一旦，候选算法列表缩短之后，真正的原型开发才刚刚开始。理想情况是，我们会建立一个机器学习的流程，使用一组经过仔细选择的评估标准来比较每个算法在数据集上的表现。在这个阶段，我们仅仅只需要处理少量的算法，所以我们可以将注意力转移到真正神奇的地方：特征工程。 step5：特征工程也许比挑选算法更重要的事情是选择正确的特征来代表数据。选择一个算法相对来说简单直接，而特征工程更像是一门艺术。 主要的问题是我们需要进行分类的数据在特征空间通常描述极少：比如，灰度和像素用来预测图片不是一个好的选择。所以，我们需要发现能提高信噪比和降低噪声的数据变换。没有这些数据变换，我们手头的工作几乎是无法完成的。比如，在方向梯度直方图（HOG）出现之前，复杂视觉任务（行人检测和面部检测）通常是很难做到的。 尽管多数特征的效果要通过实验来评估，但是了解一些选取数据特征的方法是比较好的。比较好的技术有： 主成分分析法（PCA）：是一种线性降维方法，可以找出包含信息量较高的特征主成分，可以解释数据中的大多数方差。 尺度不变特征变换（SIFT）：计算机视觉领域中的一种有专利的算法，用以检测和描述图片的局部特征。它有一个开源的替代方法 ORB（Oriented FAST and rotated BRIEF）。 加速稳健特征 （SURFSIFT ）：SIFT的更稳健版本，有专利。 方向梯度直方图（HOG）：一种特征描述方法，在计算机视觉中用于计数一张图像中局部部分的梯度方向的频率。 更多参考 当然，你也可以用你自己的特征描述。如果你有一对候选，你可以使用封装好的手段来进行智能特征选择： 前向搜索： 开始不选择任何特征 然后选择最相关的特征：给现有的特征集添加候选特征，然后计算模型的交叉验证误差；然后对剩余特征重复执行以上步骤；最后添加服从最小误差的候选特征 一直重复直到特征选择数目达到期望水平 后向搜索： 从所有特征开始 移除最不相关的特征：从现有特征集移除掉一个候选特征，然后计算模型的交叉验证误差；然后对所有其他候选特征重复执行以上步骤；最后，去掉服从最大误差的候选特征 一直重复直到特征选择数目达到期望水平 永远使用交叉验证来增减特征！ step6：优化高维参数（选择性）最后，你还需要优化算法的高维参数。比如，主成分分析中主成分的个数，k 近邻算法的参数 k，或者是神经网络中的层数和学习速率。最好的方法是使用交叉验证来选择。 一旦你做完以上所有的步骤，便有可能创造一个强力的机器学习系统。但是，正如你可能早就猜到了：细节是魔鬼，你必须不断试验（trial-and-error）。（完）]]></content>
      <categories>
        <category>Machine-Learning</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>算法</tag>
        <tag>翻译</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[计算机程序设计（C++）作业1]]></title>
    <url>%2F2017%2F2017-03-06-MOOC-C%2B%2B-homework-1%2F</url>
    <content type="text"><![CDATA[计算机程序设计（C++）第一周单元测试及编程作业 注： C++的语句必须以分号结束 变量必须先声明后使用 标识符的第一个字符必须是字母字符（大写、小写或带下划线 () 的字母）。 由于 C++ 标识符区分大小写，因此fileName 与 FileName 不同标识符不能与关键字有完全相同的拼写和大小写。 包含关键字的标识符是合法的。 例如，Pint是一个合法标识符，即使它包含 int关键字。**在标识符开头使用两个顺序下划线字符 (_) 或在单个前导下划线后跟一个大写字母的用法是专为所有范围的 C++ 实现保留的。** 由于当前或将来的保留标识符可能发生冲突，因此应避免对文件范围的名称使用一个前导下划线后跟小写字母。 一行程序中写两个分号并不会编译错误 注： 涉及如开方sqrt()、三角函数sin、cos等数学运算，要引用包&lt;cmath&gt; 变量一定要先声明然后才可以使用]]></content>
      <categories>
        <category>coding</category>
      </categories>
      <tags>
        <tag>C++</tag>
        <tag>MOOC</tag>
        <tag>编程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[计算机程序设计（C++）week01]]></title>
    <url>%2F2017%2F2017-03-02-C%2B%2B-basic-concepts%2F</url>
    <content type="text"><![CDATA[这是西安交通大学在中国大学MOOC开的计算机程序设计（C++）第二周课程笔记，比较松散，故用印象笔记软件处理了一下，直接拍照，算是偷了一点小懒。 C++的自增自减运算自增运算符++会把操作数加 1，自减运算符--会把操作数减 1。无论是自增运算符还是自减运算符，都可以放在操作数的前面（前缀）或后面（后缀），前缀形式与后缀形式之间有一点不同。如果使用前缀形式，则会在表达式计算之前完成自增或自减，如果使用后缀形式，则会在表达式计算之后完成自增或自减。 12345678910111213141516171819202122232425#include &lt;iostream&gt;using namespace std; main()&#123; int a = 21; int c ; // a 的值在赋值之前不会自增 c = a++; cout &lt;&lt; "Line 1 - Value of a++ is :" &lt;&lt; c &lt;&lt; endl ; // 表达式计算之后，a 的值增加 1 cout &lt;&lt; "Line 2 - Value of a is :" &lt;&lt; a &lt;&lt; endl ; // a 的值在赋值之前自增 c = ++a; cout &lt;&lt; "Line 3 - Value of ++a is :" &lt;&lt; c &lt;&lt; endl ; return 0;&#125;//代码执行后的结果Line 1 - Value of a++ is :21Line 2 - Value of a is :22Line 3 - Value of ++a is :23]]></content>
      <categories>
        <category>coding</category>
        <category>C++</category>
      </categories>
      <tags>
        <tag>笔记</tag>
        <tag>数据结构</tag>
        <tag>C++</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习算法系列（6）支持向量机（三）]]></title>
    <url>%2F2017%2F2017-03-02-machine-learning-algorithm-series-svm-3%2F</url>
    <content type="text"><![CDATA[上一篇文章我们介绍了如何将支持向量机的原始优化问题转化为只有一个参数的对偶问题，但是还遗留了参数的求解没有解决，这篇文章讲到的序列最小最优算法（SMO）就是一种高效实现支持向量机学习的算法。 由上一篇文章，凸二次规划的对偶问题 \begin{equation}\begin{split}min\quad&amp;\frac{1}{2}\sum{i=1}^{N}\sum{j=1}^{N}\alphai\alpha_jy_iy_jK(x_i{\cdot}x_j)-\sum{i=1}^{N}\alphai\s.t.\quad&amp;\sum{i=1}^{N}\alpha_iy_i=0\&amp;0\le\alpha_i\le{C},i=1,2,…,N\\label{dualproblem}\end{split}\end{equation} 1. SMO 算法基本思路不同于之前解决二次规划问题所采取的手段，SMO 选择在每一步解决一个最小可能的优化问题，通过求解子问题来逼近原始问题，这可以大大提高整个算法的计算速度，是一种启发式的方法。笔者感觉 SMO 与提升树中的前向分步算法有异曲同工之妙，两者都是通过“由小到大”，先做错了不要紧，慢慢来，然后逐步接近最优解的过程。 SMO 的优势： can be done analytically 不需要额外的矩阵存储空间 SMO = analytic method + heuristic（启发式） 由上述对偶问题，假设先固定除$\alpha_1,\alpha_2$之外的所有其他变量，于是对偶问题$\eqref{dualproblem}$的子问题可以等价写为 \begin{equation}\begin{split}\underset{\alpha1,\alpha_2}{min}\quad&amp;\frac{1}{2}K{11}{\alpha1}^2+\frac{1}{2}K{22}{\alpha2}^2+y_1y_2K{12}\alpha1\alpha_2\&amp;-(\alpha_1+\alpha_2)+y_1\alpha_1\sum{i=3}^{N}yi\alpha_iK{i1}+y2\alpha_2\sum{i=3}^{N}yi\alpha_iK{i2}\s.t.\quad&amp;\alpha1y_1+\alpha_2y_2=-\sum{i=3}^{N}y_i\alpha_i=k\&amp;0\le\alpha_i\le{C},i=1,2,…,N\\label{subproblem}\end{split}\end{equation} 由对偶问题$\eqref{dualproblem}$的约束条件$\sum_{i=1}^{N}\alpha_iy_i=0$，得 \alpha_1y_1+\sum_{i=2}^{N}\alpha_iy_i=0左右两边同乘一个$y_1$，得 \alpha_1{y_1}^2+y_1\sum_{i=2}^{N}\alpha_iy_i=0因为${y_1}^2$恒为1，得 \alpha_1=-y_1\sum_{i=2}^{N}\alpha_iy_i我们将$\alpha_1$代入子问题$\eqref{subproblem}$，从而消去$\alpha_1$。也就是说，对偶问题$\eqref{dualproblem}$两个变量中只有一个是自由变量，实际上是单变量的优化问题，这样问题就更加简化了！ 2. 两个变量的凸二次规划问题求解通过变换，我们将子问题变成了只有$\alpha_2$的单变量优化问题。如何求解$\alpha_2$？考虑两种情形 （1）考虑$y_1*y_2$的两种不同情形\begin{equation}\begin{split}y_1=y_2\Rightarrow\alpha_1+\alpha_2=k\\label{t1}\end{split}\end{equation}\begin{equation}\begin{split}y_1{\neq}y_2\Rightarrow\alpha_1-\alpha_2=k\\label{t2}\end{split}\end{equation}设$L$与$H$是在对角线段端点的上界和下界，由上图所示，要得到$\alpha_2$的准确取值范围，上界我们取两个最大值的最小值，下界取两个最小值的最大值即可。由$\eqref{t1}$ \begin{equation}\begin{split}L=max(0,\alpha_1+\alpha_2-C),H=min(C,\alpha_1+\alpha_2)\\label{st1}\end{split}\end{equation} 由$\eqref{t2}$ \begin{equation}\begin{split}L=max(0,\alpha_2-\alpha_1),H=min(C,C+\alpha_2-\alpha_1)\\label{st2}\end{split}\end{equation} （2）不考虑约束条件求$\alpha_{2}^{new,unc}$首先，$\alpha2$的约束条件很多，为了避免求解太繁，我们先不考虑上面两个条件的约束得到$\alpha{2}^{new,unc}$（unc，即uncertain，不确定）的最优解，然后再求约束条件下的最优解$\alpha_{2}^{new}$。 解： **设** g(x)=\sum_{i=1}^{N}\alpha_iy_iK(x_i,x)+b**令误差** E_i=g(x_i)-y_i=(\sum_{i=1}^{N}\alpha_jy_jK(x_i{\cdot}x_j)+b)-y_i,i=1,2**记** $$v_i=\sum_{j=3}^{N}\alpha_jy_jK(x_i{\cdot}x_j)=g(x_i)-\sum_{j=1}^{N}\alpha_jy_jK(x_i{\cdot}x_j),i=1,2$$ **故****子问题**$\eqref{subproblem}$**的目标函数可以写为** \begin{equation}\begin{split} w(\alpha_1,\alpha_2)&=\frac{1}{2}K_{11}{\alpha_1}^2+\frac{1}{2}K_{22}{\alpha_2}^2+y_1y_2K_{12}\alpha_1\alpha_2\\ &-(\alpha_1+\alpha_2)+y_1\alpha_1v_1+y_2\alpha_2v_2\\ \label{objectfunction} \end{split}\end{equation} **因为** \alpha_1y_1+\alpha_2y_2=k,y_1^{2}=1**所以** \alpha_1=(k-\alpha_2y_2)y_1代入目标函数$\eqref{objectfunction}$，得 \begin{equation}\begin{split}w(\alpha2)&amp;=\frac{1}{2}K{11}{(k-\alpha2y_2)}^2+\frac{1}{2}K{22}{\alpha2}^2+y_2K{12}(k-\alpha_2y_2)\alpha_2\&amp;-(k-\alpha_2y_2)y_1-\alpha_2+v_1(k-\alpha_2y_2)+y_2\alpha_2v_2\\label{alpha_2}\end{split}\end{equation} 这样目标函数只有一个未知参数了！针对单变量的凸二次规划问题，这太简单了。 （3）单变量的凸二次规划问题对$\alpha_2$求导并令其为零 \frac{\partial w}{\partial \alpha_2}=-K_{11}ky_2+K_{11}\alpha_2+K_{22}\alpha_2+y_2K_{12}k-2y_2K_{12}y_2\alpha_2+y_1y_2-1-v_1y_2+y_2v_2\\ =K_{11}\alpha_2+K_{22}\alpha_2+y_2K_{12}k-2y_2K_{12}\alpha_2k-K_{11}ky_2+y_1y_2-1-v_1y_2+y_2v_2=0得 \begin{equation}\begin{split}(K{11}+K{22}-2K{12})\alpha_2&amp;=y_2(y_2-y_1+kK{11}-kK{12}+v_1-v_2)\&amp;=y_2(y_2-y_1+kK{11}-kK{12}\&amp;+(g(x_1)-\sum{j=1}^{2}\alphajy_jK(x_1{\cdot}x_j)-b)-(g(x_2)-\sum{j=1}^{2}\alpha_jy_jK(x_2{\cdot}x_j)-b))\\label{alpha2new}\end{split}\end{equation} 将$\alpha_1y_1+\alpha_2y_2=k$代入$\eqref{alpha2new}$，得 \begin{equation}\begin{split}(K{11}+K{22}-2K{12})\alpha{2}^{new,unc}&amp;=y2((y_2-y_1+g(x_1)-g(x_2)+(K{11}+K{22}-2K{12})\alpha{2}^{old}y_2))\&amp;=(K{11}+K{22}-2K{12})\alpha_2^{old}+y_2(E_1-E_2)\end{split}\end{equation} 是不是有点头晕了？我也是，为了帮助读者快速理解，下图给出了详细的化简步骤，相信应该有帮助吧！ 我们令$\eta=K{11}+K{22}-2K_{12}$，得 \alpha_{2}^{new,unc}=\alpha_2^{old}+\frac{y_2(E_1-E_2)}{\eta}再考虑求解约束条件$\eqref{st1}$$\eqref{st2}$后的$\alpha_{2}^{new}$ （4）考虑求解约束条件$\alpha_{2}^{new}$\alpha_{2}^{new}= \begin{cases} H, & \alpha_{2}^{new,unc}\gt{H}\\ \alpha_{2}^{new,unc}, & L\le\alpha_{2}^{new,unc}\le{H}\\ L, &\alpha_{2}^{new,unc}\lt{L} \end{cases}**因为** \alpha_{1}^{new}y_1+\alpha_{2}^{new}y_2=\alpha_{1}^{old}y_1+\alpha_{2}^{old}y_2**所以** \alpha_{1}^{new}=\alpha_{1}^{old}+y_1y_2(\alpha_{2}^{old}-\alpha_{2}^{new})于是，我们经过上述的推导，终于就得到了对偶问题子问题$\eqref{subproblem}$的解$(\alpha{1}^{new},\alpha{2}^{new})$。 （5）重新计算$b$和差值$E_i$在每次完成两个变量的优化后，我们还需要将$\alpha_2^{new}$与$\alpha_2^{old}$比较，如果变化的程度不大，则要重新计算阈值$b$和差值$E_i$ 3. 基于 Python 实现 SMO 的代码分析这是基于 Python 实现 SVM 中的 SOM 算法，来自于《机器学习实战》，代码很容易读，基本可以与上文的公式推导完全对应，所以不太理解的地方，还是把 SOM 推导过程的来龙去脉先捋一捋吧！ 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273from numpy import *from time import sleep#==============================================================================# 打开文件、两个辅助函数#==============================================================================# 打开文件逐行读取数据，得到每行的类标签‘labelMat’和数据矩阵‘dataMat’def loadDataSet(fileName): dataMat = []; labelMat = [] fr = open(fileName) for line in fr.readlines(): # ‘fr.readlines()’逐行读取数据 lineArr = line.strip().split('\t') dataMat.append([float(lineArr[0]), float(lineArr[1])]) # 将每行数据的第一个和第二个位置储存到‘dataMat’矩阵 labelMat.append(float(lineArr[2])) # 每行的第三个是数据标签，存储到‘labelMat’矩阵 return dataMat,labelMat# 辅助函数1：取两个不同下标的alphadef selectJrand(i,m): j=i #we want to select any J not equal to i while (j==i): j = int(random.uniform(0,m)) return j# 辅助函数2：用于调整大于H小于L的alpha值，对应alpha2_new的更新法则，alpha2_new只能在[L,H]的区间范围之内 def clipAlpha(aj,H,L): if aj &gt; H: aj = H if L &gt; aj: aj = L return aj#==============================================================================# 简化版本SMO算法#==============================================================================def smoSimple(dataMatIn, classLabels, C, toler, maxIter): # 5个参数：数据集、类别标签、常数c、容错率、退出前的最大循环次数 dataMatrix = mat(dataMatIn); labelMat = mat(classLabels).transpose() # 类别标签‘classLabels’转置为列向量 b = 0; m,n = shape(dataMatrix) # ‘shape’得到矩阵的行数m和列数n alphas = mat(zeros((m,1))) # 初始化alpha为元素全部为零的(m*1)列矩阵 iter = 0 while (iter &lt; maxIter): alphaPairsChanged = 0 # 用于记录alpha是否已经进行优化 for i in range(m): # 外层循环 fXi = float(multiply(alphas,labelMat).T*(dataMatrix*dataMatrix[i,:].T)) + b Ei = fXi - float(labelMat[i]) # 计算误差 if ((labelMat[i]*Ei &lt; -toler) and (alphas[i] &lt; C)) or ((labelMat[i]*Ei &gt; toler) and (alphas[i] &gt; 0)): # 检查变量是否满足KKT条件，如果不满足，那么继续迭代， # 如果满足，那么最优化问题的解就得到了 j = selectJrand(i,m) # 内层循环 fXj = float(multiply(alphas,labelMat).T*(dataMatrix*dataMatrix[j,:].T)) + b # fXj为输入xj的预测值，对应本文公式(7) Ej = fXj - float(labelMat[j]) # 预测值与真实值之间的误差 alphaIold = alphas[i].copy(); alphaJold = alphas[j].copy(); if (labelMat[i] != labelMat[j]): L = max(0, alphas[j] - alphas[i]) H = min(C, C + alphas[j] - alphas[i]) else: L = max(0, alphas[j] + alphas[i] - C) H = min(C, alphas[j] + alphas[i]) if L==H: print "L==H"; continue # 计算eta eta = 2.0 * dataMatrix[i,:]*dataMatrix[j,:].T - dataMatrix[i,:]*dataMatrix[i,:].T - dataMatrix[j,:]*dataMatrix[j,:].T if eta &gt;= 0: print "eta&gt;=0"; continue # 求alpha2_new，对应alpha2_new的更新公式，注意eta与文中的符号刚好相反，所以这里是'-=' alphas[j] -= labelMat[j]*(Ei - Ej)/eta alphas[j] = clipAlpha(alphas[j],H,L)# alpha2_new与alpha2_old比较，如果变化的程度不大，则要更新阈值b和误差值E if (abs(alphas[j] - alphaJold) &lt; 0.00001): print "j not moving enough"; continue alphas[i] += labelMat[j]*labelMat[i]*(alphaJold - alphas[j]) b1 = b - Ei- labelMat[i]*(alphas[i]-alphaIold)*dataMatrix[i,:]*dataMatrix[i,:].T - labelMat[j]*(alphas[j]-alphaJold)*dataMatrix[i,:]*dataMatrix[j,:].T b2 = b - Ej- labelMat[i]*(alphas[i]-alphaIold)*dataMatrix[i,:]*dataMatrix[j,:].T - labelMat[j]*(alphas[j]-alphaJold)*dataMatrix[j,:]*dataMatrix[j,:].T if (0 &lt; alphas[i]) and (C &gt; alphas[i]): b = b1 elif (0 &lt; alphas[j]) and (C &gt; alphas[j]): b = b2 else: b = (b1 + b2)/2.0 alphaPairsChanged += 1 print "iter: %d i:%d, pairs changed %d" % (iter,i,alphaPairsChanged) # 打印迭代次数、alpha优化的个数 if (alphaPairsChanged == 0): iter += 1 else: iter = 0 print "iteration number: %d" % iter return b,alphas 小结至此，机器学习算法系列之支持向量机三部曲暂时就告一段落了！ 在这个系列文章中，我们涉及了线性可分、线性不可分、硬间隔、软间隔、序列最小优化方法等许多概念，用到了拉格朗日乘子法、线性模型等求解办法，故数学公式的推导很繁，不过“只要功夫深，铁杵磨成针”，耐心一点，再耐心一点，便会柳暗花明。如果看了笔记还是有不明白的地方，建议看看原书，跟着李航老师的思路一步一步来。 推荐阅读 《机器学习》.周志华 《统计学习方法》.李航 《机器学习实战》.Peter Harrington https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/tr-98-14.pdf]]></content>
      <categories>
        <category>Machine-Learning</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>算法</tag>
        <tag>机器学习算法系列</tag>
        <tag>支持向量机</tag>
        <tag>SMO</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习算法系列（5）支持向量机（二）]]></title>
    <url>%2F2017%2F2017-02-26-machine-learning-algorithm-series-svm-2%2F</url>
    <content type="text"><![CDATA[在先前的一篇文章我们介绍了线性可分问题的支持向量机方法，不过它的前提条件比较严苛，现实工程环境中一般不会有那么完美的数据集。但我们要求方法更具有适用性，如果碰到线性不可分问题，那就要寻求别的解决办法了。 1. 软间隔最大化针对近似线性可分的问题，硬间隔最大化的方法就不适用了，因为一旦间隔区域有异常点，软间隔的不等式约束不一定都成立。对于不能满足函数间隔条件的数据集，下面就来讲解一种软间隔最大化的方法。 所谓“软”，也就是将约束条件放松一点点，有些数据破坏了间隔条件是被容许的（不知道这也是不是一种防止过拟合的手段？），但是也不能太多。我们给约束条件加上一个松弛变量$\xi_i$，放宽约束条件，这样硬间隔最大化里面要求的间隔强制为 1 的约束条件就变为： y_i(wx_i+b)\ge1-\xi_i同时，对松弛变量加上惩罚参数 $C\gt0$，目标函数就变为 \frac{1}{2}{||w||}^2+C\sum_{i=1}^{N}\xi_i所以线性不可分的支持向量机问题变为凸二次规划问题，我们暂且称为原始问题 \begin{equation}\begin{split}\underset{w,b,\xi}{min}\quad&amp;\frac{1}{2}{||w||}^2+C\sum_{i=1}^{N}\xi_i \\s.t.\quad&amp;y_i(wx_i+b)\ge1-\xi_i, \quad\text{$i=1,2,…,N$}\\&amp;\xi_i\ge0,\quad\text{$i=1,2,…,N$}\label{origin}\end{split}\end{equation} 2. 转化为对偶问题对凸二次规划问题，我们依然选择拉格朗日乘子法求解 （1）首先对原始问题构建拉格朗日函数，$\alpha$ 和 $\mu$ 为拉格朗日乘数 \begin{equation}L(w,b,\xi,\alpha,\mu)=\frac{1}{2}{||w||}^2+C\sum{i=1}^{N}\xi_i-\sum{i=1}^{N}\alphaiy_i(wx_i+b)-\sum{i=1}^{N}\alphai\xi_i+\sum{i=1}^{N}\alphai-\sum{i=1}^{N}\mu_i{\xi}_i \label{lagrangefunction}\end{equation} （2）极小化$L(w,b,\alpha)$，分别对$w,b,\xi$求偏导并令其等于零 \begin{equation}\begin{split}&amp;\frac{\partial L}{\partial w}=w-\sum{i=1}^{N}\alpha_iy_ix_i=0 \\&amp;\frac{\partial L}{\partial b}=\sum{i=1}^{N}\alpha_iy_i=0 \\&amp;\frac{\partial L}{\partial \xi}=C-\alpha_i-\mu_i=0\end{split} \end{equation} 得 \begin{equation}\begin{split}&amp;w=\sum{i=1}^{N}\alpha_iy_ix_i\\&amp;\sum{i=1}^{N}\alpha_iy_i=0\\&amp;C-\alpha_i-\mu_i=0\end{split}\end{equation} 然后将上述三个式子带入拉格朗日函数$\eqref{lagrangefunction}$即可将消去$w$和$b$，即 L(w,b,\xi,\alpha,\mu)=-\frac{1}{2}\sum_{i=1}^{N}\sum_{j=1}^{N}\alpha_i\alpha_jy_iy_jx_ix_j+\sum_{i=1}^{N}\alpha_i对偶问题是拉格朗日函数极大极小问题，求上式对$\alpha$的极大，即得对偶问题 \begin{equation}\begin{split}max\quad&amp;-\frac{1}{2}\sum{i=1}^{N}\sum{j=1}^{N}\alphai\alpha_jy_iy_jx_ix_j+\sum{i=1}^{N}\alphai\s.t.\quad&amp;\sum{i=1}^{N}\alpha_iy_i=0\&amp;C-\alpha_i-\mu_i=0\&amp;\alpha_i\ge0,\mu_i\ge0,i=1,2,…,N\end{split}\end{equation} 看出来没有？当拉格朗日函数转化为其对偶问题的形式，令人激动的是优化问题神奇般地只剩下一个参数了！如果我们设${\alpha_i}^*$是对偶问题的一个解，那么原始问题$w^*,b^*$的解可由下式求得，接下来的任务只需要求解$\alpha$了，如下 \begin{equation}w^*=\sum{i=1}^{N}{\alpha_i}^*y_ix_i\b^*=y_j-\sum{i=1}^{N}y_i{\alpha_i}^*(x_i·x_j)\end{equation} 3. 非线性支持向量机与核函数针对非线性分类问题，我们一般使用非线性支持向量机的方法，其主要特点是核技巧（kernel trick）。核技巧的基本想法是通过非线性变换将输入空间（一般是欧式空间或离散集合）映射到另一个特征空间，简单一点理解就是函数变换。 \phi(x):x\to\mathcal{H}K(x,z)=\phi(x)\cdot\phi(z)其中$K(x,z)$为核函数，$\phi(x)$为映射函数 因此，当核技巧应用于支持向量机中，对偶问题的目标函数中的内积$(x_i,x_j)$便可以用核函数$K(x,z)=\phi(x)\cdot\phi(z)$来代替，故对偶问题的目标函数变为只有$\alpha$一个参数了： w(\alpha)=-\frac{1}{2}\sum_{i=1}^{N}\sum_{j=1}^{N}\alpha_i\alpha_jy_iy_jK(x_i,x_j)+\sum_{i=1}^{N}\alpha_i3.1 几种常见的核函数 线性核 多项式核 高斯核（也称RBF核） 拉普拉斯核 sigmoid核 在实际中，我们经常会碰到非线性的分类问题，为了避免复杂的线性变换，这时需要应用核技巧来将数据从低维空间映射到高维空间，然后再采用线性分类器的学习方法训练模型，关于核函数的解释，知乎上的一个回答非常形象机器学习有很多关于核函数的说法，核函数的定义和作用是什么？ - 知乎，这篇文章也值得研究支持向量机: Kernel « Free Mind。 小结这篇文章介绍了软间隔方法和核函数，以及如何将原始问题转化为只有一个参数的对偶问题，至于如何求解 ${\alpha_i}$，在接下来的文章中，我们会学习一种专门对付求解${\alpha_i}$的SMO算法。 推荐阅读 《机器学习 Machine Learning》 周志华 《统计学习方法》李航 第七章]]></content>
      <categories>
        <category>Machine-Learning</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>算法</tag>
        <tag>机器学习算法系列</tag>
        <tag>支持向量机</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习算法系列（4）支持向量机（一）]]></title>
    <url>%2F2017%2F2017-02-26-machine-learning-algorithm-series-svm%2F</url>
    <content type="text"><![CDATA[上个学期忙着写论文，博客因此而有一段时间没有打理。看看自己的博客，距离机器学习算法系列的上一篇文章已经两个多月过去了，说来实在惭愧。这篇文章主要讲支持向量机的公式推导，因为许多“面经”都有提到面试官会考应聘者的公式推导，SVM 的尤其频繁。其实推导的过程比较好理解，主要的方法是拉格朗日对偶问题的优化，难一点的地方在于对 KKT 条件、SMO 算法的理解。看李航老师的《统计学习方法》的时候，密密麻麻的字母下标还是伤了不少神，不过只要有耐心，理解了之后便有“柳暗花明”的感觉。废话少说，开始推导！ 1. 线性可分支持向量机原始问题支持向量机（Support vector machine， SVM）是一种分类模型，它的任务是在特征空间上找到正确划分数据集的间隔（gap），并且能使几何间隔最大的分离超平面，简而言之，就是要找到一个平面，能够将空间中的点隔开！如果我们定义超平面可以通过线性方程：$w^Tx+b=0$ 来表示，将其记为 $(w,b)$。那么空间中任意一点 $x$ 到该超平面 $(w,b)$ 的距离可以表示为 \begin{equation}r=\frac{|w^Tx+b|}{||w||}\end{equation} 那么如何找到使得间隔最大的分离超平面呢？我们先从最简单的情形开始分析，假设超平面可以将数据集 100% 正确分类，若 $w^Tx_i+b&gt;0$，有 $y_i=+1$，若 $w^Tx_i+b&lt;0$，有 $y_i=-1$，令 \begin{equation}\left{\begin{array}{c}w^Tx_i+b\ge+1, y_i=+1; \w^Tx_i+b\le-1, y_i=-1.\end{array}\right.\end{equation} 2. 原始问题转化为对偶问题如果我们将可以容忍的间隔设为1，也就是要求距离该平面 $(w,b)$ 正负距离为1的空间内没有一个异常点（outlier）存在，那么寻找最大分离的超平面就转化为如下的约束最优化问题： \begin{equation}\underset{w,b}{max}\ \frac{2}{||w||} \s.t.\ y_i(w^Tx_i+b)\ge1,\ i=1,2,…,m.\end{equation} 如何求解这个最优化问题呢？ （1）构建拉格朗日函数 该问题的拉格朗日函数可以等价写为（求$\frac{2}{||w||}$的最大值等价于求$\frac{1}{2}{||w||}^2$最小值） \begin{equation}L(w,b,\alpha)=\frac{1}{2}{||w||}^2-\sum_{i=1}^m{\alpha_i(y_i(w^Tx_i+b-1))} \label{lagrangefunction}\end{equation} 这是一个凸二次规划问题，通过拉格朗日乘子法可得到其对偶问题。应用拉格朗日对偶性，原始问题$(w,b)$的对偶问题$(\alpha)$是极大极小问题： max\ min\ L(w,b,\alpha)分别令拉格朗日函数对$w$和$b$的偏导数为零可得 \begin{equation}w=\sum_{i=1}^m\alpha_iy_ix_i\end{equation} \begin{equation}0=\sum_{i=1}^m\alpha_iy_i\end{equation} 然后将上述两个式子带入拉格朗日函数$\eqref{lagrangefunction}$即可将消去$w$和$b$\begin{equation}\begin{aligned}L(w,b,\alpha)&amp;=\frac{1}{2}\sum{i=1}^{N}\sum{j=1}^{N}\alphai\alpha_jy_iy_j(x_i·x_j)-\sum{i=1}^{N}\alphaiy_i((\sum{j=1}^{N}\alphajy_jx_j)x_i+b)+\sum{i=1}^{N}\alphai\&amp;=\frac{1}{2}\sum{i=1}^{N}\sum{j=1}^{N}\alpha_i\alpha_jy_iy_j(x_i·x_j)+\sum{i=1}^{N}\alpha_i\end{aligned}\end{equation}然后就得到了原约束问题的对偶问题，即求拉格朗日函数$L(w,b,\alpha)$对$\alpha$的极大 \begin{equation}\underset{\alpha}{max}\ \sum{i=1}^m\alpha_i-\frac{1}{2}\sum{i=1}^m\sum{j=1}^m\alpha_i\alpha_jy_iy_j(x_i·x_j)\s.t.\ \sum{i=1}^{N}\alpha_iy_i=0\\alpha_i\ge0,\ i=1,2,…,N\end{equation} 故原始问题转化成为了求解对偶问题，为什么要这样做呢？这样做的优点是对偶问题需要求解的参数数量下降，原来需要考虑两个参数 $(w,b)$ ，现在只需要关注 $\alpha$ 便可以了，往往更容易求解。然后可以引入核函数，进而推广到非线性分类问题。设$\alpha^*$是对偶问题的最优化解，那么 \begin{equation}w^=\sum_{i=1}^m{\alpha_i}^yix_i\b^*=y_j-\sum{i=1}^{N}{\alpha_i}^*y_i(x_i·x_j)\end{equation} 求得了$w^$，$b^$，也就得到了我们需要的分离超平面。 当然这篇文章主要针对的是线性可分问题，那么线性不可分问题以及对偶问题的求解又怎么解决呢？后面的文章将会继续解释。 我把上述拉格朗日乘子法的求解过程用手写了一遍，辅助理解。 推荐阅读 周志华.机器学习[M].2016年1月第一版.清华大学出版社.2016 李航.统计学习方法[M].2012年3月第1版.清华大学出版社.2015 继续阅读机器学习算法系列笔记！]]></content>
      <categories>
        <category>Machine-Learning</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>算法</tag>
        <tag>支持向量机</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习算法系列（3）Logistic Regression]]></title>
    <url>%2F2016%2F2016-11-29-machine-learning-algorithm-series-logistic-regression%2F</url>
    <content type="text"><![CDATA[上一篇文章讲到了线性模型，线性模型形式十分简单，却有丰富的变化。一般线性模型有一定的缺陷，那就是$y=w^Tx+b$的预测值是为数值型的，当面对要求预测值为离散型就有些力不从心了，它会很容易受到异常值的影响，从而导致误差。那么，可不可以令预测值$y$变成另外一种形式呢？比如，在分类任务里，我们要求预测值为离散型的，得到一个是或否的答案，不再是原来的连续性预测值。这里就要用到机器学习中的一个重要的模型——Logistic Regression，即逻辑斯蒂回归或对数线性回归(log-linear regression) \begin{equation}\mathrm{ln}y=w^Tx+b\end{equation} 实际上是在试图让$e^{w^T+b}$逼近$y$，形式上仍然是线性回归，但实质上是在求线性空间到非线性空间的映射。 目录 Logistic 分布 Logistic 函数 极大似然估计(MLE) 梯度下降算法(Gradient Descent) Python代码实现及分析 1. Logistic 分布在学习 Logistic regression 之前，我们有必要了解一下 logistic 分布函数和密度函数 \begin{equation}F(x)=P(X\le{x})=\frac{1}{1+e^{-(x-\mu)/\gamma}}\end{equation} \begin{equation}f(x)=F’(x)=\frac{e^{-(x-\mu)/\gamma}}{\gamma{(1+e^{-(x-\mu)/\gamma})}^2}\end{equation} 式中，$\mu$为位置参数，$\gamma\gt0$是形状参数 逻辑斯蒂函数实际上是线性回归模型的预测结果取逼近真实标记的对数几率，其对应的模型是对数几率回归 2. Logistic 函数考虑一个二分类任务，其输出标记为$y∈{0,1}$，而线性回归模型产生的预测值z是实数值，于是我们需要将z转换为0/1值，理想的方法是logistic函数 \begin{equation}y=\frac{1}{1+e^{-z}}\end{equation} logistic 函数是一种“Sigmoid函数”，它将$z$值转化为一个接近 0 或 1 的$y$值，并且输出值在$z=0$附近的变化很陡峭，若预测值大于零就判为正例，小于零就判为反例，预测值为临界值零则可任意判别 如果我们将线性模型$z=w^Tx+b$代入上式，得 \begin{equation}y=\frac{1}{1+e^{-(w^Tx+b)}}\end{equation} 变形 \begin{equation}\mathrm{ln}\frac{y}{1-y}=w^Tx+b\label{zheng}\end{equation} 若将$y$视为$x$作为正例的可能性，$1-y$视为反例的可能性，两者的比值称为“对数几率”(logit odds)，反映了$x$作为正例的可能性。 由此可以看出，实际上是在用线性回归的模型的预测结果去逼近真实标记的对数几率，因此其对应的模型称为“对数几率回归”(logistic regression)。 3. 极大似然估计(MLE)类似线性回归，我们需要定义一个损失函数(loss function)，然后通过最小化损失函数来训练出一个分类器，对于logistic regression，哪种损失函数表现最好？假设选用0-1损失函数，考虑1000个样本，用训练得到的分类器分类，960个被分在了正确的一类，其余40个划分错误，那么这里损失函数的大小就是40。考虑调整$w$的大小，得到的损失函数值可能仍然是相同的，没有可以优化的空间。0-1损失函数看来是行不通，不妨看看log损失函数，为什么选择log损失函数，考虑-log(x)函数图像，这个函数图像在趋近于0的地方函数值趋于无穷大。相比0-1损失函数，它的惩罚性能实在太好，考虑公式（10），假设预测值$h_w(x^{(i)})$为1，而实际标签$y^{(i)}$为0，结果便是损失函数会变得很大。并且它的损失函数还是凸函数，存在可优化的空间。 由 logistic 函数，建立如下表达式 \begin{equation}h_w(x)=g(w^Tx)=\frac{1}{1+e^{-w^Tx}}\end{equation} 根据$\eqref{zheng}$，$p(y=1|x)$为正例的概率 $y$，解之，显然有 \begin{equation}\begin{aligned}p(y=1|x)&amp;=h_w(x)\p(y=0|x)&amp;=1-h_w(x)\p(y|x)&amp;={h_w(x)}^y{(1-h_w(x))}^{1-y}, y=0, 1\end{aligned}\end{equation} 于是，我们可以通过极大似然估计法(maximum likelihood method)来估计$w$（为了计算的方便，将偏置项$b$的权重设为1，记为$w_0$） 给定数据集${(xi,y_i)}{i=1}^m$，写出对应极大似然函数 \begin{equation}\begin{aligned}L(w)&amp;=\prod{i=1}^{m}p(y^{(i)}|x^{(i)};\theta) \&amp;=\prod{i=1}^{m}h_w{(x^{(i)})}^{y^{(i)}}{(1-h_w(x^{(i)}))}^{1-y^{(i)}}\end{aligned}\end{equation} 再对两边取对数 \begin{equation}logL(w)=\sum_{i=1}^{m}[y^{(i)}logh_w(x^{(i)})+(1-y^{(i)})log(1-h_w(x^{(i)}))]\end{equation} 因此最优解可以通过最大化负的似然函数得到 \begin{equation}J(w)=-\frac{1}{m}\sum{i=1}^{m}[y^{(i)}logh_w(x^{(i)})+(1-y^{(i)})log(1-h_w(x^{(i)}))]\J(w)=-\frac{1}{m}\sum{i=1}^{m}[y^{(i)}log\frac{hw(x^{(i)})}{1-h_w(x^{(i)})}+log(1-h_w(x^{(i)}))]\J(w)=-\frac{1}{m}\sum{i=1}^{m}[y^{(i)}(w^Tx)-log(1+e^{w^Tx})]\end{equation} 4. 梯度下降算法(Gradient Descent)上式中$J(w)$是一个关于$\theta$的高阶可导连续凸函数，根据凸优化理论，采用梯度下降法求最优解，令$\theta=(w,b),minJ(\theta)$ \begin{equation}{\theta}_j:={\theta}_j-\alpha\frac{\partial}{\partial{\theta}_j}J(\theta)\end{equation} 其中，$\frac{\partial}{\partial{\theta}j}J(\theta)=\frac{1}{m}\sum{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})x_j^{(i)}$，$\alpha$是步长，当然经过化简之后，上式对$w$的求导是很方便的。 推导过程如下，分部求导。$h(w)$ 对 $w$ 求导的结果是 $-h(w)(1-h(w))x$ \begin{equation}\begin{split}\frac{\partial}{\partial{w}j}J(w) =&amp; \frac{\partial{J(w)}}{\partial{h_w(x)}} \cdot \frac{\partial{h_w(x)}}{\partial{w}_j} \=&amp; [y^{(i)}+(y^{(i)}-1)/(1-h_w(x^{(i)})]\cdot [h_w(x^{(i)})(1-h_w(x^{(i)}))] x^{(i)}\=&amp; [(y^{(i)}h_w(x^{(i)})(1-h_w(x^{(i)}))+h_w(x^{(i)}(1-y^{(i)}))]\cdot x^{(i)}\=&amp; (h{w}(x^{(i)})-y^{(i)})\cdot x_j^{(i)}\end{split}\end{equation} 总的来说，logistic regression 是一类比较简单的分类算法，可以把它看做是一类广义线性模型，即线性模型的延伸即可。logistic regression 能够很好地胜任二分类问题，logistic regression 中关键的问题在于参数优化，传统的0-1损失函数非凸，我们不能对其进行优化以得到一个较优参数，log 损失函数是一个不错的选择，它使得损失函数呈凸函数形状（本身并非平方损失函数），这就让梯度下降算法有了施展的空间。 5. Python 代码实现 LR 及分析12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061from numpy import *import numpy as npimport matplotlib.pyplot as plt# 加载数据集，loadDataSet()函数的主要功能是打开文本文件，并逐行读取；# 每行的前两列分别为X1,X2，除此以外，还为偏置项设置一列X0def loadDataSet(): data = []; label = [] fr = open('E:/Python/data/testSet.txt') for line in fr.readlines(): lineArr = line.strip().split() data.append([1.0, float(lineArr[0]), float(lineArr[1])]) label.append(int(lineArr[2])) return data, labeldata, label = loadDataSet()# 定义sigmoid函数def sigmoid(z): return 1.0 / (1 + exp(-z))# 定义梯度下降算法# 设定步长alpha为0.001，迭代次数为500次，初始权重theta为长度为n个值全为1的向量def gradDscent(data, label): dataMatrix = np.matrix(data); labelMatrix = np.matrix(label).T m, n = shape(dataMatrix) alpha = 0.001 iters = 500 theta = ones((n, 1)) for k in range(iters): # 梯度下降算法，因为要求损失函数的最小值 # 对应公式（12） h = sigmoid(dataMatrix * theta) error = (h - labelMatrix) theta = theta - alpha * dataMatrix.T * error return thetatheta = gradDscent(data, label)theta = array(theta)dataArr = array(data)# 画出决策边界def plotfit(theta): n = dataArr.shape[0] x1 = []; y1 = [] x2 = []; y2 = [] for i in range(n): if int(label[i]) == 1: x1.append(dataArr[i,1]); y1.append(dataArr[i,2]) else: x2.append(dataArr[i,1]); y2.append(dataArr[i,2]) fig = plt.figure() ax = fig.add_subplot(111) ax.scatter(x1, y1, s = 30, c = 'red', marker = 'o') ax.scatter(x2, y2, s = 30, c = 'blue', marker = 'x') # 创建等差数列，设定x的取值范围为-3.0到3.0 x = arange(-3.0, 3.0, 0.1) y = (-theta[0] - theta[1] * x) / theta[2] ax.plot(x,y) plt.xlabel('X1');plt.ylabel('X2') plt.show()plotfit(theta) 分类的结果看起来还不错，从图上看，只有4个点被错分。 6. 继续优化：随机梯度下降（SGD）梯度下降算法在每次更新系数时都需要遍历整个数据集，这样就带来了训练速度变慢的问题，改进的办法是每一次只用一个样本点来更新回归系数，这样的办法被称为随机梯度下降算法。不同于梯度下降算法，在随机梯度下降算法中， 1234567891011121314151617181920212223242526272829303132333435def stoGradDscent0(dataMatrix, labelMatrix): m, n = shape(dataMatrix) alpha = mat([0.01]) theta = mat(ones(n)) for i in range(n): h = sigmoid(sum(dataMatrix[i] * theta.T)) error = h - labelMatrix[i] theta = theta - alpha * error * dataMatrix[i] return thetatheta = stoGradDscent0(data, label)def plotfit1(theta): import matplotlib.pyplot as plt import numpy as np dataArr = array(dataMatrix) n = dataArr.shape[0] x1 = []; y1 = [] x2 = []; y2 = [] for i in range(n): if int(label[i]) == 1: x1.append(dataArr[i,1]); y1.append(dataArr[i,2]) else: x2.append(dataArr[i,1]); y2.append(dataArr[i,2]) fig = plt.figure() ax = fig.add_subplot(111) ax.scatter(x1, y1, s = 30, c = 'red', marker = 'o') ax.scatter(x2, y2, s = 30, c = 'blue', marker = 'x') x = arange(-3.0, 3.0, 0.1) #创建等差数列 y = (-theta[0,0] - theta[0,1] * x) / theta[0,2] ax.plot(x,y) plt.xlabel('X1');plt.ylabel('X2') plt.show()plotfit1(theta) 第一次优化的效果不佳，有差不多三分之一的点被误分类，为此进行第二次算法优化 1234567891011121314151617181920def stoGradDscent1(dataMatrix, labelMatrix, iters = 150): m, n = shape(dataMatrix) theta = mat(ones(n)) for j in range(iters): dataIndex = range(m) # m = 100 for i in range(m): # 因为alpha在每次迭代的时候都会调整，这可以缓解数据的波动，alpha会减小，但不会到零 # 通过随机选取样本来更新回归系数，这种方法可以减少周期性的波动 # uniform()方法将随机生成下一个实数，它在[x,y]范围内 alpha = 4 / (1.0+j+i) + 0.01 randIndex = int(random.uniform(0,len(dataIndex))) h = sigmoid(sum(dataMatrix[randIndex]*theta.T)) error = h - label[randIndex] theta = theta - alpha * error * dataMatrix[randIndex] return thetatheta = stoGradDscent1(dataMatrix, labelMatrix, iters = 150)thetaplotfit1(theta) 优化参数后的算法可以明显看出分类效果提升了不少，仅仅只有两个点被误分类。 关于 Logistic Regression 的讨论 为什么 LR 模型要使用 sigmoid 函数？ 最大熵原理是概率模型学习的一个准则，最大熵原理认为，熵最大的模型是最好的模型。 逻辑斯蒂回归？ Logistic Regression 中文翻译为“逻辑斯谛回归”，梳理了一下周志华老师在微博上的叙述，整理如下：Logistic Regression 与中文的“逻辑”没有半点关系，不是 logic，而是 logit，logistic 大概的意思是“logit 的”，而不是“log 的”，所以周老师将 Logistic Regression 翻译为对数几率回归。感兴趣的同学可以继续阅读这篇文章：趣谈“logistic”：物流？后勤？还是“逻辑斯谛”？ 逻辑回归如何进行多分类？ 如果我们要用逻辑回归进行多分类任务，那么权重矩阵不再是$m × 1$，而是 $m × n$，并且需要用到 softmax 函数来进行归一化。softmax 函数能将 k 维数组中的元素压缩到 0-1 之间，并且所有元素的和为 1，所以考虑到逻辑回归的输出实际上是该分类的可能性估计，对于多分类问题，sigmoid 函数无法将多分类的输出映射到 0-1 之间，而 softmax 正好可以满足这个要求。 假设现有一个 $K$ 类的数据集，我们先模拟二分类时的处理办法，设第 K 类为主要类别，所有前 K-1 类为次要类别，由公式（6）： 以上推导来自多类别逻辑回归（Multinomial Logistic Regression） - huangjx36的博客 - CSDN博客，我就不造轮子了。 推荐阅读 为什么 LR 模型要使用 sigmoid 函数，背后的数学原理是什么？]]></content>
      <categories>
        <category>Machine-Learning</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>算法</tag>
        <tag>机器学习算法系列</tag>
        <tag>Logistic Regression</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[R使用笔记（3）数据导入]]></title>
    <url>%2F2016%2F2016-11-19-R-data-input%2F</url>
    <content type="text"><![CDATA[学习三种常用导入数据方法 使用键盘输入R中有一个函数edit()可以自动调用一个允许手动输入数据的编辑器，如 123data &lt;- data.frame(age=numeric(0))data &lt;- edit(data) #之后便会弹出编辑框 从带分隔符的文本中导入数据使用read.table()从带分隔符的文件中导入数据，并将其保存为一个数据框 mydataframe &lt;- read.table(file, option) 函数read.table()的选项 选项 描述 header 文件在第一行是否包含变量名的逻辑性变量 sep sep=""表示用空格分隔数据，sep=","表示用逗号分隔，sep="/t"表示用制表符分隔数据 row.names,col.names 用于指定行名列名，header=FALSE时变量会被命名为V1、V2··· na.strings 选择用于表示缺失字符向量，na.strings=c("-9","?")把-9和？全部转换为NA skip 读取数据时跳过行的数目 colClasses 指定以何种格式读取数据的列 导入Excel数据读取Excel文件最好的方式是将其在Excel中导出为一个csv文件，并用read.table()函数将其导入到R中。另外还可以用xlsx()配合xlsxjars,rJava包，不过比较麻烦（还要安装Java），还是转化成为csv文件吧，方便！ 高级导入方法前面介绍的是初级阶段的数据导入，高级阶段的包括导入XML数据、网页数据、SPSS数据、Stata数据等等，限于篇幅，这里就不一一赘述了。 特别要推荐的一款软件：Stat/Transfer，它是一款可以在34种数据格式之间作转换的独立应用程序。]]></content>
      <categories>
        <category>coding</category>
        <category>R</category>
      </categories>
      <tags>
        <tag>数据分析</tag>
        <tag>R</tag>
        <tag>数据结构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[思想王国|从0到1：开启商业与未来的秘密]]></title>
    <url>%2F2016%2F2016-11-14-reading-note-zero-to-one%2F</url>
    <content type="text"><![CDATA[“在什么重要问题上你与其他人的有不同的看法？” “We wanted flying cars, instead we got 140 characters.” 《从0到1》一书是关于如何创建创新公司的，事实上，这绝非是一本创业成功秘笈。 此书源于2012年作者在斯坦福大学教授的一门关于创业的课程，学生在校园可能熟练几项专业技能，但是学校从来没有教过他们怎么在社会中使用这些技能，这么课程的意义就在于帮助学生打开视野。 什么才是创新？大部分人认为世界的未来是由全球化决定的，但事实是——科技更有影响力。 创新是从0到1，即从无到有，属于垂直或深入的进步。代表现象是新兴科技的出现，科技可以使事情变得更简单，更容易带来经济的垂直进步，例如PayPal、天猫双十一、埃隆·马斯克。创新侧重的是凭空创造一件新的事物，新事物的出现往往会改变现有的竞争格局，从而获得超额利润。 而制造是从1到n，即“复制”、“重复”，属于水平或广泛的进步。代表现象是新兴工业国家，如中国，追赶美国的道路，属于简单的复制。从1到n更多的存在于追赶者的群体，他们没有工业技术累计的基础，前期只能依靠复制，后期寄希望于弯道超车。 垄断真的不好吗？经济学家醉心于鼓励企业之间的竞争，当市场到达完全竞争阶段，产品趋于同质化，竞争企业的利润趋近于零。2012年，美国航空公司从每位乘客上可以赚到37美分。而谷歌，创造的价值随不如美国航空，但是其利润率却是航空业的100多倍，市值是美国所有航空公司市值之和的3倍多。 航空业企业众多，多家公司之间存在竞争，而谷歌只有一家。 彼得·蒂尔强调垄断（当然不是恶意竞争的那种垄断），苹果公司设计的手机长期占据行业利润头把交椅，并非她人为造成“稀缺”。 从热力学的角度，平衡即静态，静态就是死亡。若是企业处在竞争平衡的格局中，那么它便不能对世界有丝毫影响，随时会有被取代的可能性。 彼得·蒂尔大胆提出 “因此垄断并不是商界症结所在，也不是异常存在，而是每个成功企业的写照。” 有创意的垄断企业不仅对外界社会没有坏的影响，反而它们是使得社会更加美好的驱动力。 什么决定未来的成功？看待未来的方式可以从两个维度来分析：未来明确度$X_1$（明确、不明确）、对未来的态度$X_2$（乐观、悲观）。下图为各国看待未来的方式，中国比较有意思，其他国家都害怕中国将要统治整个世界，而中国是唯一一个认为自己不会统治世界的国家。 \begin{array}{c|lcr} & \text{明确的未来} & \text{不明确的未来} \\ \hline 乐观的未来 & 1950-1970年的美国 & 1982年至今的美国 \\ 悲观的未来 & 现在的中国 & 现在的欧洲 \\ \end{array}那么如何在未来取得成功？长期的规划仍然很重要。 彼得·蒂尔对格拉德威尔“成功源于运气和偶然的优势”不屑一顾，他以马克·扎克伯格的Facebook为例，雅虎公司2006年出价10亿美元要收购Facebook，被扎克伯格果断拒绝。扎克伯格清楚地知道自己将要领导公司开创一个怎样的未来，而雅虎不清楚。 幂次法则 $For\ to\ all\ those\ who\ have,\ more\ will\ be\ given,\and\ they\ will have an abundance;\ but\ from\ those\ who\ have\ nothing,\even\ what\ they\ have\ will\ be\ taken\ away.$ “凡是有的，还要加给他，叫他有余。凡没有的，连他所有的，也要夺过来。”（《马太福音》第25章29节） 风险投资基金通常要10年之后才能退出，因为成功的公司需要时间成长。投资人最重要的任务是明白什么时候投资项目会出现指数级增长，但是现实是永远也不可能知道；大多数投资人低估了差异的程度，他们所期待的风险投资汇报呈正态分布，然而实际情况是最优秀的前两名投资项目获得的收益往往比剩下项目的全部还要多，这就是所谓的幂次法则。 你必须懂得如何投资，如何分配自己有限的资源。最重要的事情都是独一无二的，一个市场会胜过其他所有市场，一种分销策略通常要优于其他所有策略。 秘密=天才的想法？！几乎没有人能够想象仅仅靠出行的人和愿意载客的司机这个想法就能造就滴滴、Uber这样的公司，也没有人相信只靠提供小型商家信息的阿里巴巴能够成为全球知名电商。 决定这些公司成功的是初创公司的秘密（创意）。地理隔阂日渐消失，信息技术的不断发展，这个世界还存在秘密吗？为什么有的人不愿意去探索秘密？如何探索秘密？ 探索秘密的最佳办法就是无人关注的地方，但是往往这些秘密在成功之前被大大低估。 初创公司&amp;打造公司文化 Get busy living or get busy dying. 初创公司的基础决定了公司以后的成败，以为以后任何的修改都是极端困难的。比如，1791年批准的《人权》法案，美国《宪法》只修改过17次。在公司成立之初，要注意合伙人选择、所有权经营权控制权的分配、股权激励政策比现金奖励更为奏效······ 你不必效仿谷歌公司的公司文化：装饰温馨的墙面、吃不完的零食、随处可用的健身器材······ 彼得·蒂尔认为找准对的人一起共事才是最重要的企业文化——即帮派文化 你只需提供健康保险之类的基本福利，并许之其他公司无法提供的，即同优秀同事一道完成不可替代工作的机会。在薪酬福利上你可能比不上2014年的谷歌，但如果能就公司使命和团队给出最好的回答，你便与1999年的谷歌站在同一高度。 不能忽视销售的作用技术人员往往看不起销售人员，认为销售是一种没有技术含量的工作，根本不值一提。事实正好相反，因为工程科技的挑战显而易见，人们高估了工程的难度。而销售人员在背后要付出很大的努力才能使销售看起来比较容易进行。 如何销售产品？销售的第一要务是说服，而不是真诚。销售的对象不仅限于产品，还有想法、观念等等；销售的方式有病毒式营销、常规市场销售、复杂销售等形式；被销售的对象也不仅限于顾客，还有个人、媒体、政府和企业。伟大的产品经理乔布斯、从NASA拿到10亿美元合同的埃隆·马斯克都是绝顶聪明的销售人员（请允许我这么称呼）。 总结正如这本书一开始强调的，这是一本关于创业的书。 按照我的理解，这是一本关于如何创立顶级伟大公司的书，所以如果你没有抱着成立一家伟大公司的梦想，那么它对你一无是处。 彼得·蒂尔在书中给大众传递一个信息，创业不是跟风，那是从1到n，创业是创新，是从无到有，从0到1。你必须在创业理念、人员组建、企业文化、销售模式等方面占据绝佳地位才有可能在九死一生的竞争中存活下来。]]></content>
      <categories>
        <category>思想王国</category>
      </categories>
      <tags>
        <tag>读书笔记</tag>
        <tag>商业</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[R使用笔记（2）数据结构]]></title>
    <url>%2F2016%2F2016-11-13-R-data-structure%2F</url>
    <content type="text"><![CDATA[这篇文章主要涉及R语言中的五种数据结构 数据结构R中有许多用于存储数据的结构，包括标量(只含一个元素的向量)、向量（vector）、数组（array）、数据框（data frame）和列表（list）。 向量向量用于存储数值型、字符型或逻辑型数据的一维数组，结合函数c()可以用来创建向量。 示例2-1123a &lt;- c(1,2,3,4,5) #数值型b &lt;- c(&quot;one&quot;,&quot;two&quot;,&quot;three&quot;) #字符型c &lt;- c(TRUE,TRUE,FALSE) #逻辑型 矩阵矩阵是一个二维数组，每个元素都拥有相同的模式，可以通过matrix()创建矩阵。 示例2-2 1234567891011121314151617181920212223&gt; y &lt;- matrix(1:20, nrow=5, ncol=4)&gt; y [,1] [,2] [,3] [,4][1,] 1 6 11 16[2,] 2 7 12 17[3,] 3 8 13 18[4,] 4 9 14 19[5,] 5 10 15 20&gt; cells &lt;- c(1,26,24,68)&gt; rnames &lt;- c(&quot;R1&quot;, &quot;R2&quot;)&gt; cnames &lt;- c(&quot;C1&quot;, &quot;C2&quot;)&gt; mymatrix &lt;- matrix(cells, nrow=2, ncol=2, byrow=TRUE, dimnames=list(rnames, cnames))&gt; mymatrix C1 C2R1 1 26R2 24 68&gt; mymatrix &lt;- matrix(cells, nrow=2, ncol=2, byrow=FALSE, dimnames=list(rnames, cnames))&gt; mymatrix C1 C2R1 1 24R2 26 68&gt; y[1, c(3,4)] #矩阵下标的使用[1] 11 16 数组数组与矩阵类似，只不过维度可以大于2，数组可以通过array()函数来创建 示例2-3 1234567891011121314151617181920212223242526272829&gt; dim2 &lt;- c(&quot;B1&quot;, &quot;B2&quot;, &quot;B3&quot;)&gt; dim1 &lt;- c(&quot;A1&quot;, &quot;A2&quot;)&gt; dim2 &lt;- c(&quot;B1&quot;, &quot;B2&quot;, &quot;B3&quot;)&gt; dim3 &lt;- c(&quot;C1&quot;, &quot;C2&quot;, &quot;C3&quot;, &quot;C4&quot;)&gt; z &lt;- array(1:24, c(2,3,4), dimnames=list(dim1, dim2, dim3))&gt; z, , C1 B1 B2 B3A1 1 3 5A2 2 4 6, , C2 B1 B2 B3A1 7 9 11A2 8 10 12, , C3 B1 B2 B3A1 13 15 17A2 14 16 18, , C4 B1 B2 B3A1 19 21 23A2 20 22 24 数据框由于不同的列可以包含不同的模式（字符型、数值型），所以数据框（data frame）是R中最常处理的数据结构，数据框可以通过data.frame()创建 1mydata &lt;- data.frame(col1, col2, col3,...) 其中的列向量等可以为任何数据类型，每一列的名称可由names指定。 因子变量可以归结为名义型、有序型、或连续型变量，类别（名义型）变量和有序类别（有序型）在R中称为因子（factor）。 例如，假设有向量： 1diabetes &lt;- c(&quot;Type1&quot;,&quot;Type2&quot;,&quot;Type1&quot;,&quot;Type1&quot;) 语句dibetes &lt;- factor(diabetes)将此向量存储为（1，2，1，1），并在内部将其关联为1=Type1和2=Type2。 对于字符型向量，因子的水平默认依字母顺序创建，比如”Excellent””Improved””Poor”的排序方式恰好与逻辑顺序相一致，现实是很少有这种情况让人满意。 可以通过指定levels选项来覆盖默认排序：123&gt; status &lt;- c(&quot;Poor&quot;, &quot;Improved&quot;, &quot;Excellent&quot;, &quot;Poor&quot;)&gt; status &lt;- factor(status, order=TRUE, levels=c(&quot;Poor&quot;, &quot;Improved&quot;, &quot;Excellent&quot;)) #通过levels选项来覆盖默认排序 数值型变量也可以用levels和labels来编码成为因子，如，男性被编码成1，女性被编码成2 1&gt; sex &lt;- factor(sex, levels=c(1,2), labels=c(&quot;Male&quot;, &quot;Female&quot;)) 列表列表（list）就是一些对象的有序集合，可以是字符型、数值型、矩阵，可以组合成为任意多的对象，并存在一个向量里面。 示例2-4 123456&gt; g &lt;- &quot;My First List&quot;&gt; h &lt;- c(25, 26, 18, 39)&gt; j &lt;- matrix(1:10, nrow=5)&gt; k &lt;- c(&quot;one&quot;, &quot;two&quot;, &quot;three&quot;)&gt; mylist &lt;- list(title=g, ages=h, j, k)&gt; mylist]]></content>
      <categories>
        <category>coding</category>
        <category>R</category>
      </categories>
      <tags>
        <tag>数据分析</tag>
        <tag>R</tag>
        <tag>数据结构</tag>
        <tag>data-structure</tag>
        <tag>data analysis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[R使用笔记（1）R语言基本操作：安装、工作空间、包]]></title>
    <url>%2F2016%2F2016-11-13-R-basic-use-installation-workspace-packages%2F</url>
    <content type="text"><![CDATA[这篇文章主要涉及 R 语言的基本操作：安装、工作空间、包 为什么使用 R？ R是免费的！ R提供了各式各样的数据分析技术，几乎任何类型的数据分析工作都可以在R中完成 R拥有顶尖水准的制图功能 R可以被整合到其他语言编写的应用程序，包括Python、SAS、SPSS 缺点是学习曲线比较陡峭 ······ 1. 安装 R 和 RStudioR可以在CRAN（Comprehensive R Archive Network）上免费下载（国内推荐清华大学镜像）。RStudio可以在其官网下载安装。 2. 设置工作空间工作空间（workspace）就是Ｒ当前的工作环境，它存储着所有用户定义的对象（向量、矩阵、函数、数据框、列表），函数getwd()可以查看当前工作目录，setwd()用于设定当前工作目录，当然这一切在RStudio中只需点击即可完成。 函数 功能 getwd() 显示当前工作目录，e.g `setwd(C:/mydirectory)，特别注意反斜杠` setwd("mydirectory") 修改当前的工作目录为mydirectory ls() 列出当前工作空间的对象 rm(objectlist) 移除（删除）一个或多个对象 savehistory("myfile") 保存命令历史文件到myfile loadhistory("myfile") 载入一个命令历史文件，（默认值为.Rdata） save.image("myfile") 保存工作空间到文件myfile中，（默认值为.Rdata） load("myfile") 读取一个工作空间到当前会话中，（默认值为.Rdata） q() 退出R 3. 包R中有很多包（package），可以实现很多功能，技术存储包的目录称为库（library）。 使用命令install.packages(&quot;pkg name&quot;)即可一次安装一个包，要在R会话中使用，执行library(&quot;pkg name&quot;)即可。]]></content>
      <categories>
        <category>coding</category>
        <category>R</category>
      </categories>
      <tags>
        <tag>数据分析</tag>
        <tag>R</tag>
        <tag>data analysis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习算法系列（2）多元线性回归]]></title>
    <url>%2F2016%2F2016-11-12-machine-learning-algorithm-series-linearing-regression%2F</url>
    <content type="text"><![CDATA[1. 基本形式给定d个属性$x=(x_1;x_2;\cdots;x_d)$，线性模型（linear model）可以表示为一个试图通过属性的线性组合来进行预测的函数： f(x)=w^Tx+b其中，$x_i$表示属性$x$在第$i$个属性上的取值,$w$直观表达了各个属性在模型预测中的重要性，$w=(w_1;w_2;\cdots;w_d)$。那么，如何求得$w$和$b$便成为了线性回归的主要任务。 2. 误差度量（Error Measure）给定数据集$D={(x1,y_1),(x_2,y_2),\cdots,(x_m,y_m)}$，其中$x_i=(x{i1};x{i2};\cdots;x{id})$，$y_i\in\mathbb{R}$。假设有$d$个属性，线性回归的目标是学得： f(x_i)=wx_i+b使得$f(x_i)\approx{y_i}$。均方误差是回归任务中最常用的性能度量，因此可以通过让均方误差最小化得到拟合参数 (w^*,b^*)={argmin}\sum_{i=1}^m(f(x_i)-y_i))^23. 拟合参数我们可以利用最小二乘法来对$w$和$b$进行估计。为了计算方便，我们将权重和偏置项合并为一个向量$\hat{w}$，维度为（d+1）×1。相应的，将数据集$D$表示为m×（d+1）大小的矩阵$\mathtt{X}$，其中每一行代表一个示例，最后一个元素恒为1，即 \mathtt{X}= \begin{bmatrix} {x_{11}}&{x_{12}}&{\cdots}&{x_{1d}}&{1}\\ {x_{21}}&{x_{22}}&{\cdots}&{x_{21}}&{1}\\ {\vdots}&{\vdots}&{\ddots}&{\vdots}&{\vdots}\\ {x_{m1}}&{x_{m1}}&{\cdots}&{x_{md}}&{1}\\ \end{bmatrix}标记值记为$y$，运用最小二乘法(Ordinary Least Square)来估计参数 E_{\hat{w}^*}=argmin{(y-{\mathtt{X}}{\hat{w}})}^T(y-{\mathtt{X}}{\hat{w}})对$\hat{w}$求导得到 \frac{\partial{E_{\hat{w}}}}{\partial{\hat{w}}}=2\mathtt{X}^T(\mathtt{X}\hat{w}-y)令上式为零便可得$\hat{w}$的最优解 {\hat{w}}^*={({\mathtt{X}}^T{\mathtt{X}})}^{-1}{\mathtt{X}}^Ty最终，学到的多元线性回归模型为 f({\hat{x}}_i)={\hat{x}}_{i}^T{({\mathtt{X}}^T{\mathtt{X}})}^{-1}{\mathtt{X}}^Ty12345678910111213141516171819202122232425262728293031323334353637import numpy as npimport matplotlib.pyplot as pltfrom sklearn.linear_model import LinearRegressiondef plot_linear_regression(): a = 0.5 b = 1.0 # x from 0 to 10 x = 30 * np.random.random(20) # y = a*x + b with noise y = a * x + b + np.random.normal(size=x.shape) # create a linear regression classifier clf = LinearRegression() clf.fit(x[:, None], y) # predict y from the data x_new = np.linspace(0, 30, 100) y_new = clf.predict(x_new[:, None]) # plot the results ax = plt.axes() ax.scatter(x, y) ax.plot(x_new, y_new) ax.set_xlabel('x') ax.set_ylabel('y') ax.axis('tight')if __name__ == '__main__': plot_linear_regression() plt.show() 3. 小结线性回归总的来说比较容易理解，计算上不太复杂。线性回归模型适合于数值型和标称型数据，但是对于非线性的数据拟合效果不佳，比如Andrew Ng的machine learning课程中就提到用线性回归模型做疾病检测的例子，预测的结果很容易受异常值的影响。 推荐阅读 周志华.机器学习[M].2016年1月第一版.清华大学出版社.2016 李航.统计学习方法[M].2012年3月第1版.清华大学出版社.2015 Hsuan-Tien Lin.Machine Learning Foundations.Lecture 9.linear regression 继续阅读机器学习算法系列笔记！]]></content>
      <categories>
        <category>Machine-Learning</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>算法</tag>
        <tag>机器学习算法系列</tag>
        <tag>线性回归</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习算法系列（1）决策树]]></title>
    <url>%2F2016%2F2016-11-09-machine-learning-algorithm-series-decision-tree%2F</url>
    <content type="text"><![CDATA[决策树（decision tree）是一类常见的机器学习（分类）方法，首先对数据进行处理，利用归纳算法生成可读的规则和决策树，然后使用决策树对新数据进行分析。 决策树的模型与学习 特征选择 决策树的生成 1. 决策树的模型与学习1.1 简单的例子：选西瓜的一棵决策树一般，一颗决策树包含一个根节点（node），若干个内部结点（internal node）和若干个叶节点（leaf node）；叶节点对应于决策结果，其他每个结点对应于一个属性测试；每个结点包含的样本的集合根据属性测试的结果被划分到子节点中。 在沿着决策树从上到下的遍历过程中，在每个结点都有一个测试。对每个结点上问题的不同测试输出导致不同的分支，最后会达到一个叶子结点。这一过程就是利用决策树进行分类的过程，利用若干个变量来判断属性的类别。 1.2 决策树模型决策树（decision tree）是一类常见的机器学习（分类）方法，首先对数据进行处理，利用归纳算法生成可读的规则和决策树，然后使用决策树对新数据进行分析。 决策树本质上是从训练数据集中归纳出一组分类规则，决策树学习通常包括三个步骤：特征选择、决策树生成和决策树修剪。 1.3 决策树和归纳算法 决策树技术发现数据模式和规则的核心是归纳算法 归纳是从特殊到一般的过程 归纳推理从若干个事实中表征出的特征、特性和属性中，通过比较、总结、概括而得出一个规律性的结论 归纳推理视图从对象的一部分或整体的特定的观察中获得一个完备且正确的描述，即从特殊事实到普遍性规律的结论 归纳对于认识的发展和完善具有重要的意义。人类知识的增长主要来源于归纳学习，机器也是如此。 从特殊的训练样例中归纳出一般函数是机器学习的核心问题，从训练样例中进行学习的过程： 12345671. 训练集D=&#123;(x1,y1), (x2,y2),…,(xn,yn)&#125;，属性集A=&#123;a1,a2,…,av&#125;2. 学习过程将进行对目标函数f的不断逼近，每一次逼近都叫做一个假设h3. 假设需要以某种形式进行，如，y=ax+b。通过调整假设的表示，即应用不同算法，产生出假设的不同变形4. 从不同的变形中选择最佳的组合 1.4 选择方法使训练值与假设值预测出的值之间的误差平方和Err最小 min err = \sum_{i=0}^n(\hat{y}_i - y_i)^21.5 决策树算法 决策树相关的重要算法包括：ID3，C4.5，CART 决策树算法的发展历程 Hunt,Marin和Stone 于1966年研制的CLS学习系统,用于学习单个概念。 1979年, J.R. Quinlan 给出ID3算法,并在1983年和1986年对ID3进行了总结和简化,使其成为决策树学习算法的典型。 Schlimmer 和Fisher于1986年对ID3进行改造,在每个可能的决策树节点创建缓冲区,使决策树可以递增式生成,得到ID4算法。 1993年,Quinlan 进一步发展了ID3算法,改进成C4.5算法。 另一类决策树算法为CART,与C4.5不同的是,CART的决策树由二元逻辑问题生成,每个树节 点只有两个分枝,分别包括学习实例的正例与反例。 2. 特征选择决策的关键在于如何选择最优划分属性一般而言，随着划分过程不断进行，决策树分支结点所包含的样本尽可能属于同一类别，即“纯度”越来越高。通常特征选择的准则是信息增益（information gain）或信息增益比(gain ratio)。 2.1 信息熵（Entropy） 三十而立，四十而不惑，五十知天命…人生，就是一次熵不断变小的过程。 香农1948年提出信息论理论，在信息论和概率统计中，熵（entropy）是表示随机变量不确定性的度量，用来度量样本集合纯度最常用的一些指标。样本集合D中第k类样本所占比例为pk（k=1,2,…,|y|），则D的信息熵定义为： H(D)=-\sum_{k=1}^{|y|}{p_klog_2{p_k}}熵越大，随机变量的不确定性越大。当D服从0,1分布时，熵与不确定性程度的关系： 如图，当p为0或1时，熵值最小，随机变量完全没有不确定性，当p=0.5时，熵值达到最大值，随机变量的不确定性最大。 2.2 信息增益（Information gain）假定离散属性a有V个可能的取值{a1,a2,…,aV}，使用a对样本集进行划分，则会产生V个分支结点，其中第v个分支结点包含了D中所有在属性a上取值为av的样本，记为Dv，于是可以计算出属性a对样本集D进行划分所获得的信息增益（information gain）： Gain(D,a)=H(D)-\sum_{v=1}^{V}\frac{|D^v|}{|D|}H(D^v)信息增益表示得知属性a的信息而使得D的信息不确定性减少的程度，决策树学习中的信息增益等价于训练数据集中划分属性与类别的互信息（mutual information）。 信息增益表示属性a使得数据集D的分类不确定性减少的程度 对于数据集D而言，信息增益依赖于特征，不同的特征往往具有不同的信息增益 信息增益大的特征具有更强的分类能力 信息增益指标可能会出现泛化能力不佳的情形，假如用编号作为划分指标，那么信息增益将达到最大值，显然，这样的决策树不能对新样本进行预测 2.3 信息增益率（Gain ratio）著名的C4.5决策树算法不直接使用信息增益，而是使用信息增益率（gain ratio）来选择最优划分属性，增益率定义为： Gain \_ Ratio=\frac{Gain(D,a)}{IV(a)}IV(a)=-\sum_{v=1}^{V}\frac{|D^v|}{|D|}log_2{\frac{D^v}{|D|}}IV(a)称为a属性的“固有值”(intrinsic value)，所以一般增益率准则对数目较少的属性有所偏好。如果av属性的数目越大，IV(a)相应也会变大，所以比较其增益就没有太大优势。举个例子，如果需要你选择一个特征来划分人群，身份证号码是一个完美的选择，但是我们并不能在实际中应用这个特征。选择信息增益率来做特征选择就是为了避免出现这种状况，我们不仅仅要考量特征对数据的划分能力，而且还需要考察特征本省的信息熵。 2.4 基尼指数（Gini index）CART决策树使用基尼指数来选择划分属性，数据集D的纯度可以用基尼值来度量： Gini(D)反映了从数据集中随机抽取两个样本，其类别标记不一致的概率，因此Gini(D)越小，数据集D的纯度越高 于是，属性集合A中，选择使得划分后基尼指数最小的属性作为最优划分属性 3. 决策树的生成 ID3算法 ID3算法-流程 ID3算法-小结 C4.5算法 3.1 决策树ID3算法 ID3算法是一种经典的决策树学习算法，由Quinlan与1979年提出。 ID3算法主要针对属性选择问题。是决策树学习方法中最具影响和最为典型的算法。 ID3采用信息增益度选择测试属性。 当获取信息时，将不确定的内容转为确定的内容，因此信息伴随不确定性。 在决策树分类中，假定D是训练样本集合，|D|是训练样本数，离散属性a有V个可能的取值{a1,a2,…,aV}，使用a对样本集进行划分，则会产生V个分支结点，其中第v个分支结点包含了D中所有在属性a上取值为av的样本，记为Dv，于是可以计算出属性a对样本集D进行划分所获得的信息增益（information gain）： Gain(D,a)=H(D)-{\sum_{v=1}^{V}}{\frac{|D^v|}{|D|}}H(D^v)信息增益表示得知属性a的信息而使得D的信息不确定性减少的程度。经验条件熵H(D|A)表示在特征A给定的条件下对数据集D进行分类的不确定性，它们的差，即信息增益。 3.2 决策树ID3算法-流程123456789101112131415161718输入： 训练数据集D，特征集A，阈值ε；输出： 决策树T（1） 若D中所有实例均属于同一类Ck，则T为单节点树，并将Ck&gt;作为该节点的类标记，返回T；（2） 若A=∅，则T为单节点树，并将D中实例树最大的类Ck作为该节点的类标记，返回T；（3）否则，按照信息增益的计算方法，选择信息增益最大的特征Ag;（4）如果的信息增益小于阈值，则置T为单节点树，并将D中实例数最大的类Ck作为该节点的类标记，返回T；（5）否则，对Ag的每一可能值ai，依Ag=ai将D分割为若干非空子集Di，将中实例最大的类作为标记，构建子结点，由结点及其子节点构成树T，返回T；（6）对第i个子结点，以Di&lt;/sub&gt;为训练集，以A-&#123;Ag&#125;为特征集，递归地调用步（1）~步（5），得到子树Ti，返回Ti 3.3 ID3算法-小结ID3算法的基本思想是，以信息熵为度量，用于决策树结点的属性选择，每次优先选取信息量最多的属性，构建一棵熵值下降最快的决策树，到叶节点处的熵值为0 3.4 决策树C4.5生成算法ID3算法只有树的生成，所以该算法容易生成的树很容易产生过拟合，从而泛化能力不佳。C4.5的算法过程与ID3类似，只是属性选择的指标换成了信息增益比。 4. Python 实现代码分析1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283from math import logimport operatordef credataSet(): dataSet = [[1, 1, 'yes'], [1, 1, 'yes'], [1, 0, 'no'], [0, 1, 'no'], [0, 1, 'no']] labels = ['no surfacing', 'flippers'] return dataSet, labels dataset, labels = credataSet()labels ['no surfacing', 'flippers']feat = [example[1] for example in dataset]set(feat) &#123;0, 1&#125;#计算信息熵def calEnt(dataset): numEnteries = len(dataset) # 新建一个空的字典，这种用法通常用于数据集中字段计数 labelCounts = &#123;&#125; for featVec in dataset: currentLabel = featVec[-1] if currentLabel not in labelCounts.keys(): labelCounts[currentLabel] = 0 labelCounts[currentLabel] += 1 Ent = 0.0 for key in labelCounts: # 先转换为浮点 prob = float(labelCounts[key]) / numEnteries Ent -= prob * log(prob, 2) return Entent = calEnt(dataset)# 按照指定特征划分数据集# dataset：待划分的数据集，axis：划分数据集特征# value：返回的特征的值def splitDataSet(dataset, axis, value): retDataSet = [] for featVec in dataset: if featVec[axis] == value: reducedFeatVec = featVec[:axis] reducedFeatVec.extend(featVec[axis+1:]) retDataSet.append(reducedFeatVec) return retDataSetsplitdata = splitDataSet(dataset, 0, 1)splitdata [[1, 'yes'], [1, 'yes'], [0, 'no']]def chooseBestFeatureToSplit(dataset): numFeatures = len(dataset[0]) - 1 baseEntropy = calEnt(dataset) bestInfoGain = 0.0; bestFeature = -1 for i in range(numFeatures): # 取每一个样本的第i+1个元素，featList = [1, 1, 0, 1, 1] # set(featList) = &#123;0,1&#125;，set是得到列表中唯一元素值的最快方法 featList = [example[i] for example in dataset] uniqueVals = set(featList) newEntropy = 0.0 for value in uniqueVals: # 遍历特征下属性值，对每一个属性值划分一次数据集 subDataSet = splitDataSet(dataset, i, value) prob = len(subDataSet)/float(len(dataset)) newEntropy += prob * calEnt(subDataSet) # 计算两个特征各自的信息增益，并进行比较 infoGain = baseEntropy - newEntropy if (infoGain &gt; bestInfoGain): bestInfoGain = infoGain bestFeature = i return bestFeaturebestfeature = chooseBestFeatureToSplit(dataset) majorityCnt(classList)函数使用分类名称的列表然后创建值为classList中唯一值的数据字典，字典对象存储了classList中每个类标签出现的频率，最后利用operator操作键值排序字典，并返回出现次数最多的分类名称。 123456789101112131415161718192021222324252627282930def majorityCnt(classList): classCount = &#123;&#125; for vote in classList: if vote not in classCount.keys(): classCount[Vote] = 0 classCount[vote] += 1 sortedClassCount = sorted(classCount.iteritems(), key=operator.itemgetter(1), reverse=True) return sortedClassCount[0][0]def createTree(dataset, labels): classList = [example[-1] for example in dataset] if classList.count(classList[0]) == len(classList): return classList[0] # 如果数据的长度为1，那么意味着所有的特征都用完了，分类完毕并返回次数最多的分类名称 if len(dataset[0]) == 1: return majorityCnt(classList) bestFeat = chooseBestFeatureToSplit(dataset) bestFeatLabel = labels[bestFeat] myTree = &#123;bestFeatLabel:&#123;&#125;&#125; # 当dataset的特征被选中为bestFeat后便不再作为候选划分特征，所以要删除掉 del(labels[bestFeat]) featValues = [example[bestFeat] for example in dataset] uniqueVals = set(featValues) for value in uniqueVals: subLabels = labels[:] myTree[bestFeatLabel][value] = createTree(splitDataSet(dataset, bestFeat, value), subLabels) return myTreecreateTree(dataset, labels) &#123;'no surfacing': &#123;0: 'no', 1: &#123;'flippers': &#123;0: 'no', 1: 'yes'&#125;&#125;&#125;&#125; 推荐阅读 周志华.机器学习[M].2016年1月第一版.清华大学出版社.2016 李航.统计学习方法[M].2012年3月第1版.清华大学出版社.2015 继续阅读机器学习算法系列笔记！]]></content>
      <categories>
        <category>Machine-Learning</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>算法</tag>
        <tag>决策树</tag>
        <tag>机器学习算法系列</tag>
      </tags>
  </entry>
</search>
